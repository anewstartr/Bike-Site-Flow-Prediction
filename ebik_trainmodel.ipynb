{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "automotive-contribution",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:19:35.250071Z",
     "iopub.status.busy": "2025-10-22T09:19:35.249623Z",
     "iopub.status.idle": "2025-10-22T09:19:36.930262Z",
     "shell.execute_reply": "2025-10-22T09:19:36.929546Z",
     "shell.execute_reply.started": "2025-10-22T09:19:35.249963Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,RandomSampler,SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "# import optuna\n",
    "from torch.nn import functional\n",
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "infrared-scott",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:19:38.964290Z",
     "iopub.status.busy": "2025-10-22T09:19:38.963936Z",
     "iopub.status.idle": "2025-10-22T09:19:38.977856Z",
     "shell.execute_reply": "2025-10-22T09:19:38.977101Z",
     "shell.execute_reply.started": "2025-10-22T09:19:38.964259Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_merge_batches(output_dir, static_dict, expected_shape=None):\n",
    "    \"\"\"\n",
    "    批量读取并合并为三维numpy数组\n",
    "    \n",
    "    Args:\n",
    "        output_dir: 包含批次文件的目录\n",
    "        expected_shape: 期望的最终形状 (N, seq_len, feature_count)，可选\n",
    "        delete_after_merge: 合并后是否删除原始文件\n",
    "    \n",
    "    Returns:\n",
    "        merged_features: 合并后的三维数组 [N, seq_len, feature_count]\n",
    "        all_site_guids: 所有站点的GUID列表\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 查找所有的批次文件\n",
    "    hourly_feature_files = sorted(glob.glob(os.path.join(output_dir, 'batch_*_hourly_features.npy')))\n",
    "    static_feature_files = sorted(glob.glob(os.path.join(output_dir, 'batch_*_static_features.npy')))\n",
    "    guid_files = sorted(glob.glob(os.path.join(output_dir, 'batch_*_guids.txt')))\n",
    "    \n",
    "    print(f\"找到 {len(hourly_feature_files)} 个特征文件和 {len(guid_files)} 个GUID文件\")\n",
    "    \n",
    "    if len(hourly_feature_files) != len(guid_files):\n",
    "        print(\"警告: 特征文件和GUID文件数量不匹配\")\n",
    "    \n",
    "    # 2. 初始化存储\n",
    "    hourly_all_features = []\n",
    "    static_all_features = []\n",
    "    all_site_guids = []\n",
    "    batch_shapes = []\n",
    "    \n",
    "    # 3. 逐个加载批次文件\n",
    "    for i, (hourly_feat_file, static_feat_file, guid_file) in enumerate(tqdm(zip(hourly_feature_files, static_feature_files, guid_files), \n",
    "                                                   total=len(hourly_feature_files), \n",
    "                                                   desc=\"加载批次\")):\n",
    "        \n",
    "        try:\n",
    "            # 加载特征数据\n",
    "            hourly_batch_features = np.load(hourly_feat_file)\n",
    "            static_batch_features = np.load(static_feat_file)\n",
    "#             hourly_batch_shapes.append(hourly_batch_features.shape)\n",
    "#             static_batch_shapes.append(static_batch_features.shape)\n",
    "\n",
    "            \n",
    "            # 加载GUID\n",
    "            with open(guid_file, 'r') as f:\n",
    "                batch_guids = [line.strip() for line in f if line.strip()]\n",
    "            \n",
    "            # 验证数据一致性\n",
    "            if len(batch_guids) != hourly_batch_features.shape[0]:\n",
    "                print(f\"警告: 批次 {i} 的小时特征数量 {hourly_batch_features.shape[0]} 和GUID数量 {len(batch_guids)} 不匹配\")\n",
    "                continue\n",
    "            if len(batch_guids) != static_batch_features.shape[0]:\n",
    "                print(f\"警告: 批次 {i} 的静态特征数量 {static_batch_features.shape[0]} 和GUID数量 {len(batch_guids)} 不匹配\")\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            hourly_all_features.append(hourly_batch_features)\n",
    "            static_all_features.append(static_batch_features)\n",
    "            all_site_guids.extend(batch_guids)\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"加载批次 {i} 时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 4. 合并所有批次\n",
    "    if not hourly_all_features:\n",
    "        print(\"没有找到有效的数据批次\")\n",
    "        return np.empty((0, 0, 0)), np.empty((0, 0, 0)), []\n",
    "    \n",
    "    # 合并特征数组\n",
    "    try:\n",
    "        hourly_merged_features = np.concatenate(hourly_all_features, axis=0)\n",
    "        static_merged_features = np.concatenate(static_all_features, axis=0)        \n",
    "        print(f\"成功合并 {len(hourly_all_features)} 个批次\")\n",
    "        print(f\"动态特征合并后形状: {hourly_merged_features.shape}\")\n",
    "        print(f\"静态特征合并后形状: {static_merged_features.shape}\")\n",
    "        print(f\"GUID数量: {len(all_site_guids)}\")\n",
    "        sorted_features = static_dict.keys()\n",
    "        # 按照特征名称顺序提取特征\n",
    "#         for i, feat_name in enumerate(sorted_features):\n",
    "#             feature = static_input[:, i]\n",
    "#             feature_type, structure_type, dimension = self.static_dict[feat_name]\n",
    "#             if feature_type == 'continuous':\n",
    "#                 static_merged_features[]\n",
    "        \n",
    "        # 验证形状一致性\n",
    "        if len(all_site_guids) != hourly_merged_features.shape[0]:\n",
    "            print(f\"警告: 最终小时特征数量 {hourly_merged_features.shape[0]} 和GUID数量 {len(all_site_guids)} 不匹配\")\n",
    "        if len(all_site_guids) != static_merged_features.shape[0]:\n",
    "            print(f\"警告: 最终小时特征数量 {static_merged_features.shape[0]} 和GUID数量 {len(all_site_guids)} 不匹配\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"合并批次时出错: {e}\")\n",
    "        return np.empty((0, 0, 0)), np.empty((0, 0, 0)), []\n",
    "    \n",
    "    \n",
    "    return hourly_merged_features, static_merged_features, all_site_guids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worse-kuwait",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:38:23.552798Z",
     "iopub.status.busy": "2025-10-20T09:38:23.552565Z",
     "iopub.status.idle": "2025-10-20T09:38:23.616098Z",
     "shell.execute_reply": "2025-10-20T09:38:23.614918Z",
     "shell.execute_reply.started": "2025-10-20T09:38:23.552772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "saving-exhaust",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:19:43.548289Z",
     "iopub.status.busy": "2025-10-22T09:19:43.547947Z",
     "iopub.status.idle": "2025-10-22T09:46:24.282447Z",
     "shell.execute_reply": "2025-10-22T09:46:24.281574Z",
     "shell.execute_reply.started": "2025-10-22T09:19:43.548258Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载批次:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 5 个特征文件和 5 个GUID文件\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载批次: 100%|██████████| 5/5 [15:14<00:00, 182.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功合并 5 个批次\n",
      "动态特征合并后形状: (113468, 1008, 22)\n",
      "静态特征合并后形状: (113468, 26, 7)\n",
      "GUID数量: 113468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(113468, 1008, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"ebik_dataset/traindata1\"\n",
    "mapping_df = pd.read_csv(\"ebik_dataset/city_guid_mapping_3.csv\")\n",
    "max_city = len(mapping_df)\n",
    "static_dict = { \"lag7d_order_cnt\": ('continuous',None,None),\n",
    "                \"parking_capacity\": ('continuous',None,None),\n",
    "                \"temperature_avg_val\": ('continuous',None,None),\n",
    "                \"city_guid_encoded\": ('categorical','embedding',max_city+1),\n",
    "                \"day_of_week\": ('categorical','onehot',8),\n",
    "                \"workday_level\": ('categorical','onehot',3),\n",
    "                \"cycle_weather_level\": ('categorical','embedding',4)\n",
    "              }\n",
    "hourly_all_data, static_all_data, all_site_guids = load_and_merge_batches(output_dir, static_dict = static_dict, expected_shape=None)\n",
    "hourly_all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acting-occurrence",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:03:56.599501Z",
     "iopub.status.busy": "2025-10-22T08:03:56.599087Z",
     "iopub.status.idle": "2025-10-22T08:03:56.607614Z",
     "shell.execute_reply": "2025-10-22T08:03:56.606906Z",
     "shell.execute_reply.started": "2025-10-22T08:03:56.599462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.000e+00, 1.280e+02, 2.540e+01, 1.012e+03, 4.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [9.000e+00, 0.000e+00, 2.450e+01, 1.012e+03, 5.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [1.100e+01, 1.280e+02, 2.477e+01, 1.012e+03, 6.000e+00,\n",
       "         1.000e+00, 2.000e+00],\n",
       "        [1.000e+01, 1.280e+02, 2.397e+01, 1.012e+03, 7.000e+00,\n",
       "         2.000e+00, 3.000e+00],\n",
       "        [1.000e+01, 0.000e+00, 2.344e+01, 1.012e+03, 1.000e+00,\n",
       "         2.000e+00, 3.000e+00],\n",
       "        [6.000e+00, 1.280e+02, 2.346e+01, 1.012e+03, 2.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [1.000e+00, 1.280e+02, 2.342e+01, 1.012e+03, 3.000e+00,\n",
       "         1.000e+00, 2.000e+00],\n",
       "        [1.000e+01, 1.280e+02, 2.406e+01, 1.012e+03, 4.000e+00,\n",
       "         1.000e+00, 2.000e+00],\n",
       "        [6.000e+00, 0.000e+00, 2.406e+01, 1.012e+03, 5.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [2.000e+00, 1.280e+02, 2.404e+01, 1.012e+03, 6.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [5.000e+00, 1.280e+02, 2.391e+01, 1.012e+03, 7.000e+00,\n",
       "         2.000e+00, 2.000e+00],\n",
       "        [9.000e+00, 1.280e+02, 2.457e+01, 1.012e+03, 1.000e+00,\n",
       "         2.000e+00, 3.000e+00],\n",
       "        [6.000e+00, 1.280e+02, 2.253e+01, 1.012e+03, 2.000e+00,\n",
       "         1.000e+00, 2.000e+00],\n",
       "        [1.000e+00, 1.280e+02, 2.231e+01, 1.012e+03, 3.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [8.000e+00, 0.000e+00, 2.003e+01, 1.012e+03, 4.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [7.000e+00, 1.280e+02, 2.070e+01, 1.012e+03, 5.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [6.000e+00, 1.280e+02, 2.075e+01, 1.012e+03, 6.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [1.200e+01, 1.280e+02, 2.113e+01, 1.012e+03, 7.000e+00,\n",
       "         2.000e+00, 3.000e+00],\n",
       "        [8.000e+00, 1.280e+02, 2.149e+01, 1.012e+03, 1.000e+00,\n",
       "         2.000e+00, 3.000e+00],\n",
       "        [3.000e+00, 1.280e+02, 2.009e+01, 1.012e+03, 2.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [7.000e+00, 1.280e+02, 1.935e+01, 1.012e+03, 3.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [6.000e+00, 0.000e+00, 2.089e+01, 1.012e+03, 4.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [4.000e+00, 0.000e+00, 2.095e+01, 1.012e+03, 5.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [5.000e+00, 1.280e+02, 2.128e+01, 1.012e+03, 6.000e+00,\n",
       "         1.000e+00, 3.000e+00],\n",
       "        [1.100e+01, 1.280e+02, 2.042e+01, 1.012e+03, 7.000e+00,\n",
       "         2.000e+00, 2.000e+00],\n",
       "        [1.500e+01, 0.000e+00, 2.000e+01, 1.012e+03, 1.000e+00,\n",
       "         1.000e+00, 2.000e+00]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_all_data[100:101,:,0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "drawn-accident",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:46:24.284091Z",
     "iopub.status.busy": "2025-10-22T09:46:24.283752Z",
     "iopub.status.idle": "2025-10-22T09:46:24.295524Z",
     "shell.execute_reply": "2025-10-22T09:46:24.294742Z",
     "shell.execute_reply.started": "2025-10-22T09:46:24.284049Z"
    }
   },
   "outputs": [],
   "source": [
    "# 【站点数量，序列长度，特征数量】\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, his_datas, sta_datas, his_label, output_sizes, time_feature_size, static_feature_size, seq_num, time_of_day):\n",
    "        self.his_datas = his_datas  #【N，1080，X】\n",
    "        self.sta_datas = sta_datas  #【N，26，Y】\n",
    "        self.his_label = his_label  #【N，1080，1】\n",
    "        self.output_sizes = output_sizes  # 输出长度24\n",
    "        self.time_feature_size = time_feature_size  # 卷积塔时序特征数量\n",
    "        self.static_feature_size = static_feature_size  # 特征塔天粒度/静态特征数量\n",
    "        self.seq_num = seq_num  # 窗口大小\n",
    "        self.time_of_day = time_of_day  # 每天24小时\n",
    "         \n",
    "        self.site_num = his_datas.shape[0]  # 站点数量\n",
    "        self.time_num = his_datas.shape[1] // time_of_day  - (seq_num + 3) # 单个站点的样本数量：26-15=11个样本\n",
    "        self.sample_num = self.time_num * self.site_num  # 总样本数量：32*1080=3w\n",
    "        # print(his_datas.shape)\n",
    "        print('单个样本数量：', self.time_num)\n",
    "        print('站点数量：', self.site_num)\n",
    "        print('总样本数量：', self.sample_num)\n",
    "        print(\"a\", his_datas.shape, his_label.shape)\n",
    "        \n",
    "    def __getitem__(self, index): # 0-3w\n",
    "        # 是第几个样本？\n",
    "        cls_indx, time_indx = divmod(index, self.time_num)\n",
    "        start_index = time_indx * self.time_of_day\n",
    "        end_index = (time_indx + self.seq_num) * self.time_of_day\n",
    "        static_index = time_indx\n",
    "        # [站点,小时粒度序列,小时粒度特征]\n",
    "        tmp_data = self.his_datas[cls_indx, start_index:end_index, 0:self.time_feature_size].astype(float)  # [0, 14*24, time_feature_size]\n",
    "        sample_time_data = torch.tensor(tmp_data, dtype=torch.float32)\n",
    "        # [站点,天粒度序列,天粒度特征]\n",
    "        static_data = self.sta_datas[cls_indx, static_index:static_index+1, 0:self.static_feature_size].astype(float)  # [0, 1, time_feature_size]\n",
    "        sample_static_data = torch.tensor(static_data, dtype=torch.float32)\n",
    "        # [站点,序列,1]  # 预测的时候，拿不到前一天的数据。预测0807，在0806进行，只能拿到0804的数据\n",
    "        sample_labels = {}\n",
    "        label_start = end_index + self.time_of_day * 3\n",
    "        for target_idx, output_size in self.output_sizes.items():\n",
    "            # 获取对应目标的标签 [output_size, 1]\n",
    "            label_end = label_start + output_size\n",
    "            target_label = self.his_label[cls_indx, label_start:label_end, target_idx:target_idx+1].astype(float)\n",
    "            sample_labels[target_idx] = torch.tensor(target_label, dtype=torch.float32)\n",
    "        \n",
    "        return sample_time_data, sample_static_data, sample_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sample_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gothic-dairy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:46:24.297731Z",
     "iopub.status.busy": "2025-10-22T09:46:24.297359Z",
     "iopub.status.idle": "2025-10-22T09:46:24.315752Z",
     "shell.execute_reply": "2025-10-22T09:46:24.314892Z",
     "shell.execute_reply.started": "2025-10-22T09:46:24.297691Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(all_data, sta_data):  # 56天\n",
    "    tmp_data_info = np.array(all_data)\n",
    "    sta_data_info = np.array(sta_data)\n",
    "    train_start_idx = 0\n",
    "    train_end_idx = 35 * 24 \n",
    "    val_start_idx = 17 * 24\n",
    "    val_end_idx = 38 * 24 \n",
    "    test_start_idx = 21 * 24\n",
    "    test_end_idx = 42 * 24 \n",
    "    train_start_sta_idx = 0\n",
    "    train_end_sta_idx = 18\n",
    "    val_end_sta_idx = 22\n",
    "    test_end_sta_idx = 26\n",
    "    \n",
    "#     \n",
    "#     train_start_idx = 0\n",
    "#     train_end_idx = 38 * 24  # 9\n",
    "#     val_start_idx = (38 - 30) * 24  # 13使用14，14使用15\n",
    "#     val_end_idx = 42 * 24  # 4\n",
    "#     test_start_idx = (42 - 30) * 24\n",
    "#     test_end_idx = 49 * 24  # 7\n",
    "    \n",
    "#     train_end_idx = 39 * 24  # 10\n",
    "#     val_start_idx = (39 - 30) * 24  # 13使用14，14使用15\n",
    "#     val_end_idx = 41 * 24  # 4\n",
    "#     test_start_idx = (41 - 30) * 24\n",
    "#     test_end_idx = 49 * 24  # 8\n",
    "    \n",
    "    train_data = tmp_data_info[:, train_start_idx:train_end_idx, :]  # 所有特征\n",
    "    train_data_sta = sta_data_info[:, train_start_sta_idx:train_end_sta_idx, :]\n",
    "    train_label = tmp_data_info[:, train_start_idx:train_end_idx, 0:3]\n",
    "    val_data = tmp_data_info[:, val_start_idx:val_end_idx, :]\n",
    "    val_data_sta = sta_data_info[:, train_end_sta_idx:val_end_sta_idx, :]    \n",
    "    val_label = tmp_data_info[:, val_start_idx:val_end_idx, 0:3]\n",
    "    test_data = tmp_data_info[:, test_start_idx:test_end_idx, :]\n",
    "    test_data_sta = sta_data_info[:, val_end_sta_idx:test_end_sta_idx, :]  \n",
    "    test_label = tmp_data_info[:, test_start_idx:test_end_idx, 0:3]\n",
    "    return train_data, train_data_sta, train_label, val_data, val_data_sta, val_label, test_data, test_data_sta, test_label\n",
    "\n",
    "def load_multitask_data(all_data, sta_data, batch_size, output_sizes={0: 24, 1: 24, 2: 24}):\n",
    "    # 自定义collate函数处理多目标数据\n",
    "    def multitask_collate(batch):\n",
    "        time_data = torch.stack([item[0] for item in batch])\n",
    "        static_data = torch.stack([item[1] for item in batch])\n",
    "        labels = {}\n",
    "        for target_idx in output_sizes.keys():\n",
    "            labels[target_idx] = torch.stack([item[2][target_idx] for item in batch])\n",
    "        return time_data, static_data, labels\n",
    "    train_data, train_data_sta, train_label, val_data, val_data_sta, val_label, test_data, test_data_sta, test_label = train_test_split(all_data, sta_data)\n",
    "    train_dataset = MyDataset(his_datas=train_data, sta_datas = train_data_sta, his_label=train_label, \n",
    "                              output_sizes=output_sizes, time_feature_size=22, static_feature_size=7, seq_num=14, time_of_day=24)\n",
    "    n_samples = len(train_dataset)\n",
    "    indices = list(range(n_samples))\n",
    "    # 随机选择50%的样本\n",
    "    split = int(0.4 * n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices = indices[:split]  # 前50%作为本次训练样本\n",
    "    # 创建采样器\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        collate_fn=multitask_collate\n",
    "    )\n",
    "    # train_rand_sampler = RandomSampler(train_dataset, replacement=False, num_samples=int(len(train_dataset)*0.3))\n",
    "    # train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, sampler=train_rand_sampler) \n",
    "    # train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True) \n",
    "      \n",
    "    val_dataset = MyDataset(his_datas=val_data, sta_datas = val_data_sta, his_label=val_label,\n",
    "                            output_sizes=output_sizes, time_feature_size=22, static_feature_size=7, seq_num=14, time_of_day=24)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False, collate_fn=multitask_collate)\n",
    "\n",
    "    test_dataset = MyDataset(his_datas=test_data, sta_datas = test_data_sta, his_label=test_label,\n",
    "                             output_sizes=output_sizes, time_feature_size=22, static_feature_size=7, seq_num=14, time_of_day=24)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = 4, shuffle=False, collate_fn=multitask_collate)\n",
    "\n",
    "    return train_dataloader , val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "religious-nature",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:46:24.317766Z",
     "iopub.status.busy": "2025-10-22T09:46:24.317386Z",
     "iopub.status.idle": "2025-10-22T09:46:24.342292Z",
     "shell.execute_reply": "2025-10-22T09:46:24.341668Z",
     "shell.execute_reply.started": "2025-10-22T09:46:24.317725Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "def huber_loss(y_pred, y_true):\n",
    "    loss = torch.nn.SmoothL1Loss(reduction='mean',beta=5.0)(y_pred, y_true)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    loss = torch.nn.MSELoss(reduction='mean')(y_pred, y_true)\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def printbar():\n",
    "    t = datetime.datetime.now()\n",
    "    print('==========='*8 + str(t))\n",
    "\n",
    "\n",
    "import os\n",
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] =str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.daterministic = True\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        # [N, 38, 14*24]->[N, 300, 14*24]\n",
    "        # n_inputs=38, n_outputs=1\n",
    "        # weight_norm(\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.norm1 = nn.GroupNorm(1, n_outputs)  #加一层试试\n",
    "\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # [N, 300, 14*24]->[N, 300, 14*24]\n",
    "        # weight_norm(\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.norm2 = nn.GroupNorm(1, n_outputs)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.norm1, self.relu1, self.dropout1,\n",
    "                                  self.conv2, self.chomp2, self.norm2, self.relu2, self.dropout2)\n",
    "\n",
    "        # [N, 38, 14*24]->[N, 300, 14*24]\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None # 1x1 conv\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: n*emb_size*seq_len\n",
    "        out: n*layer_outchannel* seq_len\"\"\"\n",
    "        # [N, 38, 14*24]->[N, 1, 14*24]\n",
    "        out = self.net(x)\n",
    "        # [N, 38, 14*24]->[N, 1, 14*24]\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)  # [N, 1, 14*24]\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    # num_inputs=38, out_channels=[300, 200, 100, 50, 1]\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        # dilation_sizes = [1,4,16,24]\n",
    "        for i in range(num_levels):\n",
    "            \"\"\"dilated conv\"\"\"\n",
    "            dilation_size = 2 ** i   #认为此处不合理，待改                                                    \n",
    "            # dilation_size = dilation_sizes[i]\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            # [N, 300, 14*24] + [N, 200, 14*24] + [N, 100, 14*24] + [N, 50, 14*24] + [N, 1, 14*24]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    " \n",
    "\n",
    "class MultiTaskTCN(nn.Module):\n",
    "    def __init__(self, input_size, input_len, output_sizes, num_channels,\n",
    "                 kernel_size=2, dropout=0.3, emb_dropout=0.1, tied_weights=False):\n",
    "        super(MultiTaskTCN, self).__init__()\n",
    "        # [N, 38, 14*24]->[N, 3, 14*24]\n",
    "        # input_size=39, num_channels=[300, 200, 100, 50, 1],output_size=1*24\n",
    "        self.output_sizes = output_sizes\n",
    "        self.time_tasks = list(output_sizes.keys())\n",
    "        # 为每个时间粒度创建独立的预测头\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        # self.decoder = nn.Linear(input_len, output_size)\n",
    "        self.drop = nn.Dropout(emb_dropout)\n",
    "        self.emb_dropout = emb_dropout\n",
    "        # 为每个时间粒度创建独立的预测头\n",
    "        self.prediction_heads = nn.ModuleDict()\n",
    "        for scale in self.time_tasks:\n",
    "            self.prediction_heads[str(scale)] = nn.Linear(\n",
    "                input_len,\n",
    "                output_sizes[scale]\n",
    "            )\n",
    "            self.prediction_heads[str(scale)].bias.data.fill_(0)\n",
    "            self.prediction_heads[str(scale)].weight.data.normal_(0, 0.01)\n",
    "        # self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "#         self.decoder.bias.data.fill_(0)\n",
    "#         self.decoder.weight.data.normal_(0, 0.01)\n",
    "        for head in self.prediction_heads:\n",
    "            head.bias.data.fill_(0)\n",
    "            head.weight.data.normal_(0, 0.01)\n",
    "            \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Input ought to have dimension (N, C_in, L_in), \n",
    "        where L_in is the seq_len; \n",
    "        here the input is (N, L, C)\"\"\"\n",
    "        # [N, 14*24, 38]->[N, 38, 14*24]\n",
    "        y = input.transpose(1, 2)\n",
    "        # [N, 38, 14*24]->[N, 3, 14*24]\n",
    "        y = self.tcn(y)\n",
    "        # [N, 3, 14*24]->拆分多目标->[N, 1, 1*24]->[N, 1*24, 1]\n",
    "        # 为每个时间粒度生成预测\n",
    "        predictions = {}\n",
    "        for scale in self.time_tasks:\n",
    "            scale_features = y[:,scale:scale+1,:]\n",
    "            predictions[scale] = self.prediction_heads[str(scale)](scale_features).transpose(1, 2).contiguous()\n",
    "        \n",
    "        return predictions\n",
    "#         y = self.decoder(y).transpose(1, 2)\n",
    "#         return y.contiguous()\n",
    "\n",
    "\n",
    "    \n",
    "# class TCN(nn.Module):\n",
    "#     def __init__(self, input_size, input_len, output_size, num_channels,\n",
    "#                  kernel_size=2, dropout=0.3, emb_dropout=0.1, tied_weights=False):\n",
    "#         super(TCN, self).__init__()\n",
    "#         # [N, 38, 14*24]->[N, 1, 14*24]\n",
    "#         # input_size=39, num_channels=[300, 200, 100, 50, 1],output_size=1*24\n",
    "#         self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "#         self.decoder = nn.Linear(input_len, output_size)\n",
    "#         self.drop = nn.Dropout(emb_dropout)\n",
    "#         self.emb_dropout = emb_dropout\n",
    "#         self.init_weights()\n",
    "#         self.linear = nn.Linear(input_len, output_size)\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         self.decoder.bias.data.fill_(0)\n",
    "#         self.decoder.weight.data.normal_(0, 0.01)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         \"\"\"Input ought to have dimension (N, C_in, L_in), \n",
    "#         where L_in is the seq_len; \n",
    "#         here the input is (N, L, C)\"\"\"\n",
    "#         # [N, 14*24, 38]->[N, 38, 14*24]\n",
    "#         y = input.transpose(1, 2)\n",
    "#         # [N, 38, 14*24]->[N, 1, 14*24]\n",
    "#         y = self.tcn(y)\n",
    "#         # [N, 1, 14*24]->[N, 1, 1*24]->[N, 1*24, 1]\n",
    "#         y = self.decoder(y).transpose(1, 2)\n",
    "#         # 使用linear的效果  [N, 14*24, 1]->[N, 1, 14*24]->[N, 1, 1*24]->[N, 1*24, 1]\n",
    "#         return y.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "standing-grant",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:46:24.343782Z",
     "iopub.status.busy": "2025-10-22T09:46:24.343529Z",
     "iopub.status.idle": "2025-10-22T09:46:24.370371Z",
     "shell.execute_reply": "2025-10-22T09:46:24.369635Z",
     "shell.execute_reply.started": "2025-10-22T09:46:24.343756Z"
    }
   },
   "outputs": [],
   "source": [
    "class StaticFeatureProcessor(nn.Module):\n",
    "    def __init__(self, static_dict, dropout=0.2, hidden_dims=[64, 32]):\n",
    "        \"\"\"\n",
    "        static_dict: dict, 定义静态特征的属性和维度\n",
    "                    格式: {feature_index: ('type', 'onehot/embedding', dimension)}\n",
    "                    例如: {\"feat1\": ('continuous', None, None), \"feat2\": ('categorical', 'onehot', 5), \"feat3\": ('categorical', 'embedding', 10)}\n",
    "        hidden_dims: list of int, MLP隐藏层维度\n",
    "        \"\"\"\n",
    "        super(StaticFeatureProcessor, self).__init__()\n",
    "        \n",
    "        self.static_dict = static_dict\n",
    "        self.num_features = len(static_dict)\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        # 解析static_dict，分离连续和类别特征\n",
    "        self.continuous_indices = []\n",
    "        self.categorical_onehot_indices = []\n",
    "        self.categorical_embedding_indices = []\n",
    "        self.cat_feature_dims = []\n",
    "        self.embedding_dims = {}\n",
    "        \n",
    "        # 按照特征名称排序处理\n",
    "        sorted_features = static_dict.keys()\n",
    "        # print(sorted_features)\n",
    "        self.feature_names = sorted_features\n",
    "        \n",
    "        # 为embedding特征创建embedding层\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        \n",
    "        for i, feat_name in enumerate(self.feature_names):\n",
    "            feature_type, structure_type, dimension = static_dict[feat_name]\n",
    "            \n",
    "            if feature_type == 'continuous':\n",
    "                self.continuous_indices.append(i)\n",
    "            elif feature_type == 'categorical':\n",
    "                if structure_type == 'onehot':\n",
    "                    self.categorical_onehot_indices.append(i)\n",
    "                    self.cat_feature_dims.append(dimension)  #预留\n",
    "                elif structure_type == 'embedding':\n",
    "                    self.categorical_embedding_indices.append(i)\n",
    "                    if dimension > 128:\n",
    "                        embedding_dim = 32\n",
    "                    else:\n",
    "                        embedding_dim = max(2, dimension // 4)  # 确保至少为2\n",
    "                    self.embeddings[feat_name] = nn.Embedding(dimension+10, embedding_dim) #预留\n",
    "                    self.embedding_dims[feat_name] = embedding_dim\n",
    "        \n",
    "        # 计算总的输入维度\n",
    "        self.total_onehot_dim = sum(self.cat_feature_dims)\n",
    "        self.total_embedding_dim = sum(self.embedding_dims.values())\n",
    "        self.num_continuous = len(self.continuous_indices)\n",
    "        \n",
    "#         # 对连续特征添加BatchNorm层\n",
    "#         if self.num_continuous > 0:\n",
    "#             self.continuous_bn = nn.BatchNorm1d(self.num_continuous)\n",
    "#         else:\n",
    "#             self.continuous_bn = None\n",
    "        self.continuous_bn = None\n",
    "        \n",
    "        # MLP输入维度 = one-hot维度 + embedding维度 + 连续特征维度\n",
    "        mlp_input_dim = self.total_onehot_dim + self.total_embedding_dim + self.num_continuous\n",
    "        # print(mlp_input_dim,self.total_onehot_dim,self.total_embedding_dim,self.num_continuous)\n",
    "        # 构建MLP\n",
    "        mlp_layers = []\n",
    "        input_dim = mlp_input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            mlp_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            mlp_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            mlp_layers.append(nn.Dropout(self.dropout_rate))\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        self.output_dim = hidden_dims[-1] if hidden_dims else mlp_input_dim\n",
    "        \n",
    "        \n",
    "    def safe_embedding_lookup(self, embedding_layer, indices, feature_name):\n",
    "        \"\"\"安全的embedding查找，处理索引越界\"\"\"\n",
    "        num_embeddings = embedding_layer.num_embeddings\n",
    "        \n",
    "        # 检查是否有越界索引\n",
    "        out_of_bounds_mask = indices >= num_embeddings\n",
    "        if out_of_bounds_mask.any():\n",
    "            print(f\"警告: 特征 {feature_name} 发现 {out_of_bounds_mask.sum().item()} 个越界索引\")\n",
    "            print(f\"最大索引: {indices.max().item()}, embedding大小: {num_embeddings}\")\n",
    "            \n",
    "            # 将越界索引映射到最后一个位置\n",
    "            safe_indices = torch.where(\n",
    "                indices < num_embeddings,\n",
    "                indices,\n",
    "                torch.tensor(num_embeddings - 1, device=indices.device)\n",
    "            )\n",
    "            \n",
    "            # 记录被映射的索引\n",
    "            mapped_indices = indices[out_of_bounds_mask]\n",
    "            print(f\"被映射的索引示例: {mapped_indices[:10].cpu().numpy()}\")\n",
    "            \n",
    "            return embedding_layer(safe_indices)\n",
    "        else:\n",
    "            return embedding_layer(indices)\n",
    "    \n",
    "    def forward(self, static_input):\n",
    "        \"\"\"\n",
    "        static_input: tensor of shape (batch_size, num_features)\n",
    "                      按照static_dict中特征名称的顺序排列的特征\n",
    "        \"\"\"\n",
    "        # 处理输入形状\n",
    "        if static_input.dim() == 3:\n",
    "            if static_input.size(1) == 1:\n",
    "                static_input = static_input.squeeze(1)  # (batch_size, 1, num_features) -> (batch_size, num_features)\n",
    "            else:\n",
    "                raise ValueError(f\"Expected static_input shape (batch_size, num_features) or (batch_size, 1, num_features), but got {static_input.shape}\")\n",
    "        \n",
    "        batch_size = static_input.size(0)\n",
    "        \n",
    "        # 分离连续特征和类别特征\n",
    "        continuous_features = []\n",
    "        onehot_features = []\n",
    "        embedding_features = []\n",
    "        \n",
    "        # 按照特征名称顺序提取特征\n",
    "        for i, feat_name in enumerate(self.feature_names):\n",
    "            feature = static_input[:, i]\n",
    "            feature_type, structure_type, dimension = self.static_dict[feat_name]\n",
    "            \n",
    "            if feature_type == 'continuous':\n",
    "                continuous_features.append(feature.unsqueeze(1))\n",
    "            elif feature_type == 'categorical':\n",
    "                if structure_type == 'onehot':\n",
    "                    onehot_features.append((feature, dimension, i))\n",
    "                elif structure_type == 'embedding':\n",
    "                    embedding_features.append((feature, feat_name))\n",
    "        \n",
    "        # 处理连续特征\n",
    "        if continuous_features:\n",
    "            num_features = torch.cat(continuous_features, dim=1)\n",
    "            if self.continuous_bn is not None:\n",
    "                num_features = self.continuous_bn(num_features)\n",
    "        else:\n",
    "            num_features = None\n",
    "\n",
    "        # 处理one-hot类别特征\n",
    "        one_hot_embeddings = []\n",
    "        for feature_tensor, num_classes, idx in onehot_features:\n",
    "            feature_long = feature_tensor.long()\n",
    "            one_hot = functional.one_hot(feature_long, num_classes=num_classes)\n",
    "            one_hot_embeddings.append(one_hot.float())\n",
    "        \n",
    "\n",
    "        # 处理embedding类别特征 - 使用安全查找\n",
    "        embedding_vectors = []\n",
    "        for feature_tensor, feat_name in embedding_features:\n",
    "            feature_long = feature_tensor.long()\n",
    "            embedding_layer = self.embeddings[feat_name]\n",
    "            \n",
    "            embedding_vec = self.safe_embedding_lookup(embedding_layer, feature_long, feat_name)\n",
    "            embedding_vectors.append(embedding_vec)\n",
    "        \n",
    "        # 拼接所有特征\n",
    "        all_features = []\n",
    "        \n",
    "        # 拼接one-hot特征\n",
    "        if one_hot_embeddings:\n",
    "            cat_onehot_embeddings = torch.cat(one_hot_embeddings, dim=1)\n",
    "            all_features.append(cat_onehot_embeddings)\n",
    "        # print(cat_onehot_embeddings.shape)\n",
    "        # 拼接embedding特征\n",
    "        if embedding_vectors:\n",
    "            cat_embedding_embeddings = torch.cat(embedding_vectors, dim=1)\n",
    "            all_features.append(cat_embedding_embeddings)\n",
    "        # print(cat_embedding_embeddings.shape)\n",
    "\n",
    "        # 拼接连续特征\n",
    "        if num_features is not None:\n",
    "            all_features.append(num_features)\n",
    "        \n",
    "        if not all_features:\n",
    "            raise ValueError(\"No features available after processing\")\n",
    "        \n",
    "        # 拼接所有特征\n",
    "        features = torch.cat(all_features, dim=1)\n",
    "        # print(features.shape)\n",
    "        # 通过MLP\n",
    "        output = self.mlp(features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HybridTCN(nn.Module):\n",
    "    def __init__(self, input_size, input_len, output_sizes, num_channels,\n",
    "                 static_dict, static_hidden_dims=[16,8],\n",
    "                 kernel_size=2, dropout=0.3, static_dropout=0.2, tied_weights=False):\n",
    "        super(HybridTCN, self).__init__()\n",
    "        \n",
    "        self.output_sizes = output_sizes\n",
    "        self.time_tasks = list(output_sizes.keys())\n",
    "        \n",
    "        # 时间序列处理部分\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        \n",
    "        # 静态特征处理部分 - 使用static_dict\n",
    "        self.static_processor = StaticFeatureProcessor(\n",
    "            static_dict, static_dropout, static_hidden_dims\n",
    "        )\n",
    "        \n",
    "        # 修正：直接从static_processor获取输出维度\n",
    "        static_output_dim = self.static_processor.output_dim\n",
    "        \n",
    "        # 预测头，内置静态信息融合\n",
    "        self.prediction_heads = nn.ModuleDict()\n",
    "        for scale in self.time_tasks:\n",
    "            fusion_input_dim = input_len + static_output_dim\n",
    "            self.prediction_heads[str(scale)] = nn.Sequential(\n",
    "                nn.Linear(fusion_input_dim, fusion_input_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(fusion_input_dim // 2, output_sizes[scale])\n",
    "            )\n",
    "        \n",
    "        self.drop = nn.Dropout(static_dropout)\n",
    "\n",
    "    def forward(self, time_series_input, static_input):\n",
    "        \"\"\"\n",
    "        time_series_input: tensor of shape (batch_size, seq_len, input_size)\n",
    "        static_input: tensor of shape (batch_size, num_static_features)\n",
    "                      按照static_dict中特征名称的顺序排列\n",
    "        \"\"\"\n",
    "        # 处理时间序列\n",
    "        y = time_series_input.transpose(1, 2)  # (N, time_fea, 14*24)\n",
    "        y = self.tcn(y)  # (N, 3, 14*24)\n",
    "        \n",
    "        # 处理静态特征\n",
    "        static_features = self.static_processor(static_input)  # (N, static_output_dim)\n",
    "        static_features = torch.unsqueeze(static_features, 1)  # (N, 1, static_output_dim)\n",
    "        \n",
    "        # 为每个时间粒度生成预测\n",
    "        predictions = {}\n",
    "        for scale in self.time_tasks:\n",
    "            # 获取该时间粒度的特征\n",
    "            scale_features = y[:, scale:scale+1, :]  # (N, 1, 14*24)\n",
    "            \n",
    "            # 拼接特征\n",
    "            fused_features = torch.cat([static_features, scale_features], dim=2)  # (N, 1, static_output_dim+14*24)\n",
    "            \n",
    "            # 通过预测头\n",
    "            predictions[scale] = self.prediction_heads[str(scale)](fused_features).transpose(1, 2).contiguous()\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-southeast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "willing-disorder",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:46:24.371524Z",
     "iopub.status.busy": "2025-10-22T09:46:24.371284Z",
     "iopub.status.idle": "2025-10-22T09:46:24.379384Z",
     "shell.execute_reply": "2025-10-22T09:46:24.378804Z",
     "shell.execute_reply.started": "2025-10-22T09:46:24.371497Z"
    }
   },
   "outputs": [],
   "source": [
    "class PeakHuberLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PeakHuberLoss, self).__init__()\n",
    "    def forward(self, y_pred, y_true, delta = 5):\n",
    "        error = y_true - y_pred\n",
    "        peak_mask = (y_true >= 5)\n",
    "        # 峰值用Huber Loss，非峰值用MAE\n",
    "        peak_loss = torch.where(torch.abs(error[peak_mask]) <= delta, \n",
    "                               0.5 * error[peak_mask]**2, \n",
    "                               delta * (torch.abs(error[peak_mask]) - 0.5 * delta)).mean() if torch.any(peak_mask) else 0.0\n",
    "        non_peak_loss = torch.abs(error[~peak_mask]).mean() if torch.any(~peak_mask) else 0.0\n",
    "        return peak_loss * 2 + non_peak_loss  # 峰值损失权重加倍\n",
    "    \n",
    "\n",
    "class MultiTaskPHLoss(nn.Module):\n",
    "    def __init__(self, loss_weights=None):\n",
    "        super(MultiTaskPHLoss, self).__init__()\n",
    "        self.peakhuberloss = PeakHuberLoss()\n",
    "        self.loss_weights = loss_weights\n",
    "    \n",
    "    def forward(self, predictions, targets, delta = 5):\n",
    "        total_loss = 0\n",
    "        losses = {}\n",
    "        \n",
    "        for scale, pred in predictions.items():\n",
    "            target = targets[scale]\n",
    "            scale_loss = self.peakhuberloss(pred, target, delta = delta)\n",
    "            \n",
    "            # 应用权重（如果有）\n",
    "            weight = self.loss_weights[scale] if self.loss_weights else 1.0\n",
    "            weighted_loss = weight * scale_loss\n",
    "            \n",
    "            losses[scale] = scale_loss.item()\n",
    "            total_loss += weighted_loss\n",
    "        \n",
    "        return total_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "green-waterproof",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:13:37.617052Z",
     "iopub.status.busy": "2025-10-21T08:13:37.616595Z",
     "iopub.status.idle": "2025-10-21T08:13:37.704211Z",
     "shell.execute_reply": "2025-10-21T08:13:37.703195Z",
     "shell.execute_reply.started": "2025-10-21T08:13:37.617011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "starting-wright",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:46:24.380483Z",
     "iopub.status.busy": "2025-10-22T09:46:24.380250Z",
     "iopub.status.idle": "2025-10-22T09:53:47.738164Z",
     "shell.execute_reply": "2025-10-22T09:53:47.737220Z",
     "shell.execute_reply.started": "2025-10-22T09:46:24.380459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "(113468, 1008, 22)\n",
      "(113468, 26, 7)\n",
      "单个样本数量： 18\n",
      "站点数量： 113468\n",
      "总样本数量： 2042424\n",
      "a (113468, 840, 22) (113468, 840, 3)\n",
      "单个样本数量： 4\n",
      "站点数量： 113468\n",
      "总样本数量： 453872\n",
      "a (113468, 504, 22) (113468, 504, 3)\n",
      "单个样本数量： 4\n",
      "站点数量： 113468\n",
      "总样本数量： 453872\n",
      "a (113468, 504, 22) (113468, 504, 3)\n"
     ]
    }
   ],
   "source": [
    "#### setup_seed(12345)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "output_sizes = {0: 24, 1: 24, 2: 24}\n",
    "\n",
    "# device = 'cpu'\n",
    "print('device:', device)\n",
    "print(hourly_all_data.shape)\n",
    "print(static_all_data.shape)\n",
    "\n",
    "# 加载数据\n",
    "train_dataloader, val_dataloader, test_dataloader = load_multitask_data(hourly_all_data[:, :, :], static_all_data[:,:,:], 1024, output_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chronic-mechanics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:47.740384Z",
     "iopub.status.busy": "2025-10-22T09:53:47.740113Z",
     "iopub.status.idle": "2025-10-22T09:53:47.749077Z",
     "shell.execute_reply": "2025-10-22T09:53:47.748346Z",
     "shell.execute_reply.started": "2025-10-22T09:53:47.740355Z"
    }
   },
   "outputs": [],
   "source": [
    "static_dict = { \"lag7d_order_cnt\": ('continuous',None,None),\n",
    "                \"parking_capacity\": ('continuous',None,None),\n",
    "                \"temperature_avg_val\": ('continuous',None,None),\n",
    "                \"city_guid_encoded\": ('categorical','embedding',max_city+1),\n",
    "                \"day_of_week\": ('categorical','onehot',8),\n",
    "                \"workday_level\": ('categorical','onehot',3),\n",
    "                \"cycle_weather_level\": ('categorical','embedding',4)\n",
    "              }\n",
    "# 保存到 JSON 文件\n",
    "def save_static_dict(static_dict, file_path):\n",
    "    # 将元组转换为列表以便JSON序列化\n",
    "    serializable_dict = {}\n",
    "    for key, value in static_dict.items():\n",
    "        serializable_dict[key] = list(value) if value[1] is not None else [value[0], None, value[2]]\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "save_static_dict(static_dict, 'ebik_dataset/static_dict_config_3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "german-monitoring",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:34:24.036535Z",
     "iopub.status.busy": "2025-10-20T09:34:24.036136Z",
     "iopub.status.idle": "2025-10-20T09:34:24.040919Z",
     "shell.execute_reply": "2025-10-20T09:34:24.040105Z",
     "shell.execute_reply.started": "2025-10-20T09:34:24.036502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city_guid_encoded', 'cycle_weather_level', 'day_of_week', 'lag7d_order_cnt', 'parking_capacity', 'temperature_avg_val', 'workday_level']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(static_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exotic-steering",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:47.750434Z",
     "iopub.status.busy": "2025-10-22T09:53:47.750191Z",
     "iopub.status.idle": "2025-10-22T09:53:53.751264Z",
     "shell.execute_reply": "2025-10-22T09:53:53.749729Z",
     "shell.execute_reply.started": "2025-10-22T09:53:47.750409Z"
    }
   },
   "outputs": [],
   "source": [
    "model_save_path = 'ebik_model/net_ebik_splitower_7.pth'\n",
    "output_sizes = {0: 24, 1: 24, 2: 24}  # 10/16/21终点预测，输出对应长度\n",
    "\n",
    "num_channels = [64, 128, 32, 3]  # TCN隐藏层维度\n",
    "\n",
    "# 训练模型\n",
    "lr = 0.001\n",
    "# loss_weights = {10: 1.0, 16: 1.0, 21: 1.0}  # 更长期的预测给予更高权重    \n",
    "es_cnt = 0\n",
    "max_es_epoch = 15\n",
    "min_val_loss = float('inf')\n",
    "epoches = 70\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "tcn_model = HybridTCN(input_size=22, input_len=14*24, output_sizes=output_sizes, num_channels=num_channels, \n",
    "                         static_dict=static_dict, static_hidden_dims=[32, 16],\n",
    "                         kernel_size=3, dropout=0.25, static_dropout=0.2, tied_weights=False).to(device)\n",
    "criterion = MultiTaskPHLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    tcn_model.parameters(),\n",
    "    lr=lr,                    # 学习率\n",
    "    betas=(0.9, 0.999),         # 动量参数\n",
    "    eps=1e-8,                   # 数值稳定性\n",
    "    weight_decay=1e-2,          # 权重衰减\n",
    "    amsgrad=False               # 是否使用 AMSGrad 变体\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "earlier-jefferson",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T09:53:53.753243Z",
     "iopub.status.busy": "2025-10-22T09:53:53.752914Z",
     "iopub.status.idle": "2025-10-22T12:36:34.593336Z",
     "shell.execute_reply": "2025-10-22T12:36:34.592535Z",
     "shell.execute_reply.started": "2025-10-22T09:53:53.753211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0, Train Loss:92.601941, Val Loss:76.477463\n",
      "Epoch=2, Train Loss:83.546698, Val Loss:75.811064\n",
      "Epoch=4, Train Loss:82.819341, Val Loss:75.489043\n",
      "Epoch=6, Train Loss:81.908659, Val Loss:74.622880\n",
      "Epoch=8, Train Loss:81.304820, Val Loss:75.485731\n",
      "Epoch=10, Train Loss:80.718140, Val Loss:75.097236\n",
      "Epoch=12, Train Loss:80.226729, Val Loss:75.081207\n",
      "Epoch=14, Train Loss:79.919971, Val Loss:74.948944\n",
      "Epoch=16, Train Loss:79.482328, Val Loss:74.746026\n",
      "Epoch=18, Train Loss:79.086415, Val Loss:72.854394\n",
      "Epoch=20, Train Loss:78.791865, Val Loss:73.771309\n",
      "Epoch=22, Train Loss:78.518774, Val Loss:77.722579\n",
      "Epoch=24, Train Loss:77.997797, Val Loss:73.622288\n",
      "Epoch=26, Train Loss:77.874087, Val Loss:74.371407\n",
      "Epoch=28, Train Loss:77.341410, Val Loss:75.251709\n",
      "Epoch=30, Train Loss:77.155854, Val Loss:73.531235\n",
      "Epoch=32, Train Loss:76.907828, Val Loss:73.941101\n",
      "Epoch=34, Train Loss:76.590003, Val Loss:74.243076\n",
      "Epoch=36, Train Loss:76.304098, Val Loss:74.158977\n",
      "Epoch=38, Train Loss:75.966438, Val Loss:75.836747\n",
      "Epoch=40, Train Loss:75.704142, Val Loss:74.638635\n",
      "Epoch=42, Train Loss:75.335599, Val Loss:74.343509\n",
      "Epoch=44, Train Loss:75.269285, Val Loss:74.603606\n",
      "Epoch=46, Train Loss:74.886851, Val Loss:74.097642\n",
      "Epoch=48, Train Loss:74.747266, Val Loss:74.292664\n",
      "触发早停机制！\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoches + 1):\n",
    "    tcn_model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    # 修正的数据遍历 - 参数顺序与数据集返回顺序一致\n",
    "    for time_data, static_data, all_labels in train_dataloader:\n",
    "        time_data, static_data = time_data.to(device), static_data.to(device)\n",
    "\n",
    "        # 标签移动到设备\n",
    "        labels_on_device = {}\n",
    "        for target_idx, label in all_labels.items():\n",
    "            labels_on_device[target_idx] = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 模型前向传播 - 参数顺序正确\n",
    "        forecast = tcn_model(time_data, static_data)  # [batch_size, output_size, num_targets]\n",
    "\n",
    "        loss, task_losses = criterion(forecast, labels_on_device)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"训练损失为NaN!\")\n",
    "            break\n",
    "\n",
    "        train_losses.append(loss.detach().item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 计算平均训练损失\n",
    "    train_loss_avg = sum(train_losses) / len(train_losses) if train_losses else 0\n",
    "\n",
    "    # 验证循环\n",
    "    if e % 2 == 0:  # 每2个epoch验证一次\n",
    "        tcn_model.eval()\n",
    "        val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_time_data, val_static_data, val_labels in val_dataloader:\n",
    "                val_time_data, val_static_data = val_time_data.to(device), val_static_data.to(device)\n",
    "\n",
    "                val_labels_on_device = {}\n",
    "                for target_idx, label in val_labels.items():\n",
    "                    val_labels_on_device[target_idx] = label.to(device)\n",
    "\n",
    "                val_forecast = tcn_model(val_time_data, val_static_data)\n",
    "                val_loss, val_task_losses = criterion(val_forecast, val_labels_on_device)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        val_loss_avg = sum(val_losses) / len(val_losses) if val_losses else 0\n",
    "\n",
    "        print(f'Epoch={e}, Train Loss:{train_loss_avg:.6f}, Val Loss:{val_loss_avg:.6f}')\n",
    "\n",
    "        # 早停机制\n",
    "        if val_loss_avg < min_val_loss:\n",
    "            min_val_loss = val_loss_avg\n",
    "            es_cnt = 0\n",
    "            # 这里可以保存模型\n",
    "            torch.save(tcn_model.state_dict(), model_save_path)\n",
    "        else:\n",
    "            es_cnt += 1\n",
    "            if es_cnt >= max_es_epoch:\n",
    "                print('触发早停机制！')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "complimentary-webmaster",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T13:43:23.338220Z",
     "iopub.status.busy": "2025-10-22T13:43:23.337829Z",
     "iopub.status.idle": "2025-10-22T13:59:43.729216Z",
     "shell.execute_reply": "2025-10-22T13:59:43.728337Z",
     "shell.execute_reply.started": "2025-10-22T13:43:23.338185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 38.096264\n",
      "Target 0 Loss: 12.169564\n",
      "Target 1 Loss: 12.576924\n",
      "Target 2 Loss: 13.349776\n"
     ]
    }
   ],
   "source": [
    "output_sizes = {0: 24, 1: 24, 2: 24}  # 3/6/12小时预测，输出对应长度\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "cat_feature_dims = [4,8]  #类别变量类别数\n",
    "num_channels = [64, 128, 32, 3]  # TCN隐藏层维度\n",
    "criterion = MultiTaskPHLoss()\n",
    "# 加载模型，预测\n",
    "tcn_save_model = HybridTCN(input_size=22, input_len=14*24, output_sizes=output_sizes, num_channels=num_channels, \n",
    "                         static_dict=static_dict, static_hidden_dims=[32, 16],\n",
    "                         kernel_size=3, dropout=0.25, static_dropout=0.2, tied_weights=False).to(device)\n",
    "# batch*length*size 输入， batch = 32个点位，len = 7天*24小时，size = 8个特征 \n",
    "tcn_save_model.load_state_dict(torch.load('ebik_model/net_ebik_splitower_7.pth'))\n",
    "\n",
    "# 初始化存储\n",
    "test_losses = []\n",
    "test_task_losses_dict = {target_idx: [] for target_idx in output_sizes.keys()}\n",
    "true_values = {target_idx: [] for target_idx in output_sizes.keys()}\n",
    "pred_values = {target_idx: [] for target_idx in output_sizes.keys()}\n",
    "\n",
    "# 测试循环\n",
    "tcn_save_model.eval()\n",
    "with torch.no_grad():\n",
    "    for test_time_data, test_static_data, test_labels in test_dataloader:\n",
    "        test_time_data, test_static_data = test_time_data.to(device), test_static_data.to(device)\n",
    "        \n",
    "        # 将每个目标的标签移动到设备\n",
    "        test_labels_on_device = {}\n",
    "        for target_idx, label in test_labels.items():\n",
    "            test_labels_on_device[target_idx] = label.to(device)\n",
    "        \n",
    "        # 标准化输入数据（如果需要）\n",
    "        # test_data, mean, std = transform_series2(test_data)\n",
    "        \n",
    "        # 前向传播\n",
    "        test_forecasts = tcn_save_model(test_time_data, test_static_data)\n",
    "        \n",
    "        # 反标准化预测结果（如果需要）\n",
    "        # for target_idx, forecast in test_forecasts.items():\n",
    "        #     test_forecasts[target_idx] = transform_series2_decode(forecast, mean, std)\n",
    "        \n",
    "        # 计算多目标损失\n",
    "        test_loss, test_task_losses = criterion(test_forecasts, test_labels_on_device)\n",
    "        \n",
    "        # 存储损失\n",
    "        test_losses.append(test_loss.item())\n",
    "        for target_idx, loss_val in test_task_losses.items():\n",
    "            test_task_losses_dict[target_idx].append(loss_val)\n",
    "        \n",
    "        # 存储真实值和预测值\n",
    "        for target_idx in output_sizes.keys():\n",
    "            true_values[target_idx].append(test_labels[target_idx].cpu().numpy())\n",
    "            pred_values[target_idx].append(test_forecasts[target_idx].cpu().numpy())\n",
    "\n",
    "# 计算平均损失\n",
    "test_loss_avg = sum(test_losses) / len(test_losses) if test_losses else 0\n",
    "print(f'Test Loss: {test_loss_avg:.6f}')\n",
    "\n",
    "# 计算每个目标的平均损失\n",
    "for target_idx in output_sizes.keys():\n",
    "    target_loss_avg = sum(test_task_losses_dict[target_idx]) / len(test_task_losses_dict[target_idx])\n",
    "    print(f'Target {target_idx} Loss: {target_loss_avg:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "emotional-frank",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T02:40:43.186587Z",
     "iopub.status.busy": "2025-10-20T02:40:43.186247Z",
     "iopub.status.idle": "2025-10-20T02:40:43.191903Z",
     "shell.execute_reply": "2025-10-20T02:40:43.191148Z",
     "shell.execute_reply.started": "2025-10-20T02:40:43.186558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "virgin-basement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T02:45:19.610725Z",
     "iopub.status.busy": "2025-10-23T02:45:19.610319Z",
     "iopub.status.idle": "2025-10-23T02:45:24.981108Z",
     "shell.execute_reply": "2025-10-23T02:45:24.980179Z",
     "shell.execute_reply.started": "2025-10-23T02:45:19.610687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113468, 4, 24)\n",
      "\n",
      "=== 目标 0 MAPE分析 ===\n",
      "Day 1: MAPE = 0.3317, WMAPE = 0.3416\n",
      "Day 1 Morning MAPE = 0.3666, WMAPE = 0.3798\n",
      "Day 1 Afternoon MAPE = 0.3504, WMAPE = 0.3547\n",
      "Day 1 Evening MAPE = 0.3238, WMAPE = 0.3475\n",
      "Day 2: MAPE = 0.3269, WMAPE = 0.3441\n",
      "Day 2 Morning MAPE = 0.3456, WMAPE = 0.3631\n",
      "Day 2 Afternoon MAPE = 0.3520, WMAPE = 0.3587\n",
      "Day 2 Evening MAPE = 0.3221, WMAPE = 0.3556\n",
      "Day 3: MAPE = 0.3233, WMAPE = 0.3572\n",
      "Day 3 Morning MAPE = 0.3309, WMAPE = 0.3620\n",
      "Day 3 Afternoon MAPE = 0.3415, WMAPE = 0.3669\n",
      "Day 3 Evening MAPE = 0.3192, WMAPE = 0.3686\n",
      "Day 4: MAPE = 0.3109, WMAPE = 0.3461\n",
      "Day 4 Morning MAPE = 0.3334, WMAPE = 0.3886\n",
      "Day 4 Afternoon MAPE = 0.3308, WMAPE = 0.3549\n",
      "Day 4 Evening MAPE = 0.3107, WMAPE = 0.3446\n",
      "目标 0 平均MAPE: 0.3232, 平均WMAPE: 0.3472\n",
      "目标 0 早峰平均MAPE: 0.3441, 平均WMAPE: 0.3734\n",
      "目标 0 午峰平均MAPE: 0.3437, 平均WMAPE: 0.3588\n",
      "目标 0 晚峰平均MAPE: 0.3189, 平均WMAPE: 0.3541\n",
      "(113468, 4, 24)\n",
      "\n",
      "=== 目标 1 MAPE分析 ===\n",
      "Day 1: MAPE = 0.3242, WMAPE = 0.3422\n",
      "Day 1 Morning MAPE = 0.3412, WMAPE = 0.3564\n",
      "Day 1 Afternoon MAPE = 0.3917, WMAPE = 0.4180\n",
      "Day 1 Evening MAPE = 0.3247, WMAPE = 0.3526\n",
      "Day 2: MAPE = 0.3203, WMAPE = 0.3432\n",
      "Day 2 Morning MAPE = 0.3359, WMAPE = 0.3514\n",
      "Day 2 Afternoon MAPE = 0.3905, WMAPE = 0.4166\n",
      "Day 2 Evening MAPE = 0.3153, WMAPE = 0.3540\n",
      "Day 3: MAPE = 0.3198, WMAPE = 0.3510\n",
      "Day 3 Morning MAPE = 0.3215, WMAPE = 0.3497\n",
      "Day 3 Afternoon MAPE = 0.3900, WMAPE = 0.4094\n",
      "Day 3 Evening MAPE = 0.3165, WMAPE = 0.3646\n",
      "Day 4: MAPE = 0.3066, WMAPE = 0.3473\n",
      "Day 4 Morning MAPE = 0.3235, WMAPE = 0.3680\n",
      "Day 4 Afternoon MAPE = 0.3945, WMAPE = 0.4278\n",
      "Day 4 Evening MAPE = 0.3007, WMAPE = 0.3440\n",
      "目标 1 平均MAPE: 0.3177, 平均WMAPE: 0.3459\n",
      "目标 1 早峰平均MAPE: 0.3305, 平均WMAPE: 0.3564\n",
      "目标 1 午峰平均MAPE: 0.3917, 平均WMAPE: 0.4180\n",
      "目标 1 晚峰平均MAPE: 0.3143, 平均WMAPE: 0.3538\n",
      "(113468, 4, 24)\n",
      "\n",
      "=== 目标 2 MAPE分析 ===\n",
      "Day 1: MAPE = 0.3289, WMAPE = 0.3459\n",
      "Day 1 Morning MAPE = 0.3471, WMAPE = 0.3596\n",
      "Day 1 Afternoon MAPE = 0.3342, WMAPE = 0.3527\n",
      "Day 1 Evening MAPE = 0.4242, WMAPE = 0.4343\n",
      "Day 2: MAPE = 0.3286, WMAPE = 0.3521\n",
      "Day 2 Morning MAPE = 0.3388, WMAPE = 0.3588\n",
      "Day 2 Afternoon MAPE = 0.3383, WMAPE = 0.3662\n",
      "Day 2 Evening MAPE = 0.4248, WMAPE = 0.4438\n",
      "Day 3: MAPE = 0.3277, WMAPE = 0.3583\n",
      "Day 3 Morning MAPE = 0.3293, WMAPE = 0.3610\n",
      "Day 3 Afternoon MAPE = 0.3396, WMAPE = 0.3742\n",
      "Day 3 Evening MAPE = 0.4287, WMAPE = 0.4506\n",
      "Day 4: MAPE = 0.3111, WMAPE = 0.3513\n",
      "Day 4 Morning MAPE = 0.3144, WMAPE = 0.3568\n",
      "Day 4 Afternoon MAPE = 0.3289, WMAPE = 0.3690\n",
      "Day 4 Evening MAPE = 0.4339, WMAPE = 0.4464\n",
      "目标 2 平均MAPE: 0.3241, 平均WMAPE: 0.3519\n",
      "目标 2 早峰平均MAPE: 0.3324, 平均WMAPE: 0.3591\n",
      "目标 2 午峰平均MAPE: 0.3352, 平均WMAPE: 0.3655\n",
      "目标 2 晚峰平均MAPE: 0.4279, 平均WMAPE: 0.4438\n"
     ]
    }
   ],
   "source": [
    "# 计算每个目标的MAPE（按天计算）\n",
    "for target_idx in output_sizes.keys():\n",
    "    # 合并所有批次的预测和真实值\n",
    "    lth = len(true_values[target_idx])\n",
    "    all_true = np.array(true_values[target_idx])\n",
    "    all_true = np.reshape(all_true, (lth, 4, 24))\n",
    "    all_pred = np.array(pred_values[target_idx])\n",
    "    all_pred = np.reshape(all_pred, (lth, 4, 24))\n",
    "    print(all_true.shape)\n",
    "    \n",
    "    print(f\"\\n=== 目标 {target_idx} MAPE分析 ===\")\n",
    "    \n",
    "    # 计算每天的MAPE（只考虑真值>=5的点）\n",
    "    daily_mapes = []\n",
    "    morning_mapes = []\n",
    "    afternoon_mapes = []\n",
    "    evening_mapes = []\n",
    "    daily_wapes = []  # 新增：存储每天的WMAPE\n",
    "    morning_wapes = []  # 新增：存储每天早峰的WMAPE\n",
    "    afternoon_wapes = []  # 新增：存储每天午峰的WMAPE\n",
    "    evening_wapes = []  # 新增：存储每天晚峰的WMAPE\n",
    "    \n",
    "    for i in range(all_pred.shape[1]):\n",
    "        sub_day_pred = all_pred[:, i,:].round()\n",
    "        sub_day_true = all_true[:, i,:]\n",
    "        \n",
    "        # 只考虑真值>=5的点\n",
    "        where_res = np.where(sub_day_true>=5)\n",
    "        sub_day_true1 = sub_day_true[where_res]\n",
    "        sub_day_pred1 = sub_day_pred[where_res]\n",
    "        \n",
    "        # 计算MAPE\n",
    "        mape = np.mean(np.abs((sub_day_pred1 - sub_day_true1) / sub_day_true1))\n",
    "        daily_mapes.append(mape)\n",
    "        \n",
    "        # 计算WMAPE (新增)\n",
    "        wape = np.sum(np.abs(sub_day_pred1 - sub_day_true1)) / np.sum(sub_day_true1)\n",
    "        daily_wapes.append(wape)\n",
    "        \n",
    "        print(f\"Day {i+1}: MAPE = {mape:.4f}, WAPE = {wape:.4f}\")\n",
    "        \n",
    "        # 早峰时段 (9时)\n",
    "        mor_day_pred = all_pred[:, i, 8:9].round()\n",
    "        mor_day_true = all_true[:, i, 8:9]\n",
    "        where_resmon = np.where(mor_day_true>=5)\n",
    "        mor_day_true1 = mor_day_true[where_resmon]\n",
    "        mor_day_pred1 = mor_day_pred[where_resmon]\n",
    "        \n",
    "        mor_mape = np.mean(np.abs((mor_day_pred1 - mor_day_true1) / mor_day_true1))\n",
    "        morning_mapes.append(mor_mape)\n",
    "        \n",
    "        # 早峰WMAPE (新增)\n",
    "        mor_wape = np.sum(np.abs(mor_day_pred1 - mor_day_true1)) / np.sum(mor_day_true1)\n",
    "        morning_wmapes.append(mor_wape)\n",
    "        \n",
    "        print(f\"Day {i+1} Morning MAPE = {mor_mape:.4f}, WAPE = {mor_wape:.4f}\")\n",
    "        \n",
    "        # 午峰时段 (14时)\n",
    "        aft_day_pred = all_pred[:, i, 13:14].round()\n",
    "        aft_day_true = all_true[:, i, 13:14]\n",
    "        where_resaft = np.where(aft_day_true>=5)\n",
    "        aft_day_true1 = aft_day_true[where_resaft]\n",
    "        aft_day_pred1 = aft_day_pred[where_resaft]\n",
    "        \n",
    "        aft_mape = np.mean(np.abs((aft_day_pred1 - aft_day_true1) / aft_day_true1))\n",
    "        afternoon_mapes.append(aft_mape)\n",
    "        \n",
    "        # 午峰WMAPE (新增)\n",
    "        aft_wape = np.sum(np.abs(aft_day_pred1 - aft_day_true1)) / np.sum(aft_day_true1)\n",
    "        afternoon_wapes.append(aft_wape)\n",
    "        \n",
    "        print(f\"Day {i+1} Afternoon MAPE = {aft_mape:.4f}, WAPE = {aft_wmape:.4f}\")\n",
    "        \n",
    "        # 晚峰时段 (20时)\n",
    "        eve_day_pred = all_pred[:, i, 19:20].round()\n",
    "        eve_day_true = all_true[:, i, 19:20]\n",
    "        where_reseve = np.where(eve_day_true>=5)\n",
    "        eve_day_true1 = eve_day_true[where_reseve]\n",
    "        eve_day_pred1 = eve_day_pred[where_reseve]\n",
    "        \n",
    "        eve_mape = np.mean(np.abs((eve_day_pred1 - eve_day_true1) / eve_day_true1))\n",
    "        evening_mapes.append(eve_mape)\n",
    "        \n",
    "        # 晚峰WMAPE (新增)\n",
    "        eve_wape = np.sum(np.abs(eve_day_pred1 - eve_day_true1)) / np.sum(eve_day_true1)\n",
    "        evening_wapes.append(eve_wape)\n",
    "        \n",
    "        print(f\"Day {i+1} Evening MAPE = {eve_mape:.4f}, WAPE = {eve_wape:.4f}\")\n",
    "    \n",
    "    # 计算平均MAPE和WMAPE\n",
    "    if daily_mapes:\n",
    "        avg_mape = np.mean(daily_mapes)\n",
    "        avg_wape = np.mean(daily_wapes)  # 新增\n",
    "        print(f\"目标 {target_idx} 平均MAPE: {avg_mape:.4f}, 平均WAPE: {avg_wmape:.4f}\")\n",
    "    if morning_mapes:\n",
    "        monavg_mape = np.mean(morning_mapes)\n",
    "        monavg_wape = np.mean(morning_wapes)  # 新增\n",
    "        print(f\"目标 {target_idx} 早峰平均MAPE: {monavg_mape:.4f}, 平均WAPE: {monavg_wape:.4f}\")\n",
    "    if afternoon_mapes:\n",
    "        aftavg_mape = np.mean(afternoon_mapes)\n",
    "        aftavg_wape = np.mean(afternoon_wapes)  # 新增\n",
    "        print(f\"目标 {target_idx} 午峰平均MAPE: {aftavg_mape:.4f}, 平均WAPE: {aftavg_wape:.4f}\")\n",
    "    if evening_mapes:\n",
    "        eveavg_mape = np.mean(evening_mapes)\n",
    "        eveavg_wape = np.mean(evening_wapes)  # 新增\n",
    "        print(f\"目标 {target_idx} 晚峰平均MAPE: {eveavg_mape:.4f}, 平均WAPE: {eveavg_wape:.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-journalism",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
