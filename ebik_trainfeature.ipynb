{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cardiovascular-cancellation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:18:50.776246Z",
     "iopub.status.busy": "2025-10-22T08:18:50.775860Z",
     "iopub.status.idle": "2025-10-22T08:18:50.789737Z",
     "shell.execute_reply": "2025-10-22T08:18:50.788695Z",
     "shell.execute_reply.started": "2025-10-22T08:18:50.776212Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import avg, from_json, to_json\n",
    "from pyspark.sql.functions import col, concat, lit, explode, array, struct\n",
    "from pyspark.sql.types import StringType, ArrayType,IntegerType, FloatType,StructType,StructField,BooleanType, DateType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from aibrain_common.utils.date_convert_utils import DateConvertUtils\n",
    "from aibrain_common.component import tools\n",
    "import uuid\n",
    "from aibrain_common.data.dataset_builder import DatasetBuilder\n",
    "from aibrain_common.utils import env_utils\n",
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "\n",
    "# %matplotlib notebook\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "        config('spark.executor.memory', '12g').\\\n",
    "        config('spark.executor.cores', '6').\\\n",
    "        config('spark.driver.memory','10g').\\\n",
    "        config('spark.executor.instances', '10').\\\n",
    "        config('spark.driver.maxResultSize', '50000m').\\\n",
    "        appName('ebiktrainfeature').\\\n",
    "        enableHiveSupport().getOrCreate()\n",
    "\n",
    "\n",
    "time_str_formats = {\n",
    "    \"hour\": \"%Y%m%d%H\",\n",
    "    \"day\": \"%Y%m%d\",\n",
    "}\n",
    "\n",
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tracked-sympathy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:18:50.791345Z",
     "iopub.status.busy": "2025-10-22T08:18:50.791092Z",
     "iopub.status.idle": "2025-10-22T08:18:50.796056Z",
     "shell.execute_reply": "2025-10-22T08:18:50.795467Z",
     "shell.execute_reply.started": "2025-10-22T08:18:50.791320Z"
    }
   },
   "outputs": [],
   "source": [
    "def datetime2str(date: datetime, rtype=\"hour\"):\n",
    "    if rtype not in time_str_formats:\n",
    "        raise ValueError(\"rtype Error!\")\n",
    "    else:\n",
    "        return date.strftime(time_str_formats[rtype])\n",
    "\n",
    "\n",
    "def str2datetime(s, itype=\"hour\"):\n",
    "    return datetime.strptime(s, time_str_formats[itype])\n",
    "def add_delta(time_str: str, delta: dict, itype=\"day\", rtype=\"day\"):\n",
    "    target_time = str2datetime(time_str, itype) + timedelta(**delta)\n",
    "    target_time = datetime2str(target_time, rtype)\n",
    "    return target_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "norwegian-password",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:18:50.797728Z",
     "iopub.status.busy": "2025-10-22T08:18:50.797407Z",
     "iopub.status.idle": "2025-10-22T08:18:50.805952Z",
     "shell.execute_reply": "2025-10-22T08:18:50.805345Z",
     "shell.execute_reply.started": "2025-10-22T08:18:50.797697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today is :20250928\n"
     ]
    }
   ],
   "source": [
    "today = '20251020'  #测试阶段预测已知的值\n",
    "\n",
    "# today = DateConvertUtils().parse_data_date('${yyyymmdd}')\n",
    "\n",
    "print(\"today is :%s\" % today)\n",
    "\n",
    "pt = today # 运行当天t\n",
    "week = 6 # 训练数据拉取周数\n",
    "tomorrow = add_delta(today, {'days': 1}, \"day\", \"day\") # 预测的日期（t+1）\n",
    "yesterday = add_delta(today, {'days': -1}, \"day\", \"day\") # 前一天，用于找大点\n",
    "\n",
    "\n",
    "day1 = - (week * 7 - 1) # 差值为13，获取14天的数据特征\n",
    "end_date = today  # 能够获取到的最新的真值的日期为t-2(流入流出特征需要两天真值来计算)\n",
    "start_date = add_delta(end_date, {'days': day1}, \"day\", \"day\")  # 预测需要用到前14天特征，\n",
    "twoweek_ago_date = add_delta(end_date, {'days': - 13}, \"day\", \"day\") \n",
    "# start_date = '20250708' \n",
    "start_date_minus11 = (datetime.strptime(start_date, '%Y%m%d') + timedelta(days=-11)).date().strftime('%Y%m%d')  #还需要+11天得到lag14日特征(当前为t-3)\n",
    "start_date_2 = (datetime.strptime(start_date, '%Y%m%d') + timedelta(days=-2)).date().strftime('%Y%m%d')\n",
    "start_date_add2 = (datetime.strptime(start_date, '%Y%m%d') + timedelta(days=2)).date().strftime('%Y%m%d')\n",
    "# start_date_add1 = (datetime.strptime(start_date, '%Y%m%d') + timedelta(days=1)).date().strftime('%Y%m%d')\n",
    "# start_date_1 = (datetime.strptime(start_date, '%Y%m%d') + timedelta(days=-1)).date().strftime('%Y%m%d')\n",
    "# end_date = '20250812'  # 0812  一共36天,最后四天两天周末两天周中用作验证\n",
    "# end_date_add1 = (datetime.strptime(end_date, '%Y%m%d') + timedelta(days=1)).date().strftime('%Y%m%d')\n",
    "# end_date_1 = (datetime.strptime(end_date, '%Y%m%d') + timedelta(days=-1)).date().strftime('%Y%m%d')\n",
    "end_date_add2 = (datetime.strptime(end_date, '%Y%m%d') + timedelta(days=2)).date().strftime('%Y%m%d')\n",
    "end_date_2 = (datetime.strptime(end_date, '%Y%m%d') + timedelta(days=-2)).date().strftime('%Y%m%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vertical-fruit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:19:02.184187Z",
     "iopub.status.busy": "2025-10-22T08:19:02.183598Z",
     "iopub.status.idle": "2025-10-22T08:19:02.470886Z",
     "shell.execute_reply": "2025-10-22T08:19:02.470183Z",
     "shell.execute_reply.started": "2025-10-22T08:19:02.184130Z"
    }
   },
   "outputs": [],
   "source": [
    "df_flow = spark.sql(f'''\n",
    "with \n",
    "    bike_start_order as (\n",
    "        select \n",
    "            bike_start_park_guid as parking_guid,\n",
    "            count(order_id) as daily_order_cnt\n",
    "        from dwd.dwd_trd_ord_ebik_order_ent_di\n",
    "        where pt between '{twoweek_ago_date}' and '{end_date}'\n",
    "        group by bike_start_park_guid,pt\n",
    "    ),\n",
    "    \n",
    "    streets_max_orders as (\n",
    "        select \n",
    "            parking_guid,\n",
    "            max(daily_order_cnt) as max_daily_orders\n",
    "        from bike_start_order\n",
    "        group by parking_guid\n",
    "        having max_daily_orders >= 15\n",
    "    ),\n",
    "        \n",
    "    streets_mean_orders as (\n",
    "        select \n",
    "            parking_guid,\n",
    "            mean(daily_order_cnt) as mean_daily_orders\n",
    "        from bike_start_order\n",
    "        group by parking_guid\n",
    "        having mean_daily_orders >= 10\n",
    "    ),\n",
    "    \n",
    "    filtered_streets as (\n",
    "        select parking_guid from streets_max_orders\n",
    "        union\n",
    "        select parking_guid from streets_mean_orders\n",
    "    ),\n",
    "\n",
    "    net_data as (\n",
    "        select \n",
    "            site_guid, city_guid, label_10, label_16, label_21, hour, pt\n",
    "        from turing.ebike_site_period_net_out_label_di\n",
    "        where pt between '{start_date_minus11}' and '{end_date}'\n",
    "    )\n",
    "\n",
    "select net_data.* from net_data\n",
    "where net_data.site_guid in (select parking_guid from filtered_streets)\n",
    "and net_data.site_guid is not null\n",
    "\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "final-insert",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:19:06.378997Z",
     "iopub.status.busy": "2025-10-22T08:19:06.378503Z",
     "iopub.status.idle": "2025-10-22T08:19:06.627364Z",
     "shell.execute_reply": "2025-10-22T08:19:06.626594Z",
     "shell.execute_reply.started": "2025-10-22T08:19:06.378956Z"
    }
   },
   "outputs": [],
   "source": [
    "@F.udf(StringType())\n",
    "def generate_datetime_hour(date_str, hour_int):\n",
    "    return f\"{date_str}{hour_int:02d}\"\n",
    "\n",
    "# 定义窗口规范\n",
    "window_spec = Window.partitionBy(\"site_guid\",\"hour\").orderBy(\"date\")\n",
    "# 定义滞后天数列表\n",
    "lags = [1, 2, 3, 4, 11]\n",
    "\n",
    "# 2. lag24数据 (t-4日)\n",
    "long_df = df_flow.withColumn(\"dt\", generate_datetime_hour(\"pt\", \"hour\")  # 如pt=20250701 + hour=12 → dt=2025070112\n",
    "    ).withColumn('date', F.to_date(F.col('dt').cast('string'), 'yyyyMMddHH')\n",
    "    )\n",
    "\n",
    "# 为每个特征生成滞后列\n",
    "for k in lags:\n",
    "    # 计算滞后的小时数\n",
    "    # lag_hours = 24 * k\n",
    "    long_df = long_df \\\n",
    "        .withColumn(f\"lag{k}d_10\", F.lag(\"label_10\", k).over(window_spec)) \\\n",
    "        .withColumn(f\"lag{k}d_16\", F.lag(\"label_16\", k).over(window_spec)) \\\n",
    "        .withColumn(f\"lag{k}d_21\", F.lag(\"label_21\", k).over(window_spec)) \\\n",
    "\n",
    "long_df = long_df.filter(F.col(\"pt\").between(start_date, end_date)).fillna(0)\n",
    "# long_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "accomplished-scale",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:19:09.500426Z",
     "iopub.status.busy": "2025-10-22T08:19:09.500064Z",
     "iopub.status.idle": "2025-10-22T08:19:09.910137Z",
     "shell.execute_reply": "2025-10-22T08:19:09.909231Z",
     "shell.execute_reply.started": "2025-10-22T08:19:09.500396Z"
    }
   },
   "outputs": [],
   "source": [
    "ebik_park_hf = spark.read.format(\"iceberg\").load(\"dwb.dwb_veh_ebik_park_hf\") \\\n",
    "    .filter(F.col(\"pt\").between(start_date,end_date)&\n",
    "        (F.col(\"min\") == \"00\")\n",
    "    ).select(\"parking_guid\",\"put_veh_cnt\",\"idle_12h_cnt\",\"idle_1d_cnt\",\"hr\",\"pt\"\n",
    "    ).withColumn(\"dt2\", F.concat(F.col(\"pt\"), F.col(\"hr\"))\n",
    "    )\n",
    "\n",
    "long_df = long_df.join(ebik_park_hf, (long_df.site_guid==ebik_park_hf.parking_guid) & (long_df.dt==ebik_park_hf.dt2), 'left'\n",
    "                ).select(long_df.site_guid,long_df.city_guid,\"label_10\",\"label_16\",\"label_21\",\"put_veh_cnt\",\"idle_12h_cnt\",\"idle_1d_cnt\",\n",
    "        \"lag1d_10\",\"lag1d_16\",\"lag1d_21\",\"lag2d_10\",\"lag2d_16\",\"lag2d_21\",\n",
    "        \"lag3d_10\",\"lag3d_16\",\"lag3d_21\",\"lag4d_10\",\"lag4d_16\",\"lag4d_21\",\n",
    "        \"lag11d_10\",\"lag11d_16\",\"lag11d_21\",\"hour\",long_df.pt,\"dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "competitive-eagle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:19:10.321552Z",
     "iopub.status.busy": "2025-10-22T08:19:10.321143Z",
     "iopub.status.idle": "2025-10-22T08:19:10.413460Z",
     "shell.execute_reply": "2025-10-22T08:19:10.412666Z",
     "shell.execute_reply.started": "2025-10-22T08:19:10.321516Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_dict = {\n",
    "    \"label_10\": 0,\n",
    "    \"label_16\": 0,\n",
    "    \"label_21\": 0,\n",
    "    \"put_veh_cnt\": 0,\n",
    "    \"idle_12h_cnt\": 0,\n",
    "    \"idle_1d_cnt\": 0,    \n",
    "    \"lag1d_10\": 0,\n",
    "    \"lag1d_16\": 0,\n",
    "    \"lag1d_21\": 0,\n",
    "    \"lag2d_10\": 0,\n",
    "    \"lag2d_16\": 0,\n",
    "    \"lag2d_21\": 0,\n",
    "    \"lag3d_10\": 0,\n",
    "    \"lag3d_16\": 0,\n",
    "    \"lag3d_21\": 0,\n",
    "    \"lag4d_10\": 0,\n",
    "    \"lag4d_16\": 0,\n",
    "    \"lag4d_21\": 0,\n",
    "    \"lag11d_10\": 0,\n",
    "    \"lag11d_16\": 0,\n",
    "    \"lag11d_21\": 0\n",
    "    # 'cycle_weather_level': 2,  # 假设填充2\n",
    "    # 'workday_level': 1,\n",
    "    # 'temperature_avg_val': 25.00,  # 假设填充25\n",
    "}\n",
    "\n",
    "\n",
    "long_df = long_df.fillna(fill_dict)\n",
    "long_df = long_df.select(\n",
    "    'dt', \n",
    "    'site_guid',\n",
    "    'city_guid',\n",
    "    \"label_10\", \n",
    "    \"label_16\", \n",
    "    \"label_21\",\n",
    "    \"put_veh_cnt\",\n",
    "    \"idle_12h_cnt\",\n",
    "    \"idle_1d_cnt\",\n",
    "    \"lag1d_10\",\n",
    "    \"lag1d_16\",\n",
    "    \"lag1d_21\",\n",
    "    \"lag2d_10\",\n",
    "    \"lag2d_16\",\n",
    "    \"lag2d_21\",\n",
    "    \"lag3d_10\",\n",
    "    \"lag3d_16\",\n",
    "    \"lag3d_21\",\n",
    "    \"lag4d_10\",\n",
    "    \"lag4d_16\",\n",
    "    \"lag4d_21\",\n",
    "    \"lag11d_10\",\n",
    "    \"lag11d_16\",\n",
    "    \"lag11d_21\", \n",
    "    'hour'\n",
    ").sort('site_guid', 'dt')\n",
    "\n",
    "# long_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "promising-differential",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:19:14.937642Z",
     "iopub.status.busy": "2025-10-22T08:19:14.937026Z",
     "iopub.status.idle": "2025-10-22T08:19:15.113629Z",
     "shell.execute_reply": "2025-10-22T08:19:15.112234Z",
     "shell.execute_reply.started": "2025-10-22T08:19:14.937575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征列: ['label_10', 'label_16', 'label_21', 'put_veh_cnt', 'idle_12h_cnt', 'idle_1d_cnt', 'lag1d_10', 'lag1d_16', 'lag1d_21', 'lag2d_10', 'lag2d_16', 'lag2d_21', 'lag3d_10', 'lag3d_16', 'lag3d_21', 'lag4d_10', 'lag4d_16', 'lag4d_21', 'lag11d_10', 'lag11d_16', 'lag11d_21', 'hour']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, from_json, to_json\n",
    "def create_optimized_intermediate_table_v2(long_df, pred_date, seq_len, id_col=\"site_guid\", time_col=\"dt\", feature_cols=None):\n",
    "    \"\"\"\n",
    "    修复版本：避免复杂UDF，使用更直接的方法\n",
    "    \"\"\"\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [col for col in long_df.columns if col not in [id_col, time_col, 'city_guid']]\n",
    "    \n",
    "    from pyspark.sql.functions import collect_list, array, lit, row_number, concat_ws, to_json\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    print(f\"特征列: {feature_cols}\")\n",
    "    \n",
    "   # 数据预处理\n",
    "    window_spec = Window.partitionBy(id_col).orderBy(F.desc(time_col))\n",
    "    windowed_df = long_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                         .filter(F.col(\"row_num\") <= seq_len) \\\n",
    "                         .orderBy(id_col, time_col)\n",
    "    \n",
    "    # 创建特征向量\n",
    "    windowed_df = windowed_df.withColumn(\n",
    "        \"feature_vector\", \n",
    "        array(*[F.col(feat).cast(\"double\") for feat in feature_cols])\n",
    "    )\n",
    "    \n",
    "    # 分组并收集\n",
    "    grouped_df = windowed_df.groupBy(id_col,\"city_guid\").agg(\n",
    "        collect_list(\"feature_vector\").alias(\"features_array\"),\n",
    "        F.count(\"*\").alias(\"seq_count\")\n",
    "    ).filter(F.col(\"seq_count\") == seq_len)\n",
    "    \n",
    "    # 方案A: 存储为JSON字符串（推荐）\n",
    "    final_df = grouped_df.withColumn(\"features_json\", to_json(F.col(\"features_array\"))\n",
    "                                    ).select(id_col,\"city_guid\",\"features_json\")\n",
    "    # final_df.show(2, truncate=False)\n",
    "    return final_df\n",
    "# outputtable = 'turing_dev.turing_ebike_fixtime_train_features_df'\n",
    "final_df = create_optimized_intermediate_table_v2(long_df, '20250928', 42*24, id_col=\"site_guid\", time_col=\"dt\", feature_cols=None)\n",
    "\n",
    "# 保存\n",
    "# final_df.createOrReplaceTempView(\"final_feature_df\")\n",
    "# spark.sql(f\"\"\"\n",
    "#     INSERT OVERWRITE TABLE {output_table_name} \n",
    "#     PARTITION(pt='{pred_date}') \n",
    "#     SELECT {id_col}, features_json\n",
    "#     FROM final_feature_df\n",
    "# \"\"\")\n",
    "\n",
    "# print(f\"优化的中间表已保存: {output_table_name}\")\n",
    "# print(f\"序列长度: {seq_len}, 特征数量: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "acoustic-regular",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T10:01:34.083528Z",
     "iopub.status.busy": "2025-10-14T10:01:34.083181Z",
     "iopub.status.idle": "2025-10-14T10:01:48.562997Z",
     "shell.execute_reply": "2025-10-14T10:01:48.561897Z",
     "shell.execute_reply.started": "2025-10-14T10:01:34.083497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|           site_guid|           city_guid|       features_json|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|00a1cc65951c42f2a...|e272dd16cd5144168...|[[1.0,3.0,3.0,4.0...|\n",
      "|0167b931b61742eaa...|d64cfb70450648c1b...|[[0.0,0.0,1.0,22....|\n",
      "|0721f81075364b429...|f85204eed54348a4b...|[[5.0,5.0,7.0,7.0...|\n",
      "|0a395e40l1l5ryfvw...|BEEF3892FC8C4F35B...|[[0.0,0.0,0.0,1.0...|\n",
      "|0a831ec1l1lyewsuj...|85177E9C4BE74D748...|[[0.0,0.0,0.0,1.0...|\n",
      "|0a83244bl1lv5tps8...|4e8c12b8428947c8a...|[[0.0,0.0,0.0,0.0...|\n",
      "|0a8326bal1l358gku...|282D06BE2EAE4D859...|[[0.0,0.0,1.0,1.0...|\n",
      "|0a8326bal1lb4apnh...|83903688AD394E9BB...|[[0.0,1.0,1.0,0.0...|\n",
      "|0a8326bal1lb86dvf...|83903688AD394E9BB...|[[4.0,4.0,4.0,2.0...|\n",
      "|0a8326bal1lcj3p9p...|83903688AD394E9BB...|[[7.0,8.0,8.0,7.0...|\n",
      "|0a8326bal1lhtnvdx...|F495A777469444F08...|[[5.0,5.0,6.0,10....|\n",
      "|0a832885l1lf23ifm...|B2DD2675A8D44EE49...|[[0.0,0.0,0.0,0.0...|\n",
      "|0a832a0el1lwmdihi...|1011a2c73ce149a48...|[[4.0,5.0,5.0,11....|\n",
      "|0a832bbfl1lrazw65...|0985D3EDE31E4C1EB...|[[1.0,1.0,3.0,6.0...|\n",
      "|0a8331d4l1l9a745q...|d53a7a5720434db9b...|[[4.0,4.0,4.0,4.0...|\n",
      "|0a8331d4l1ln6xbtm...|B2DD2675A8D44EE49...|[[0.0,0.0,0.0,0.0...|\n",
      "|0a833754l1l78j4f4...|F495A777469444F08...|[[8.0,14.0,17.0,2...|\n",
      "|0a833a0el1l48pgcv...|b095862e5eba48b28...|[[3.0,4.0,4.0,35....|\n",
      "|0a833a0el1l4zb7d9...|D9DBDF2F159143778...|[[9.0,10.0,10.0,7...|\n",
      "|0a834034l1lz7r79j...|5034C0F30A154908B...|[[13.0,15.0,15.0,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "comparable-triangle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:19:19.154130Z",
     "iopub.status.busy": "2025-10-22T08:19:19.153540Z",
     "iopub.status.idle": "2025-10-22T08:19:47.098846Z",
     "shell.execute_reply": "2025-10-22T08:19:47.098233Z",
     "shell.execute_reply.started": "2025-10-22T08:19:19.154069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的日期范围大小: 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[site_guid: string, city_guid: string, date_str: string, date: timestamp]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_start_date = (datetime.strptime(start_date, '%Y%m%d') + timedelta(days=13+1)).date().strftime('%Y%m%d')  #小时数据在开始日之后14天集齐第一组特征，以其为t-2预测t+1，\n",
    "label_end_date = (datetime.strptime(end_date, '%Y%m%d') + timedelta(days=-2)).date().strftime('%Y%m%d')   #天级数据以t-1预测t+1,故需要在最后一个真值前-2\n",
    "# 使用DataFrame API生成日期序列，更稳定 \n",
    "start_dt = datetime.strptime(label_start_date, '%Y%m%d')\n",
    "end_dt = datetime.strptime(label_end_date, '%Y%m%d')\n",
    "\n",
    "# 创建日期范围\n",
    "date_range = [start_dt + timedelta(days=x) for x in range((end_dt - start_dt).days + 1)]\n",
    "\n",
    "# 转换为Spark DataFrame\n",
    "date_df = spark.createDataFrame(\n",
    "    [(date.strftime('%Y%m%d'), date) for date in date_range],\n",
    "    [\"date_str\", \"date\"]\n",
    ")\n",
    "\n",
    "print(f\"生成的日期范围大小: {date_df.count()}\")\n",
    "\n",
    "window_site = Window.orderBy(\"site_guid,pt\")\n",
    "res_site = final_df.select(\"site_guid\",\"city_guid\")\n",
    "\n",
    "# 交叉连接点位ID和日期\n",
    "res_time_site = res_site.crossJoin(date_df)\n",
    "res_time_site.cache()\n",
    "# res_time_site.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "proprietary-stations",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T06:14:05.702944Z",
     "iopub.status.busy": "2025-10-20T06:14:05.702566Z",
     "iopub.status.idle": "2025-10-20T06:19:35.193143Z",
     "shell.execute_reply": "2025-10-20T06:19:35.191944Z",
     "shell.execute_reply.started": "2025-10-20T06:14:05.702911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------+-------------------+\n",
      "|          site_guid|           city_guid|date_str|               date|\n",
      "+-------------------+--------------------+--------+-------------------+\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250901|2025-09-01 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250902|2025-09-02 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250903|2025-09-03 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250904|2025-09-04 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250905|2025-09-05 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250906|2025-09-06 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250907|2025-09-07 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250908|2025-09-08 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250909|2025-09-09 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250910|2025-09-10 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250911|2025-09-11 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250912|2025-09-12 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250913|2025-09-13 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250914|2025-09-14 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250915|2025-09-15 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250916|2025-09-16 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250917|2025-09-17 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250918|2025-09-18 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250919|2025-09-19 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250920|2025-09-20 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250921|2025-09-21 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250922|2025-09-22 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250923|2025-09-23 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250924|2025-09-24 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250925|2025-09-25 00:00:00|\n",
      "|1074588174745837568|40E4554E4C14445FA...|20250926|2025-09-26 00:00:00|\n",
      "+-------------------+--------------------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test1 = res_time_site.filter(F.col(\"site_guid\")=='1074588174745837568')\n",
    "test1.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ordered-visiting",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T07:21:17.433858Z",
     "iopub.status.busy": "2025-10-22T07:21:17.433472Z",
     "iopub.status.idle": "2025-10-22T07:23:06.721028Z",
     "shell.execute_reply": "2025-10-22T07:23:06.720287Z",
     "shell.execute_reply.started": "2025-10-22T07:21:17.433821Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用全量数据得出city encode\n",
    "city_guid_encode = spark.sql('''\n",
    "    select distinct city_guid from \n",
    "        dim.dim_spt_fence_info\n",
    "    where pt between 20251010 and 20251020\n",
    "    and area_status = 5        \n",
    "    and area_type = 207\n",
    "''')\n",
    "indexer = StringIndexer(inputCol=\"city_guid\", outputCol=\"city_guid_encoded\")\n",
    "indexer_model = indexer.fit(city_guid_encode)\n",
    "# static_feature_df = indexer_model.transform(static_feature_df)\n",
    "\n",
    "# 保存 StringIndexer 模型（用于后续新数据）\n",
    "idxmap = indexer_model.transform(\n",
    "    city_guid_encode.select(\"city_guid\").distinct()\n",
    ").orderBy(\"city_guid_encoded\").toPandas()\n",
    "idxmap.to_csv(\"ebik_dataset/city_guid_mapping_3.csv\",index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ordinary-hardware",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:20:18.260288Z",
     "iopub.status.busy": "2025-10-22T08:20:18.259789Z",
     "iopub.status.idle": "2025-10-22T08:20:18.575742Z",
     "shell.execute_reply": "2025-10-22T08:20:18.574624Z",
     "shell.execute_reply.started": "2025-10-22T08:20:18.260243Z"
    }
   },
   "outputs": [],
   "source": [
    "window_start_dt = start_dt - timedelta(days=10)\n",
    "daily_orders = spark.table(\"dwd.dwd_trd_ord_ebik_order_ent_di\")\\\n",
    "    .filter(F.col(\"pt\").between(window_start_dt.strftime('%Y%m%d'), end_dt.strftime('%Y%m%d')))\\\n",
    "    .groupBy(F.col(\"bike_start_park_guid\").alias(\"parking_guid\"), \"pt\")\\\n",
    "    .agg(F.count(\"order_id\").alias(\"daily_order_cnt\"),\n",
    "    F.max(\"start_parking_capacity\").alias(\"parking_capacity\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"parking_guid\").orderBy(\"pt\")\n",
    "daily_orders = daily_orders.withColumn(f\"lag7d_order_cnt\", F.lag(\"daily_order_cnt\", 5).over(window_spec))   #真实对应上周数据\n",
    "\n",
    "# 读取 CSV 城市映射表\n",
    "mapping_df = pd.read_csv(\"ebik_dataset/city_guid_mapping_3.csv\")\n",
    "mapping_df2 = spark.createDataFrame(mapping_df)\n",
    "max_citys = len(mapping_df) \n",
    "\n",
    "static_feature_df = res_time_site.join(\n",
    "    daily_orders.select(\n",
    "        F.col(\"parking_guid\"),\n",
    "        F.col(\"pt\"),\n",
    "        F.col(\"lag7d_order_cnt\"),\n",
    "        F.col(\"parking_capacity\")\n",
    "    ),\n",
    "    (F.col(\"site_guid\") == F.col(\"parking_guid\")) &\n",
    "    (F.col(\"pt\") == F.col(\"date_str\")),\n",
    "    \"left\"\n",
    ").join(\n",
    "    mapping_df2.select(\"city_guid\", \"city_guid_encoded\"),\n",
    "    on=\"city_guid\",\n",
    "    how=\"left\"\n",
    ").fillna({\"city_guid_encoded\": max_citys}\n",
    ").select(\n",
    "    F.col(\"site_guid\"),\n",
    "    \"city_guid\",\n",
    "    \"date_str\",\n",
    "    F.date_format(F.date_add(F.col(\"date\"), 2), \"yyyyMMdd\").alias(\"date_pred\"),\n",
    "    F.coalesce(\"city_guid_encoded\",F.lit(max_citys)).alias(\"city_guid_encoded\"), \n",
    "    F.dayofweek(F.date_add(F.col(\"date\"), 2)).alias(\"day_of_week\"),\n",
    "    F.coalesce(\"lag7d_order_cnt\", F.lit(0)).alias(\"lag7d_order_cnt\"),\n",
    "    F.coalesce(\"parking_capacity\", F.lit(0)).alias(\"parking_capacity\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 查看编码后的数据\n",
    "# df_encoded.select(\"city_guid\", \"city_guid_encoded\").show()\n",
    "# static_feature_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "funny-palestinian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T07:44:02.749743Z",
     "iopub.status.busy": "2025-10-22T07:44:02.749339Z",
     "iopub.status.idle": "2025-10-22T07:44:02.802105Z",
     "shell.execute_reply": "2025-10-22T07:44:02.801493Z",
     "shell.execute_reply.started": "2025-10-22T07:44:02.749707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_df = pd.read_csv(\"ebik_dataset/city_guid_mapping_3.csv\")\n",
    "mapping_df2 =  spark.createDataFrame(mapping_df)\n",
    "len(mapping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acute-appliance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T07:32:11.269255Z",
     "iopub.status.busy": "2025-10-20T07:32:11.268915Z",
     "iopub.status.idle": "2025-10-20T07:32:22.352471Z",
     "shell.execute_reply": "2025-10-20T07:32:22.351775Z",
     "shell.execute_reply.started": "2025-10-20T07:32:11.269223Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "valid-group",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T07:30:40.012474Z",
     "iopub.status.busy": "2025-10-20T07:30:40.012127Z",
     "iopub.status.idle": "2025-10-20T07:30:40.022137Z",
     "shell.execute_reply": "2025-10-20T07:30:40.021449Z",
     "shell.execute_reply.started": "2025-10-20T07:30:40.012441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          city_guid  city_guid_encoded\n",
      "0  B2DD2675A8D44EE49224906B90E8EAF9                0.0\n",
      "1  AA99442B62E7485086F77A1DBD4FDF65                1.0\n",
      "2  D9DBDF2F159143778C7748C47B262BB4                2.0\n",
      "3  BEEF3892FC8C4F35BC8B7F832FA311D7                3.0\n",
      "4  A2A7BDAF3532442CA33B8A5F4BA2F6C5                4.0\n"
     ]
    }
   ],
   "source": [
    "# 用 pandas 读取 CSV，跳过可能的错误行\n",
    "try:\n",
    "    df_test = pd.read_csv(\"ebik_dataset/city_guid_mapping.csv\")\n",
    "    print(df_test.head())\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    # 用逐行读取检查问题行\n",
    "    with open(\"ebik_dataset/city_guid_mapping.csv\", \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 100:  # 检查第 1 行和第 105 行附近\n",
    "                print(f\"Line {i}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "approximate-forge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:20:58.104738Z",
     "iopub.status.busy": "2025-10-22T08:20:58.104339Z",
     "iopub.status.idle": "2025-10-22T08:20:58.248952Z",
     "shell.execute_reply": "2025-10-22T08:20:58.248043Z",
     "shell.execute_reply.started": "2025-10-22T08:20:58.104700Z"
    }
   },
   "outputs": [],
   "source": [
    "@F.udf(StringType())\n",
    "def generate_datetime_hour_minus2(date_str, hour_int):\n",
    "    date_m2 = (datetime.strptime(date_str, '%Y%m%d') + timedelta(days=-2)).date().strftime('%Y%m%d') \n",
    "    return f\"{date_m2}{hour_int:02d}\"\n",
    "\n",
    "start_dt1 = (start_dt + timedelta(days=1)).date()\n",
    "end_dt1 = (end_dt + timedelta(days=1)).date()\n",
    "\n",
    "# 2. 获取数据\n",
    "wtw = spark.table('turing_dev.turing_net_pred_wea_temp_wkd_feature') \\\n",
    "    .select('city_guid','forecast_date','pred_date','cycle_weather_level','temperature_avg_val','workday_level') \\\n",
    "    .filter(F.col('pred_date').between(start_dt1.strftime('%Y%m%d'), end_dt1.strftime('%Y%m%d'))) \\\n",
    "    .groupBy('city_guid', 'pred_date') \\\n",
    "    .agg(\n",
    "        F.first('cycle_weather_level').alias('cycle_weather_level'),\n",
    "        F.first('temperature_avg_val').alias('temperature_avg_val'),\n",
    "        F.first('workday_level').alias('workday_level')\n",
    "    )\n",
    "\n",
    "static_feature_df = static_feature_df.join(wtw, \n",
    "                       (static_feature_df.date_pred==wtw.pred_date) &\n",
    "                       (static_feature_df.city_guid==wtw.city_guid), 'left'\n",
    "        ).dropna(subset = ['site_guid']\n",
    "        ).select(static_feature_df.site_guid, static_feature_df.city_guid, F.col('date_pred').alias('pt'),\n",
    "                static_feature_df.lag7d_order_cnt, static_feature_df.parking_capacity,\n",
    "                F.coalesce(wtw.temperature_avg_val,F.lit(20)).alias(\"temperature_avg_val\"), \n",
    "                static_feature_df.city_guid_encoded, static_feature_df.day_of_week,\n",
    "                F.coalesce(wtw.workday_level,F.lit(1)).alias(\"workday_level\"),\n",
    "                F.coalesce(wtw.cycle_weather_level,F.lit(2)).alias(\"cycle_weather_level\")\n",
    "        ).sort('site_guid', 'date_str')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-export",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "commercial-sympathy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T07:46:38.283445Z",
     "iopub.status.busy": "2025-10-20T07:46:38.283111Z",
     "iopub.status.idle": "2025-10-20T07:47:23.742187Z",
     "shell.execute_reply": "2025-10-20T07:47:23.741344Z",
     "shell.execute_reply.started": "2025-10-20T07:46:38.283415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+---------------+----------------+-------------------+-----------------+-----------+-------------+-------------------+\n",
      "|           site_guid|           city_guid|      pt|lag7d_order_cnt|parking_capacity|temperature_avg_val|city_guid_encoded|day_of_week|workday_level|cycle_weather_level|\n",
      "+--------------------+--------------------+--------+---------------+----------------+-------------------+-----------------+-----------+-------------+-------------------+\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250901|             11|              20|            27.9800|             21.0|          4|            1|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250902|              3|             144|            28.7700|             21.0|          5|            1|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250903|              5|             144|            29.7400|             21.0|          6|            1|                  2|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250904|              9|             144|            29.5100|             21.0|          7|            2|                  2|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250905|              8|              84|            28.5400|             21.0|          1|            2|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250906|             11|               0|            26.3600|             21.0|          2|            1|                  1|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250907|             17|             144|            25.3300|             21.0|          3|            1|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250908|             12|               0|            25.2100|             21.0|          4|            1|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250909|             11|              86|            26.8300|             21.0|          5|            1|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250910|              5|             144|            26.2900|             21.0|          6|            1|                  1|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250911|              6|               0|            25.1800|             21.0|          7|            2|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250912|              3|             144|            25.6200|             21.0|          1|            2|                  2|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250913|              1|              44|            26.5400|             21.0|          2|            1|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250914|              4|              44|            28.0800|             21.0|          3|            1|                  2|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250915|             11|               0|            23.9100|             21.0|          4|            1|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250916|              8|              42|            22.3300|             21.0|          5|            1|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250917|              7|             144|            22.8200|             21.0|          6|            1|                  2|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250918|             21|             144|            21.8500|             21.0|          7|            2|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250919|             16|              46|            22.4000|             21.0|          1|            2|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250920|              5|              32|            23.1300|             21.0|          2|            1|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250921|             16|             144|            21.8000|             21.0|          3|            1|                  2|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250922|              7|             144|            21.8900|             21.0|          4|            1|                  1|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250923|             13|               0|            22.5300|             21.0|          5|            1|                  2|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250924|             15|             144|            23.4500|             21.0|          6|            1|                  2|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250925|             12|               0|            23.8000|             21.0|          7|            2|                  3|\n",
      "|0002139af19b44849...|39CA4DFE94E647909...|20250926|             11|             144|                 20|             21.0|          1|            1|                  2|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250901|             23|              40|            20.0300|              6.0|          4|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250902|             13|              40|            21.1200|              6.0|          5|            1|                  2|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250903|             14|              40|            19.6700|              6.0|          6|            1|                  2|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250904|             16|              40|            20.1200|              6.0|          7|            2|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250905|              7|              40|            19.8200|              6.0|          1|            2|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250906|             13|              40|            20.6200|              6.0|          2|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250907|             17|              40|            21.1700|              6.0|          3|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250908|             10|              40|            21.4700|              6.0|          4|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250909|              9|              40|            20.7600|              6.0|          5|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250910|              4|              40|            20.9200|              6.0|          6|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250911|             16|              40|            21.1900|              6.0|          7|            2|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250912|             10|              40|            20.1000|              6.0|          1|            2|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250913|             11|              40|            20.7200|              6.0|          2|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250914|             10|              40|            19.3200|              6.0|          3|            1|                  2|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250915|             12|              40|            13.9200|              6.0|          4|            1|                  2|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250916|              5|              40|            14.2800|              6.0|          5|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250917|             12|              40|            16.6500|              6.0|          6|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250918|             16|              40|            15.8400|              6.0|          7|            2|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250919|             12|              40|            15.0200|              6.0|          1|            2|                  2|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250920|             11|              40|            15.4100|              6.0|          2|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250921|              9|              40|            16.9200|              6.0|          3|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250922|              6|              40|            16.1600|              6.0|          4|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250923|              9|              40|            16.9300|              6.0|          5|            1|                  3|\n",
      "|000262e190a042f6a...|91ECB1D8546748C29...|20250924|              6|              40|            16.2400|              6.0|          6|            1|                  3|\n",
      "+--------------------+--------------------+--------+---------------+----------------+-------------------+-----------------+-----------+-------------+-------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static_feature_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "minimal-teens",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:21:13.008142Z",
     "iopub.status.busy": "2025-10-22T08:21:13.007675Z",
     "iopub.status.idle": "2025-10-22T08:27:40.982684Z",
     "shell.execute_reply": "2025-10-22T08:27:40.981766Z",
     "shell.execute_reply.started": "2025-10-22T08:21:13.008102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征列: ['lag7d_order_cnt', 'parking_capacity', 'temperature_avg_val', 'city_guid_encoded', 'day_of_week', 'workday_level', 'cycle_weather_level']\n",
      "优化的中间表已保存: turing_dev.turing_ebike_fixtime_train_features_df_2\n"
     ]
    }
   ],
   "source": [
    "final_df_daily = create_optimized_intermediate_table_v2(static_feature_df, '20250928', 26, id_col=\"site_guid\", time_col=\"pt\", feature_cols=None)\n",
    "# final_df_daily.show(50)\n",
    "final_df = final_df.alias(\"df1\").join(final_df_daily.alias(\"df2\"), \n",
    "                                     \"site_guid\", \n",
    "                                     'left'\n",
    "                                    ).select(final_df.site_guid, \n",
    "                                           final_df.city_guid,\n",
    "                                           final_df.features_json, \n",
    "                                           final_df_daily.features_json.alias(\"daily_features_json\"))\n",
    "final_df.createOrReplaceTempView(\"final_feature_df\")\n",
    "output_table_name = \"turing_dev.turing_ebike_fixtime_train_features_df_2\"\n",
    "pred_date = 20251001\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {output_table_name} \n",
    "    PARTITION(pt='{pred_date}') \n",
    "    SELECT site_guid, city_guid, features_json, daily_features_json\n",
    "    FROM final_feature_df\n",
    "\"\"\")\n",
    "\n",
    "print(f\"优化的中间表已保存: {output_table_name}\")\n",
    "# print(f\"序列长度: {seq_len}, 特征数量: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_feature_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "absent-floating",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:28:15.149485Z",
     "iopub.status.busy": "2025-10-22T08:28:15.149132Z",
     "iopub.status.idle": "2025-10-22T08:28:15.169326Z",
     "shell.execute_reply": "2025-10-22T08:28:15.168634Z",
     "shell.execute_reply.started": "2025-10-22T08:28:15.149453Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "def batch_predict_from_spark_table_optimized(table_name, pred_date, output_dir, batch_size=2500, \n",
    "                                           hourly_seq_len=336, static_seq_len=26, hourly_feature_count=11, static_feature_count=5):\n",
    "    \n",
    "    \n",
    "    # 1. 为数据添加行号以便分批处理\n",
    "    df = spark.sql(f\"\"\"\n",
    "        SELECT *, \n",
    "               ROW_NUMBER() OVER (ORDER BY site_guid) as row_id\n",
    "        FROM turing_dev.turing_ebike_fixtime_train_features_df_2\n",
    "        WHERE pt = '{pred_date}'\n",
    "    \"\"\")\n",
    "    \n",
    "    # 获取总行数\n",
    "    total_count = df.count()\n",
    "    print(f\"总共需要处理 {total_count} 个点位\")\n",
    "    \n",
    "    # 2. 计算批次数\n",
    "    num_batches = (total_count + batch_size - 1) // batch_size\n",
    "    \n",
    "    all_time_features = []\n",
    "    all_daily_features = []\n",
    "    all_site_guids = []\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_row = batch_idx * batch_size + 1  # row_number从1开始\n",
    "        end_row = min((batch_idx + 1) * batch_size, total_count)\n",
    "        \n",
    "        print(f\"处理批次 {batch_idx + 1}/{num_batches}, 行范围: {start_row}-{end_row}\")\n",
    "        \n",
    "        # 3. 获取当前批次数据\n",
    "        hourly_batch_df = df.filter(F.col('row_id').between(start_row, end_row)\n",
    "                            ).select('site_guid','hourly_features')\n",
    "        static_batch_df = df.filter(F.col('row_id').between(start_row, end_row)\n",
    "                            ).select('site_guid','static_features')\n",
    "        \n",
    "        hourly_batch_data = hourly_batch_df.collect()\n",
    "        static_batch_data = static_batch_df.collect()\n",
    "        \n",
    "        if not hourly_batch_data or not static_batch_data:\n",
    "            continue\n",
    "            \n",
    "        # 处理批次数据\n",
    "        hourly_batch_features, site_guids = prepare_batch_features(\n",
    "            hourly_batch_data, hourly_seq_len, hourly_feature_count, 'hourly_features'\n",
    "        )\n",
    "        static_batch_features, site_guids = prepare_batch_features(\n",
    "            static_batch_data, static_seq_len, static_feature_count, 'static_features'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(hourly_batch_features) > 0:\n",
    "            np.save(os.path.join(output_dir, f'batch_{batch_idx}_hourly_features.npy'), hourly_batch_features)\n",
    "            with open(os.path.join(output_dir, f'batch_{batch_idx}_guids.txt'), 'w') as f:\n",
    "                for guid in site_guids:\n",
    "                    f.write(f\"{guid}\\n\")\n",
    "        \n",
    "            print(f\"已保存时间特征，批次 {batch_idx}: {hourly_batch_features.shape}\")\n",
    "            del hourly_batch_features, site_guids\n",
    "    \n",
    "        \n",
    "        if len(static_batch_features) > 0:\n",
    "            np.save(os.path.join(output_dir, f'batch_{batch_idx}_static_features.npy'), static_batch_features)\n",
    "#             with open(os.path.join(output_dir, f'batch_{batch_idx}_guids.txt'), 'w') as f:\n",
    "#                 for guid in site_guids:\n",
    "#                     f.write(f\"{guid}\\n\")\n",
    "        \n",
    "            print(f\"已保存静态特征，批次 {batch_idx}: {static_batch_features.shape}\")\n",
    "            del static_batch_features\n",
    "#     # 4. 合并结果\n",
    "#     if all_features:\n",
    "#         final_features = np.concatenate(all_features, axis=0)\n",
    "#         print(f\"最终特征集形状: {final_features.shape}\")\n",
    "#         print(f\"总共处理了 {len(all_site_guids)} 个有效点位\")\n",
    "#     else:\n",
    "#         final_features = np.empty((0, seq_len, feature_count))\n",
    "#         print(\"警告: 没有有效的特征数据\")\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_batch_features(batch_data, seq_len, feature_count, col_name):\n",
    "    \"\"\"\n",
    "    解码JSON格式的特征数据\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    batch_features = []\n",
    "    site_guids = []\n",
    "    \n",
    "    for row in batch_data:\n",
    "        site_guid = row['site_guid']\n",
    "        features_json = row[col_name]\n",
    "        \n",
    "        # print(f\"处理 {site_guid}\")\n",
    "        # print(f\"JSON数据类型: {type(features_json)}\")\n",
    "        \n",
    "        try:\n",
    "            # 解析JSON\n",
    "            if isinstance(features_json, str):\n",
    "                features_list = json.loads(features_json)\n",
    "                # print(\"JSON解析成功\")\n",
    "            else:\n",
    "                features_list = features_json\n",
    "                print(\"直接使用原始数据\")\n",
    "            \n",
    "            # print(f\"解析后数据类型: {type(features_list)}\")\n",
    "            # print(f\"解析后数据长度: {len(features_list) if features_list else 0}\")\n",
    "            \n",
    "            # if features_list and len(features_list) > 0:\n",
    "                # print(f\"第一个时间步: {features_list[0]}\")\n",
    "            \n",
    "            # 转换为numpy数组\n",
    "            if features_list and len(features_list) == seq_len:\n",
    "                feature_matrix = []\n",
    "                \n",
    "                for i, time_step in enumerate(features_list):\n",
    "                    if isinstance(time_step, (list, tuple)) and len(time_step) == feature_count:\n",
    "                        row_data = [float(x) for x in time_step]\n",
    "                        feature_matrix.append(row_data)\n",
    "                    else:\n",
    "                        print(f\"时间步 {i} 格式错误: {type(time_step)}, 长度: {len(time_step) if hasattr(time_step, '__len__') else 'N/A'}\")\n",
    "                        break\n",
    "                else:\n",
    "                    # 所有时间步都正确处理\n",
    "                    feature_array = np.array(feature_matrix, dtype=np.float32)\n",
    "                    # print(f\"转换后shape: {feature_array.shape}\")\n",
    "                    \n",
    "                    if feature_array.shape == (seq_len, feature_count):\n",
    "                        batch_features.append(feature_array)\n",
    "                        site_guids.append(site_guid)\n",
    "                        # print(\"✓ 成功添加\")\n",
    "                    else:\n",
    "                        print(f\"✗ 维度不匹配: {feature_array.shape} vs ({seq_len}, {feature_count})\")\n",
    "            else:\n",
    "                print(f\"✗ 序列长度不匹配: {len(features_list) if features_list else 0} vs {seq_len}\")\n",
    "                \n",
    "        # except json.JSONDecodeError as e:\n",
    "            # print(f\"✗ JSON解析错误 {site_guid}: {e}\")\n",
    "            # print(f\"原始数据: {features_json[:200]}...\")\n",
    "        except Exception as e:\n",
    "            # print(f\"✗ 转换错误 {site_guid}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # 转换为3D数组 (batch_size, seq_len, feature_count)\n",
    "    if batch_features:\n",
    "        batch_features = np.stack(batch_features, axis=0)\n",
    "        print(f\"最终批次形状: {batch_features.shape}\")\n",
    "        # print(f\"数据范围: min={batch_features.min():.4f}, max={batch_features.max():.4f}\")\n",
    "    else:\n",
    "        batch_features = np.empty((0, seq_len, feature_count))\n",
    "        # print(\"警告: 没有有效的特征数据\")\n",
    "    \n",
    "    return batch_features, site_guids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "associate-miniature",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T08:28:42.344481Z",
     "iopub.status.busy": "2025-10-22T08:28:42.344014Z",
     "iopub.status.idle": "2025-10-22T09:11:06.588474Z",
     "shell.execute_reply": "2025-10-22T09:11:06.587590Z",
     "shell.execute_reply.started": "2025-10-22T08:28:42.344436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共需要处理 113468 个点位\n",
      "处理批次 1/5, 行范围: 1-25000\n",
      "最终批次形状: (25000, 1008, 22)\n",
      "最终批次形状: (25000, 26, 7)\n",
      "已保存时间特征，批次 0: (25000, 1008, 22)\n",
      "已保存静态特征，批次 0: (25000, 26, 7)\n",
      "处理批次 2/5, 行范围: 25001-50000\n",
      "最终批次形状: (25000, 1008, 22)\n",
      "最终批次形状: (25000, 26, 7)\n",
      "已保存时间特征，批次 1: (25000, 1008, 22)\n",
      "已保存静态特征，批次 1: (25000, 26, 7)\n",
      "处理批次 3/5, 行范围: 50001-75000\n",
      "最终批次形状: (25000, 1008, 22)\n",
      "最终批次形状: (25000, 26, 7)\n",
      "已保存时间特征，批次 2: (25000, 1008, 22)\n",
      "已保存静态特征，批次 2: (25000, 26, 7)\n",
      "处理批次 4/5, 行范围: 75001-100000\n",
      "最终批次形状: (25000, 1008, 22)\n",
      "最终批次形状: (25000, 26, 7)\n",
      "已保存时间特征，批次 3: (25000, 1008, 22)\n",
      "已保存静态特征，批次 3: (25000, 26, 7)\n",
      "处理批次 5/5, 行范围: 100001-113468\n",
      "最终批次形状: (13468, 1008, 22)\n",
      "最终批次形状: (13468, 26, 7)\n",
      "已保存时间特征，批次 4: (13468, 1008, 22)\n",
      "已保存静态特征，批次 4: (13468, 26, 7)\n"
     ]
    }
   ],
   "source": [
    "table_name = \"turing_dev.turing_ebike_fixtime_train_features_df_2\"\n",
    "output_dir = \"ebik_dataset/traindata1\"\n",
    "pred_date = \"20251001\"\n",
    "batch_predict_from_spark_table_optimized(\n",
    "    table_name=table_name,\n",
    "    output_dir=output_dir,\n",
    "    pred_date=pred_date,\n",
    "    batch_size=25000,  # 根据内存情况调整\n",
    "    hourly_seq_len=42*24, \n",
    "    static_seq_len=26, \n",
    "    hourly_feature_count=22, \n",
    "    static_feature_count=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absent-recipe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T11:05:04.498246Z",
     "iopub.status.busy": "2025-10-20T11:05:04.497877Z",
     "iopub.status.idle": "2025-10-20T11:05:07.927633Z",
     "shell.execute_reply": "2025-10-20T11:05:07.926764Z",
     "shell.execute_reply.started": "2025-10-20T11:05:04.498209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE turing_dev.turing_ebike_site_fixtime_predict_features_df_2(\n",
    "  site_guid string COMMENT '点位id',\n",
    "  city_guid string COMMENT '城市guid',\n",
    "  hourly_features string COMMENT '时序特征',\n",
    "  static_features string COMMENT '静态特征'\n",
    "  )\n",
    "COMMENT '助力车点位定终点模型训练特征数据表2'\n",
    "PARTITIONED BY ( \n",
    "  pt string COMMENT '预测的时间')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "unique-receipt",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T08:51:29.326480Z",
     "iopub.status.busy": "2025-10-16T08:51:29.326117Z",
     "iopub.status.idle": "2025-10-16T08:51:29.598684Z",
     "shell.execute_reply": "2025-10-16T08:51:29.597901Z",
     "shell.execute_reply.started": "2025-10-16T08:51:29.326448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101816"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
