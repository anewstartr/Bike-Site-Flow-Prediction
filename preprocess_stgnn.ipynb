{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df41c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all CSV files containing 'divvy' in their filename from the dataset folder\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "import __future__\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f0ba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 file(s):\n",
      " - 202504-divvy-tripdata.csv\n",
      " - 202505-divvy-tripdata.csv\n",
      " - 202506-divvy-tripdata.csv\n",
      " - 202507-divvy-tripdata.csv\n",
      " - 202508-divvy-tripdata.csv\n",
      "Combined shape: (3106310, 14)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ride_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rideable_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "started_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ended_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start_station_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start_station_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "end_station_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "end_station_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start_lat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "start_lng",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "end_lat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "end_lng",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "member_casual",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "__source_file",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ad855285-d967-4848-87c2-7a3c53d44bf8",
       "rows": [
        [
         "0",
         "AF3863596DF9D94B",
         "classic_bike",
         "2025-04-27 14:29:34.619",
         "2025-04-27 14:36:23.584",
         "Troy St & Elston Ave",
         "15631",
         "Richmond St & Diversey Ave",
         "15645",
         "41.94524356848",
         "-87.7066499009",
         "41.93190196886",
         "-87.7011951301",
         "member",
         "202504-divvy-tripdata.csv"
        ],
        [
         "1",
         "8B38081EBE918800",
         "electric_bike",
         "2025-04-23 17:48:51.863",
         "2025-04-23 17:59:06.015",
         "Wabash Ave & Adams St",
         "KA1503000015",
         "Green St & Madison St",
         "TA1307000120",
         "41.87947235235",
         "-87.6256886059",
         "41.88185932803823",
         "-87.64926373958588",
         "member",
         "202504-divvy-tripdata.csv"
        ],
        [
         "2",
         "1C7F1DE826BBBC8D",
         "electric_bike",
         "2025-04-05 17:55:30.845",
         "2025-04-05 18:05:40.032",
         "Damen Ave & Cortland St",
         "13133",
         "California Ave & Fletcher St",
         "15642",
         "41.915983",
         "-87.677335",
         "41.93842879148",
         "-87.698007756",
         "member",
         "202504-divvy-tripdata.csv"
        ],
        [
         "3",
         "CAD23D69A79A6C3B",
         "classic_bike",
         "2025-04-03 08:22:04.493",
         "2025-04-03 08:32:06.099",
         "Clark St & Elm St",
         "TA1307000039",
         "Orleans St & Merchandise Mart Plaza",
         "TA1305000022",
         "41.902973",
         "-87.63128",
         "41.888243",
         "-87.63639",
         "member",
         "202504-divvy-tripdata.csv"
        ],
        [
         "4",
         "BE241E601482E0AB",
         "electric_bike",
         "2025-04-15 06:09:55.293",
         "2025-04-15 06:19:58.942",
         "Western Ave & Walton St",
         "KA1504000103",
         "Damen Ave & Charleston St",
         "13288",
         "41.89841768945",
         "-87.6865960164",
         "41.920082",
         "-87.677855",
         "member",
         "202504-divvy-tripdata.csv"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>__source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF3863596DF9D94B</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2025-04-27 14:29:34.619</td>\n",
       "      <td>2025-04-27 14:36:23.584</td>\n",
       "      <td>Troy St &amp; Elston Ave</td>\n",
       "      <td>15631</td>\n",
       "      <td>Richmond St &amp; Diversey Ave</td>\n",
       "      <td>15645</td>\n",
       "      <td>41.945244</td>\n",
       "      <td>-87.706650</td>\n",
       "      <td>41.931902</td>\n",
       "      <td>-87.701195</td>\n",
       "      <td>member</td>\n",
       "      <td>202504-divvy-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8B38081EBE918800</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2025-04-23 17:48:51.863</td>\n",
       "      <td>2025-04-23 17:59:06.015</td>\n",
       "      <td>Wabash Ave &amp; Adams St</td>\n",
       "      <td>KA1503000015</td>\n",
       "      <td>Green St &amp; Madison St</td>\n",
       "      <td>TA1307000120</td>\n",
       "      <td>41.879472</td>\n",
       "      <td>-87.625689</td>\n",
       "      <td>41.881859</td>\n",
       "      <td>-87.649264</td>\n",
       "      <td>member</td>\n",
       "      <td>202504-divvy-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1C7F1DE826BBBC8D</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2025-04-05 17:55:30.845</td>\n",
       "      <td>2025-04-05 18:05:40.032</td>\n",
       "      <td>Damen Ave &amp; Cortland St</td>\n",
       "      <td>13133</td>\n",
       "      <td>California Ave &amp; Fletcher St</td>\n",
       "      <td>15642</td>\n",
       "      <td>41.915983</td>\n",
       "      <td>-87.677335</td>\n",
       "      <td>41.938429</td>\n",
       "      <td>-87.698008</td>\n",
       "      <td>member</td>\n",
       "      <td>202504-divvy-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAD23D69A79A6C3B</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2025-04-03 08:22:04.493</td>\n",
       "      <td>2025-04-03 08:32:06.099</td>\n",
       "      <td>Clark St &amp; Elm St</td>\n",
       "      <td>TA1307000039</td>\n",
       "      <td>Orleans St &amp; Merchandise Mart Plaza</td>\n",
       "      <td>TA1305000022</td>\n",
       "      <td>41.902973</td>\n",
       "      <td>-87.631280</td>\n",
       "      <td>41.888243</td>\n",
       "      <td>-87.636390</td>\n",
       "      <td>member</td>\n",
       "      <td>202504-divvy-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BE241E601482E0AB</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2025-04-15 06:09:55.293</td>\n",
       "      <td>2025-04-15 06:19:58.942</td>\n",
       "      <td>Western Ave &amp; Walton St</td>\n",
       "      <td>KA1504000103</td>\n",
       "      <td>Damen Ave &amp; Charleston St</td>\n",
       "      <td>13288</td>\n",
       "      <td>41.898418</td>\n",
       "      <td>-87.686596</td>\n",
       "      <td>41.920082</td>\n",
       "      <td>-87.677855</td>\n",
       "      <td>member</td>\n",
       "      <td>202504-divvy-tripdata.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type               started_at  \\\n",
       "0  AF3863596DF9D94B   classic_bike  2025-04-27 14:29:34.619   \n",
       "1  8B38081EBE918800  electric_bike  2025-04-23 17:48:51.863   \n",
       "2  1C7F1DE826BBBC8D  electric_bike  2025-04-05 17:55:30.845   \n",
       "3  CAD23D69A79A6C3B   classic_bike  2025-04-03 08:22:04.493   \n",
       "4  BE241E601482E0AB  electric_bike  2025-04-15 06:09:55.293   \n",
       "\n",
       "                  ended_at       start_station_name start_station_id  \\\n",
       "0  2025-04-27 14:36:23.584     Troy St & Elston Ave            15631   \n",
       "1  2025-04-23 17:59:06.015    Wabash Ave & Adams St     KA1503000015   \n",
       "2  2025-04-05 18:05:40.032  Damen Ave & Cortland St            13133   \n",
       "3  2025-04-03 08:32:06.099        Clark St & Elm St     TA1307000039   \n",
       "4  2025-04-15 06:19:58.942  Western Ave & Walton St     KA1504000103   \n",
       "\n",
       "                      end_station_name end_station_id  start_lat  start_lng  \\\n",
       "0           Richmond St & Diversey Ave          15645  41.945244 -87.706650   \n",
       "1                Green St & Madison St   TA1307000120  41.879472 -87.625689   \n",
       "2         California Ave & Fletcher St          15642  41.915983 -87.677335   \n",
       "3  Orleans St & Merchandise Mart Plaza   TA1305000022  41.902973 -87.631280   \n",
       "4            Damen Ave & Charleston St          13288  41.898418 -87.686596   \n",
       "\n",
       "     end_lat    end_lng member_casual              __source_file  \n",
       "0  41.931902 -87.701195        member  202504-divvy-tripdata.csv  \n",
       "1  41.881859 -87.649264        member  202504-divvy-tripdata.csv  \n",
       "2  41.938429 -87.698008        member  202504-divvy-tripdata.csv  \n",
       "3  41.888243 -87.636390        member  202504-divvy-tripdata.csv  \n",
       "4  41.920082 -87.677855        member  202504-divvy-tripdata.csv  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset folder (Windows path)\n",
    "dataset_dir = Path(r\"D:\\myfiles\\project\\bike_prediction\\dataset\")\n",
    "# find files with 'divvy' in the filename\n",
    "csv_files = sorted(dataset_dir.glob(\"*divvy*.csv\"))\n",
    "print(f\"Found {len(csv_files)} file(s):\")\n",
    "for p in csv_files:\n",
    "    print(' -', p.name)\n",
    "\n",
    "# read them into dataframes and tag source filename\n",
    "dfs = []\n",
    "for f in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        df['__source_file'] = f.name\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {f.name}: {e}\")\n",
    "\n",
    "# combine (if any)\n",
    "if dfs:\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "else:\n",
    "    combined = pd.DataFrame()\n",
    "\n",
    "print('Combined shape:', combined.shape)\n",
    "combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56470f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly flow (station_id, hour, inflow, outflow) shape: (11486016, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "station_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hour",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "inflow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "outflow",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6a15a2d5-0c0c-46d1-b2ff-532bfb54a942",
       "rows": [
        [
         "0",
         "  24321",
         "2025-04-01 00:00:00",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "  24321",
         "2025-04-01 01:00:00",
         "0.0",
         "0.0"
        ],
        [
         "2",
         "  24321",
         "2025-04-01 02:00:00",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "  24321",
         "2025-04-01 03:00:00",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "  24321",
         "2025-04-01 04:00:00",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "  24321",
         "2025-04-01 05:00:00",
         "0.0",
         "0.0"
        ],
        [
         "6",
         "  24321",
         "2025-04-01 06:00:00",
         "0.0",
         "0.0"
        ],
        [
         "7",
         "  24321",
         "2025-04-01 07:00:00",
         "0.0",
         "0.0"
        ],
        [
         "8",
         "  24321",
         "2025-04-01 08:00:00",
         "0.0",
         "0.0"
        ],
        [
         "9",
         "  24321",
         "2025-04-01 09:00:00",
         "0.0",
         "0.0"
        ],
        [
         "10",
         "  24321",
         "2025-04-01 10:00:00",
         "0.0",
         "0.0"
        ],
        [
         "11",
         "  24321",
         "2025-04-01 11:00:00",
         "0.0",
         "0.0"
        ],
        [
         "12",
         "  24321",
         "2025-04-01 12:00:00",
         "0.0",
         "0.0"
        ],
        [
         "13",
         "  24321",
         "2025-04-01 13:00:00",
         "0.0",
         "0.0"
        ],
        [
         "14",
         "  24321",
         "2025-04-01 14:00:00",
         "0.0",
         "0.0"
        ],
        [
         "15",
         "  24321",
         "2025-04-01 15:00:00",
         "0.0",
         "0.0"
        ],
        [
         "16",
         "  24321",
         "2025-04-01 16:00:00",
         "0.0",
         "0.0"
        ],
        [
         "17",
         "  24321",
         "2025-04-01 17:00:00",
         "0.0",
         "0.0"
        ],
        [
         "18",
         "  24321",
         "2025-04-01 18:00:00",
         "0.0",
         "0.0"
        ],
        [
         "19",
         "  24321",
         "2025-04-01 19:00:00",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>inflow</th>\n",
       "      <th>outflow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 05:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 06:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 07:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 08:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 09:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 10:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 11:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 12:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 13:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 14:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 15:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 16:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 17:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 18:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01 19:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id                hour  inflow  outflow\n",
       "0       24321 2025-04-01 00:00:00     0.0      0.0\n",
       "1       24321 2025-04-01 01:00:00     0.0      0.0\n",
       "2       24321 2025-04-01 02:00:00     0.0      0.0\n",
       "3       24321 2025-04-01 03:00:00     0.0      0.0\n",
       "4       24321 2025-04-01 04:00:00     0.0      0.0\n",
       "5       24321 2025-04-01 05:00:00     0.0      0.0\n",
       "6       24321 2025-04-01 06:00:00     0.0      0.0\n",
       "7       24321 2025-04-01 07:00:00     0.0      0.0\n",
       "8       24321 2025-04-01 08:00:00     0.0      0.0\n",
       "9       24321 2025-04-01 09:00:00     0.0      0.0\n",
       "10      24321 2025-04-01 10:00:00     0.0      0.0\n",
       "11      24321 2025-04-01 11:00:00     0.0      0.0\n",
       "12      24321 2025-04-01 12:00:00     0.0      0.0\n",
       "13      24321 2025-04-01 13:00:00     0.0      0.0\n",
       "14      24321 2025-04-01 14:00:00     0.0      0.0\n",
       "15      24321 2025-04-01 15:00:00     0.0      0.0\n",
       "16      24321 2025-04-01 16:00:00     0.0      0.0\n",
       "17      24321 2025-04-01 17:00:00     0.0      0.0\n",
       "18      24321 2025-04-01 18:00:00     0.0      0.0\n",
       "19      24321 2025-04-01 19:00:00     0.0      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aggregate hourly inflow/outflow per station\n",
    "\n",
    "# Ensure the combined orders table exists\n",
    "if 'combined' not in globals():\n",
    "    print(\"Variable 'combined' not found. Run the previous cell that reads CSVs to create it.\")\n",
    "else:\n",
    "    df = combined.copy()\n",
    "\n",
    "    # Ensure datetime columns are parsed\n",
    "    for col in ['started_at', 'ended_at']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    # 统一station_id类型为字符串，避免mixtype问题\n",
    "    for col in ['start_station_id', 'end_station_id']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "\n",
    "    # OUTFLOW: use started_at and start_station_id\n",
    "    out = df.dropna(subset=['started_at', 'start_station_id']).copy()\n",
    "    out['hour'] = out['started_at'].dt.floor('h')\n",
    "    outflow = (\n",
    "        out.groupby(['start_station_id', 'hour'])\n",
    "        .size()\n",
    "        .reset_index(name='outflow')\n",
    "        .rename(columns={'start_station_id': 'station_id'})\n",
    "    )\n",
    "\n",
    "    # INFLOW: use ended_at and end_station_id\n",
    "    inn = df.dropna(subset=['ended_at', 'end_station_id']).copy()\n",
    "    inn['hour'] = inn['ended_at'].dt.floor('h')\n",
    "    inflow = (\n",
    "        inn.groupby(['end_station_id', 'hour'])\n",
    "        .size()\n",
    "        .reset_index(name='inflow')\n",
    "        .rename(columns={'end_station_id': 'station_id'})\n",
    "    )\n",
    "\n",
    "    # Merge inflow and outflow\n",
    "    hourly = pd.merge(inflow, outflow, on=['station_id', 'hour'], how='outer').fillna(0)\n",
    "\n",
    "    # Try to convert station_id to numeric if possible\n",
    "    try:\n",
    "        hourly['station_id'] = pd.to_numeric(hourly['station_id'])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    hourly = hourly.sort_values(['station_id', 'hour']).reset_index(drop=True)\n",
    "\n",
    "    # Create a complete date range from 2025-04-01 00:00 to 2025-08-31 23:00\n",
    "    start_time = pd.Timestamp('2025-04-01 00:00:00')\n",
    "    end_time = pd.Timestamp('2025-08-31 23:00:00')\n",
    "    full_hours = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "    \n",
    "    # Get all unique station IDs\n",
    "    all_stations = hourly['station_id'].unique()\n",
    "    \n",
    "    # Create a complete cartesian product of (station_id, hour)\n",
    "    complete_index = pd.MultiIndex.from_product(\n",
    "        [all_stations, full_hours],\n",
    "        names=['station_id', 'hour']\n",
    "    )\n",
    "    complete_df = pd.DataFrame(index=complete_index).reset_index()\n",
    "    \n",
    "    # Merge with hourly data and fill missing values with 0\n",
    "    hourly = pd.merge(complete_df, hourly, on=['station_id', 'hour'], how='left').fillna(0)\n",
    "    hourly = hourly.sort_values(['station_id', 'hour']).reset_index(drop=True)\n",
    "\n",
    "    print('Hourly flow (station_id, hour, inflow, outflow) shape:', hourly.shape)\n",
    "    display(hourly.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbcdc73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_station_id    3072\n",
      "total_outflow       3072\n",
      "dtype: int64\n",
      "start_station_id    653\n",
      "total_outflow       653\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "out1 = out.groupby(['start_station_id']).size().sort_values(ascending=False).reset_index(name='total_outflow')\n",
    "# out1.head(50)\n",
    "print(out1.count())\n",
    "print(out1[out1['total_outflow']>1000].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1887ff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result shape: (11410944, 7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "station_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "hour",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "hour_of_day",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "future_12h_inflow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "future_12h_outflow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "future_12h_netflow",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a399c0d8-d650-468b-8ddf-c6de72d6c0a0",
       "rows": [
        [
         "0",
         "  24321",
         "2025-04-01",
         "2025-04-01 00:00:00",
         "0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "  24321",
         "2025-04-01",
         "2025-04-01 01:00:00",
         "1",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2",
         "  24321",
         "2025-04-01",
         "2025-04-01 02:00:00",
         "2",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "  24321",
         "2025-04-01",
         "2025-04-01 03:00:00",
         "3",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "  24321",
         "2025-04-01",
         "2025-04-01 04:00:00",
         "4",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "  24321",
         "2025-04-01",
         "2025-04-01 05:00:00",
         "5",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "6",
         "  24321",
         "2025-04-01",
         "2025-04-01 06:00:00",
         "6",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "7",
         "  24321",
         "2025-04-01",
         "2025-04-01 07:00:00",
         "7",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "8",
         "  24321",
         "2025-04-01",
         "2025-04-01 08:00:00",
         "8",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "9",
         "  24321",
         "2025-04-01",
         "2025-04-01 09:00:00",
         "9",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "10",
         "  24321",
         "2025-04-01",
         "2025-04-01 10:00:00",
         "10",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "11",
         "  24321",
         "2025-04-01",
         "2025-04-01 11:00:00",
         "11",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "12",
         "  24321",
         "2025-04-01",
         "2025-04-01 12:00:00",
         "12",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "13",
         "  24321",
         "2025-04-01",
         "2025-04-01 13:00:00",
         "13",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "14",
         "  24321",
         "2025-04-01",
         "2025-04-01 14:00:00",
         "14",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "15",
         "  24321",
         "2025-04-01",
         "2025-04-01 15:00:00",
         "15",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "16",
         "  24321",
         "2025-04-01",
         "2025-04-01 16:00:00",
         "16",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "17",
         "  24321",
         "2025-04-01",
         "2025-04-01 17:00:00",
         "17",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "18",
         "  24321",
         "2025-04-01",
         "2025-04-01 18:00:00",
         "18",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "19",
         "  24321",
         "2025-04-01",
         "2025-04-01 19:00:00",
         "19",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>future_12h_inflow</th>\n",
       "      <th>future_12h_outflow</th>\n",
       "      <th>future_12h_netflow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 05:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 06:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 07:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 10:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 11:00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 12:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 13:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 14:00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 15:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 16:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 17:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 18:00:00</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24321</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 19:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id        date                hour  hour_of_day  future_12h_inflow  \\\n",
       "0       24321  2025-04-01 2025-04-01 00:00:00            0                0.0   \n",
       "1       24321  2025-04-01 2025-04-01 01:00:00            1                0.0   \n",
       "2       24321  2025-04-01 2025-04-01 02:00:00            2                0.0   \n",
       "3       24321  2025-04-01 2025-04-01 03:00:00            3                0.0   \n",
       "4       24321  2025-04-01 2025-04-01 04:00:00            4                0.0   \n",
       "5       24321  2025-04-01 2025-04-01 05:00:00            5                0.0   \n",
       "6       24321  2025-04-01 2025-04-01 06:00:00            6                0.0   \n",
       "7       24321  2025-04-01 2025-04-01 07:00:00            7                0.0   \n",
       "8       24321  2025-04-01 2025-04-01 08:00:00            8                0.0   \n",
       "9       24321  2025-04-01 2025-04-01 09:00:00            9                0.0   \n",
       "10      24321  2025-04-01 2025-04-01 10:00:00           10                0.0   \n",
       "11      24321  2025-04-01 2025-04-01 11:00:00           11                0.0   \n",
       "12      24321  2025-04-01 2025-04-01 12:00:00           12                0.0   \n",
       "13      24321  2025-04-01 2025-04-01 13:00:00           13                0.0   \n",
       "14      24321  2025-04-01 2025-04-01 14:00:00           14                0.0   \n",
       "15      24321  2025-04-01 2025-04-01 15:00:00           15                0.0   \n",
       "16      24321  2025-04-01 2025-04-01 16:00:00           16                0.0   \n",
       "17      24321  2025-04-01 2025-04-01 17:00:00           17                0.0   \n",
       "18      24321  2025-04-01 2025-04-01 18:00:00           18                0.0   \n",
       "19      24321  2025-04-01 2025-04-01 19:00:00           19                0.0   \n",
       "\n",
       "    future_12h_outflow  future_12h_netflow  \n",
       "0                  0.0                 0.0  \n",
       "1                  0.0                 0.0  \n",
       "2                  0.0                 0.0  \n",
       "3                  0.0                 0.0  \n",
       "4                  0.0                 0.0  \n",
       "5                  0.0                 0.0  \n",
       "6                  0.0                 0.0  \n",
       "7                  0.0                 0.0  \n",
       "8                  0.0                 0.0  \n",
       "9                  0.0                 0.0  \n",
       "10                 0.0                 0.0  \n",
       "11                 0.0                 0.0  \n",
       "12                 0.0                 0.0  \n",
       "13                 0.0                 0.0  \n",
       "14                 0.0                 0.0  \n",
       "15                 0.0                 0.0  \n",
       "16                 0.0                 0.0  \n",
       "17                 0.0                 0.0  \n",
       "18                 0.0                 0.0  \n",
       "19                 0.0                 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hourly.columns: ['station_id', 'hour', 'inflow', 'outflow']\n",
    "\n",
    "# 确保排序正确\n",
    "hourly = hourly.sort_values(['station_id', 'hour']).reset_index(drop=True)\n",
    "\n",
    "# 为每个站点计算未来12小时的累计 inflow/outflow\n",
    "hourly['future_12h_inflow'] = (\n",
    "    hourly.groupby('station_id')['inflow']\n",
    "    .transform(lambda x: x[::-1].rolling(window=12, min_periods=12).sum()[::-1])\n",
    ")\n",
    "\n",
    "hourly['future_12h_outflow'] = (\n",
    "    hourly.groupby('station_id')['outflow']\n",
    "    .transform(lambda x: x[::-1].rolling(window=12, min_periods=12).sum()[::-1])\n",
    ")\n",
    "\n",
    "# 计算未来12小时净流出\n",
    "hourly['future_12h_netflow'] = hourly['future_12h_inflow'] - hourly['future_12h_outflow']\n",
    "\n",
    "# 提取日期和小时（方便查看）\n",
    "hourly['date'] = hourly['hour'].dt.date\n",
    "hourly['hour_of_day'] = hourly['hour'].dt.hour\n",
    "\n",
    "# 仅保留到 8月30日0时（因为再往后不足12小时）\n",
    "cutoff = pd.Timestamp('2025-08-30 23:00:00')\n",
    "hourly_future12 = hourly[hourly['hour'] <= cutoff].copy()\n",
    "\n",
    "# 选择输出列\n",
    "hourly_future12 = hourly_future12[[\n",
    "    'station_id', 'date', 'hour', 'hour_of_day', \n",
    "    'future_12h_inflow', 'future_12h_outflow', 'future_12h_netflow'\n",
    "]]\n",
    "\n",
    "print(\"Result shape:\", hourly_future12.shape)\n",
    "display(hourly_future12.head(20))\n",
    "\n",
    "\n",
    "hourly_future12.to_csv('site_feature12_df_divvy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4fe971b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_future12.to_csv('site_feature12_df_divvy.csv', index=False)\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652556d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Reading feature12_df.csv...\n",
      "Loaded shape: (11410944, 7)\n",
      "Columns: ['station_id', 'date', 'hour', 'hour_of_day', 'future_12h_inflow', 'future_12h_outflow', 'future_12h_netflow']\n",
      "\n",
      "Step 2: Filtering stations by average daily orders (Apr 1 - Jul 1)...\n",
      "Found 753 stations with avg daily orders >= 10\n",
      "Sample stations: ['13001', '13006', '13008', '13011', '13016']\n",
      "\n",
      "Step 3: Filtering training samples from future12_df...\n",
      "Found 753 stations with avg daily orders >= 10\n",
      "Sample stations: ['13001', '13006', '13008', '13011', '13016']\n",
      "\n",
      "Step 3: Filtering training samples from future12_df...\n",
      "Training samples shape: (2746944, 7)\n",
      "\n",
      "Step 4: Creating lag features (12h_max_net_flow from 1d/2d/6d/13d ago)...\n",
      "Training samples shape: (2746944, 7)\n",
      "\n",
      "Step 4: Creating lag features (12h_max_net_flow from 1d/2d/6d/13d ago)...\n",
      "\n",
      "Final training dataframe shape: (2746944, 11)\n",
      "Columns: ['station_id', 'date', 'hour', 'hour_of_day', 'future_12h_inflow', 'future_12h_outflow', 'future_12h_netflow', 'net_flow_12h_lag_1d', 'net_flow_12h_lag_2d', 'net_flow_12h_lag_6d', 'net_flow_12h_lag_13d']\n",
      "\n",
      "First 20 rows:\n",
      "\n",
      "Final training dataframe shape: (2746944, 11)\n",
      "Columns: ['station_id', 'date', 'hour', 'hour_of_day', 'future_12h_inflow', 'future_12h_outflow', 'future_12h_netflow', 'net_flow_12h_lag_1d', 'net_flow_12h_lag_2d', 'net_flow_12h_lag_6d', 'net_flow_12h_lag_13d']\n",
      "\n",
      "First 20 rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "station_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "hour",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "hour_of_day",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "future_12h_inflow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "future_12h_outflow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "future_12h_netflow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "net_flow_12h_lag_1d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "net_flow_12h_lag_2d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "net_flow_12h_lag_6d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "net_flow_12h_lag_13d",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2040c76a-201b-4cd8-abe6-9e50f577887a",
       "rows": [
        [
         "0",
         "13001",
         "2025-04-01",
         "2025-04-01 00:00:00",
         "0",
         "23.0",
         "13.0",
         "10.0",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "13001",
         "2025-04-01",
         "2025-04-01 01:00:00",
         "1",
         "23.0",
         "16.0",
         "7.0",
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "13001",
         "2025-04-01",
         "2025-04-01 02:00:00",
         "2",
         "25.0",
         "18.0",
         "7.0",
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "13001",
         "2025-04-01",
         "2025-04-01 03:00:00",
         "3",
         "28.0",
         "19.0",
         "9.0",
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "13001",
         "2025-04-01",
         "2025-04-01 04:00:00",
         "4",
         "31.0",
         "22.0",
         "9.0",
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "13001",
         "2025-04-01",
         "2025-04-01 05:00:00",
         "5",
         "33.0",
         "30.0",
         "3.0",
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "13001",
         "2025-04-01",
         "2025-04-01 06:00:00",
         "6",
         "36.0",
         "36.0",
         "0.0",
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "13001",
         "2025-04-01",
         "2025-04-01 07:00:00",
         "7",
         "40.0",
         "36.0",
         "4.0",
         null,
         null,
         null,
         null
        ],
        [
         "8",
         "13001",
         "2025-04-01",
         "2025-04-01 08:00:00",
         "8",
         "34.0",
         "33.0",
         "1.0",
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "13001",
         "2025-04-01",
         "2025-04-01 09:00:00",
         "9",
         "26.0",
         "33.0",
         "-7.0",
         null,
         null,
         null,
         null
        ],
        [
         "10",
         "13001",
         "2025-04-01",
         "2025-04-01 10:00:00",
         "10",
         "25.0",
         "33.0",
         "-8.0",
         null,
         null,
         null,
         null
        ],
        [
         "11",
         "13001",
         "2025-04-01",
         "2025-04-01 11:00:00",
         "11",
         "24.0",
         "30.0",
         "-6.0",
         null,
         null,
         null,
         null
        ],
        [
         "12",
         "13001",
         "2025-04-01",
         "2025-04-01 12:00:00",
         "12",
         "23.0",
         "30.0",
         "-7.0",
         null,
         null,
         null,
         null
        ],
        [
         "13",
         "13001",
         "2025-04-01",
         "2025-04-01 13:00:00",
         "13",
         "24.0",
         "27.0",
         "-3.0",
         null,
         null,
         null,
         null
        ],
        [
         "14",
         "13001",
         "2025-04-01",
         "2025-04-01 14:00:00",
         "14",
         "22.0",
         "25.0",
         "-3.0",
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "13001",
         "2025-04-01",
         "2025-04-01 15:00:00",
         "15",
         "19.0",
         "24.0",
         "-5.0",
         null,
         null,
         null,
         null
        ],
        [
         "16",
         "13001",
         "2025-04-01",
         "2025-04-01 16:00:00",
         "16",
         "16.0",
         "21.0",
         "-5.0",
         null,
         null,
         null,
         null
        ],
        [
         "17",
         "13001",
         "2025-04-01",
         "2025-04-01 17:00:00",
         "17",
         "14.0",
         "13.0",
         "1.0",
         null,
         null,
         null,
         null
        ],
        [
         "18",
         "13001",
         "2025-04-01",
         "2025-04-01 18:00:00",
         "18",
         "11.0",
         "6.0",
         "5.0",
         null,
         null,
         null,
         null
        ],
        [
         "19",
         "13001",
         "2025-04-01",
         "2025-04-01 19:00:00",
         "19",
         "4.0",
         "5.0",
         "-1.0",
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>future_12h_inflow</th>\n",
       "      <th>future_12h_outflow</th>\n",
       "      <th>future_12h_netflow</th>\n",
       "      <th>net_flow_12h_lag_1d</th>\n",
       "      <th>net_flow_12h_lag_2d</th>\n",
       "      <th>net_flow_12h_lag_6d</th>\n",
       "      <th>net_flow_12h_lag_13d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 05:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 06:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 07:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>40.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>26.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 10:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>25.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 11:00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 12:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 13:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 14:00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 15:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 16:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 17:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 18:00:00</td>\n",
       "      <td>18</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 19:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id        date                hour  hour_of_day  future_12h_inflow  \\\n",
       "0       13001  2025-04-01 2025-04-01 00:00:00            0               23.0   \n",
       "1       13001  2025-04-01 2025-04-01 01:00:00            1               23.0   \n",
       "2       13001  2025-04-01 2025-04-01 02:00:00            2               25.0   \n",
       "3       13001  2025-04-01 2025-04-01 03:00:00            3               28.0   \n",
       "4       13001  2025-04-01 2025-04-01 04:00:00            4               31.0   \n",
       "5       13001  2025-04-01 2025-04-01 05:00:00            5               33.0   \n",
       "6       13001  2025-04-01 2025-04-01 06:00:00            6               36.0   \n",
       "7       13001  2025-04-01 2025-04-01 07:00:00            7               40.0   \n",
       "8       13001  2025-04-01 2025-04-01 08:00:00            8               34.0   \n",
       "9       13001  2025-04-01 2025-04-01 09:00:00            9               26.0   \n",
       "10      13001  2025-04-01 2025-04-01 10:00:00           10               25.0   \n",
       "11      13001  2025-04-01 2025-04-01 11:00:00           11               24.0   \n",
       "12      13001  2025-04-01 2025-04-01 12:00:00           12               23.0   \n",
       "13      13001  2025-04-01 2025-04-01 13:00:00           13               24.0   \n",
       "14      13001  2025-04-01 2025-04-01 14:00:00           14               22.0   \n",
       "15      13001  2025-04-01 2025-04-01 15:00:00           15               19.0   \n",
       "16      13001  2025-04-01 2025-04-01 16:00:00           16               16.0   \n",
       "17      13001  2025-04-01 2025-04-01 17:00:00           17               14.0   \n",
       "18      13001  2025-04-01 2025-04-01 18:00:00           18               11.0   \n",
       "19      13001  2025-04-01 2025-04-01 19:00:00           19                4.0   \n",
       "\n",
       "    future_12h_outflow  future_12h_netflow  net_flow_12h_lag_1d  \\\n",
       "0                 13.0                10.0                  NaN   \n",
       "1                 16.0                 7.0                  NaN   \n",
       "2                 18.0                 7.0                  NaN   \n",
       "3                 19.0                 9.0                  NaN   \n",
       "4                 22.0                 9.0                  NaN   \n",
       "5                 30.0                 3.0                  NaN   \n",
       "6                 36.0                 0.0                  NaN   \n",
       "7                 36.0                 4.0                  NaN   \n",
       "8                 33.0                 1.0                  NaN   \n",
       "9                 33.0                -7.0                  NaN   \n",
       "10                33.0                -8.0                  NaN   \n",
       "11                30.0                -6.0                  NaN   \n",
       "12                30.0                -7.0                  NaN   \n",
       "13                27.0                -3.0                  NaN   \n",
       "14                25.0                -3.0                  NaN   \n",
       "15                24.0                -5.0                  NaN   \n",
       "16                21.0                -5.0                  NaN   \n",
       "17                13.0                 1.0                  NaN   \n",
       "18                 6.0                 5.0                  NaN   \n",
       "19                 5.0                -1.0                  NaN   \n",
       "\n",
       "    net_flow_12h_lag_2d  net_flow_12h_lag_6d  net_flow_12h_lag_13d  \n",
       "0                   NaN                  NaN                   NaN  \n",
       "1                   NaN                  NaN                   NaN  \n",
       "2                   NaN                  NaN                   NaN  \n",
       "3                   NaN                  NaN                   NaN  \n",
       "4                   NaN                  NaN                   NaN  \n",
       "5                   NaN                  NaN                   NaN  \n",
       "6                   NaN                  NaN                   NaN  \n",
       "7                   NaN                  NaN                   NaN  \n",
       "8                   NaN                  NaN                   NaN  \n",
       "9                   NaN                  NaN                   NaN  \n",
       "10                  NaN                  NaN                   NaN  \n",
       "11                  NaN                  NaN                   NaN  \n",
       "12                  NaN                  NaN                   NaN  \n",
       "13                  NaN                  NaN                   NaN  \n",
       "14                  NaN                  NaN                   NaN  \n",
       "15                  NaN                  NaN                   NaN  \n",
       "16                  NaN                  NaN                   NaN  \n",
       "17                  NaN                  NaN                   NaN  \n",
       "18                  NaN                  NaN                   NaN  \n",
       "19                  NaN                  NaN                   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Station ID统计: 共753个不同的站点\n",
      "每站行数统计: min=3648, max=3648, unique_counts=1\n",
      "\n",
      "Missing values:\n",
      "station_id                   0\n",
      "date                         0\n",
      "hour                         0\n",
      "hour_of_day                  0\n",
      "future_12h_inflow            0\n",
      "future_12h_outflow           0\n",
      "future_12h_netflow           0\n",
      "net_flow_12h_lag_1d      18072\n",
      "net_flow_12h_lag_2d      36144\n",
      "net_flow_12h_lag_6d     108432\n",
      "net_flow_12h_lag_13d    234936\n",
      "dtype: int64\n",
      "station_id                   0\n",
      "date                         0\n",
      "hour                         0\n",
      "hour_of_day                  0\n",
      "future_12h_inflow            0\n",
      "future_12h_outflow           0\n",
      "future_12h_netflow           0\n",
      "net_flow_12h_lag_1d      18072\n",
      "net_flow_12h_lag_2d      36144\n",
      "net_flow_12h_lag_6d     108432\n",
      "net_flow_12h_lag_13d    234936\n",
      "dtype: int64\n",
      "\n",
      "Train sample dataframe saved as 'lag_feature_df'\n",
      "\n",
      "Train sample dataframe saved as 'lag_feature_df'\n"
     ]
    }
   ],
   "source": [
    "# 第一步:读取feature12_df.csv\n",
    "print(\"Step 1: Reading feature12_df.csv...\")\n",
    "# hourly_future12 = pd.read_csv('feature12_df.csv')\n",
    "# hourly_future12['hour'] = pd.to_datetime(hourly_future12['hour'], errors='coerce')\n",
    "print(f\"Loaded shape: {hourly_future12.shape}\")\n",
    "print(f\"Columns: {hourly_future12.columns.tolist()}\")\n",
    "\n",
    "# 第二步：从原始订单表中筛选4月1日-7月1日平均每日流出订单量>=10的点位\n",
    "if 'combined' not in globals():\n",
    "    print(\"ERROR: 'combined' not found. Run the previous cells first.\")\n",
    "else:\n",
    "    print(\"\\nStep 2: Filtering stations by average daily orders (Apr 1 - Jul 1)...\")\n",
    "    df_orders = combined.copy()\n",
    "    \n",
    "    # 统一station_id类型为字符串，避免mixtype问题\n",
    "    for col in ['start_station_id', 'end_station_id']:\n",
    "        if col in df_orders.columns:\n",
    "            df_orders[col] = df_orders[col].astype(str)\n",
    "    \n",
    "    # 解析时间列\n",
    "    for col in ['started_at', 'ended_at']:\n",
    "        if col in df_orders.columns:\n",
    "            df_orders[col] = pd.to_datetime(df_orders[col], errors='coerce')\n",
    "    \n",
    "    # 筛选时间范围：2025-04-01 到 2025-07-01\n",
    "    start_date = pd.Timestamp('2025-04-01')\n",
    "    end_date = pd.Timestamp('2025-07-01')\n",
    "    \n",
    "    # 按started_at筛选出发站点\n",
    "    df_out = df_orders[(df_orders['started_at'] >= start_date) & (df_orders['started_at'] < end_date)].copy()\n",
    "    df_out['date'] = df_out['started_at'].dt.date\n",
    "    \n",
    "    # 计算每个站点每日的出发订单量\n",
    "    daily_out = df_out.groupby(['start_station_id', 'date']).size().reset_index(name='count')\n",
    " \n",
    "    # 对于每个站点，计算平均每日订单量\n",
    "    avg_daily_out = daily_out.groupby('start_station_id')['count'].mean().reset_index(name='avg_daily_orders')\n",
    "    \n",
    "    # 筛选平均每日订单量 >= 10 的站点\n",
    "    high_traffic_stations = avg_daily_out[avg_daily_out['avg_daily_orders'] >= 10]['start_station_id'].unique()\n",
    "    print(f\"Found {len(high_traffic_stations)} stations with avg daily orders >= 10\")\n",
    "    print(f\"Sample stations: {high_traffic_stations[:5].tolist()}\")\n",
    "    \n",
    "    # 统一hourly_future12的station_id为字符串类型\n",
    "    hourly_future12['station_id'] = hourly_future12['station_id'].astype(str)\n",
    "    \n",
    "    # 第三步：在future12_df中筛选这些站点作为训练样本\n",
    "    print(\"\\nStep 3: Filtering training samples from future12_df...\")\n",
    "    lag_feature_df = hourly_future12[hourly_future12['station_id'].isin(high_traffic_stations)].copy()\n",
    "    print(f\"Training samples shape: {lag_feature_df.shape}\")\n",
    "    \n",
    "    # 第四步：生成lag特征\n",
    "    print(\"\\nStep 4: Creating lag features (12h_max_net_flow from 1d/2d/6d/13d ago)...\")\n",
    "    \n",
    "    # 确保按站点和小时排序\n",
    "    lag_feature_df = lag_feature_df.sort_values(['station_id', 'hour']).reset_index(drop=True)\n",
    "    \n",
    "    # 定义lag周期（以小时为单位）\n",
    "    lag_days = [1, 2, 6, 13]\n",
    "    lag_hours = {1: 24, 2: 48, 6: 144, 13: 312}  # 1天=24小时，2天=48小时等\n",
    "    \n",
    "    for days, hours_lag in lag_hours.items():\n",
    "        col_name = f'net_flow_12h_lag_{days}d'\n",
    "        lag_feature_df[col_name] = lag_feature_df.groupby('station_id')['future_12h_netflow'].shift(hours_lag)\n",
    "    \n",
    "    # 显示结果\n",
    "    print(\"\\nFinal training dataframe shape:\", lag_feature_df.shape)\n",
    "    print(\"Columns:\", lag_feature_df.columns.tolist())\n",
    "    print(\"\\nFirst 20 rows:\")\n",
    "    display(lag_feature_df.head(20))\n",
    "    \n",
    "    # 检查station_id是否有重复问题\n",
    "    station_counts = lag_feature_df.groupby('station_id').size()\n",
    "    print(f\"\\nStation ID统计: 共{len(station_counts)}个不同的站点\")\n",
    "    print(f\"每站行数统计: min={station_counts.min()}, max={station_counts.max()}, unique_counts={station_counts.nunique()}\")\n",
    "    \n",
    "    # 显示缺失值统计\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(lag_feature_df.isnull().sum())\n",
    "    \n",
    "    # 保存为变量供后续使用\n",
    "    lag_feature_df.to_csv('lag_feature_df_divvy.csv', index=False)\n",
    "    print(\"\\nTrain sample dataframe saved as 'lag_feature_df'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d3b8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zusong Zhang\\AppData\\Local\\Temp\\ipykernel_28928\\3698132358.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lag_feature_df = pd.read_csv('lag_feature_df_divvy.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2743296, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lag_feature_df = pd.read_csv('lag_feature_df_divvy.csv')\n",
    "lag_feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f9a178",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lag_feature_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlag_feature_df\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mstation_id\u001b[39m\u001b[33m'\u001b[39m].value_counts()\n",
      "\u001b[31mNameError\u001b[39m: name 'lag_feature_df' is not defined"
     ]
    }
   ],
   "source": [
    "lag_feature_df['station_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76003942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "station_id",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hour",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hour_of_day",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "future_12h_inflow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "future_12h_outflow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "future_12h_netflow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "net_flow_12h_lag_1d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "net_flow_12h_lag_2d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "net_flow_12h_lag_6d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "net_flow_12h_lag_13d",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e31428e0-679d-4e22-bcae-c78bb4458407",
       "rows": [
        [
         "0",
         "13001",
         "2025-04-01",
         "2025-04-01 00:00:00",
         "0",
         "23.0",
         "13.0",
         "10.0",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "13001",
         "2025-04-01",
         "2025-04-01 01:00:00",
         "1",
         "23.0",
         "16.0",
         "7.0",
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "13001",
         "2025-04-01",
         "2025-04-01 02:00:00",
         "2",
         "25.0",
         "18.0",
         "7.0",
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "13001",
         "2025-04-01",
         "2025-04-01 03:00:00",
         "3",
         "28.0",
         "19.0",
         "9.0",
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "13001",
         "2025-04-01",
         "2025-04-01 04:00:00",
         "4",
         "31.0",
         "22.0",
         "9.0",
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "13001",
         "2025-04-01",
         "2025-04-01 05:00:00",
         "5",
         "33.0",
         "30.0",
         "3.0",
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "13001",
         "2025-04-01",
         "2025-04-01 06:00:00",
         "6",
         "36.0",
         "36.0",
         "0.0",
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "13001",
         "2025-04-01",
         "2025-04-01 07:00:00",
         "7",
         "40.0",
         "36.0",
         "4.0",
         null,
         null,
         null,
         null
        ],
        [
         "8",
         "13001",
         "2025-04-01",
         "2025-04-01 08:00:00",
         "8",
         "34.0",
         "33.0",
         "1.0",
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "13001",
         "2025-04-01",
         "2025-04-01 09:00:00",
         "9",
         "26.0",
         "33.0",
         "-7.0",
         null,
         null,
         null,
         null
        ],
        [
         "10",
         "13001",
         "2025-04-01",
         "2025-04-01 10:00:00",
         "10",
         "25.0",
         "33.0",
         "-8.0",
         null,
         null,
         null,
         null
        ],
        [
         "11",
         "13001",
         "2025-04-01",
         "2025-04-01 11:00:00",
         "11",
         "24.0",
         "30.0",
         "-6.0",
         null,
         null,
         null,
         null
        ],
        [
         "12",
         "13001",
         "2025-04-01",
         "2025-04-01 12:00:00",
         "12",
         "23.0",
         "30.0",
         "-7.0",
         null,
         null,
         null,
         null
        ],
        [
         "13",
         "13001",
         "2025-04-01",
         "2025-04-01 13:00:00",
         "13",
         "24.0",
         "27.0",
         "-3.0",
         null,
         null,
         null,
         null
        ],
        [
         "14",
         "13001",
         "2025-04-01",
         "2025-04-01 14:00:00",
         "14",
         "22.0",
         "25.0",
         "-3.0",
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "13001",
         "2025-04-01",
         "2025-04-01 15:00:00",
         "15",
         "19.0",
         "24.0",
         "-5.0",
         null,
         null,
         null,
         null
        ],
        [
         "16",
         "13001",
         "2025-04-01",
         "2025-04-01 16:00:00",
         "16",
         "16.0",
         "21.0",
         "-5.0",
         null,
         null,
         null,
         null
        ],
        [
         "17",
         "13001",
         "2025-04-01",
         "2025-04-01 17:00:00",
         "17",
         "14.0",
         "13.0",
         "1.0",
         null,
         null,
         null,
         null
        ],
        [
         "18",
         "13001",
         "2025-04-01",
         "2025-04-01 18:00:00",
         "18",
         "11.0",
         "6.0",
         "5.0",
         null,
         null,
         null,
         null
        ],
        [
         "19",
         "13001",
         "2025-04-01",
         "2025-04-01 19:00:00",
         "19",
         "4.0",
         "5.0",
         "-1.0",
         null,
         null,
         null,
         null
        ],
        [
         "20",
         "13001",
         "2025-04-01",
         "2025-04-01 20:00:00",
         "20",
         "2.0",
         "5.0",
         "-3.0",
         null,
         null,
         null,
         null
        ],
        [
         "21",
         "13001",
         "2025-04-01",
         "2025-04-01 21:00:00",
         "21",
         "5.0",
         "6.0",
         "-1.0",
         null,
         null,
         null,
         null
        ],
        [
         "22",
         "13001",
         "2025-04-01",
         "2025-04-01 22:00:00",
         "22",
         "6.0",
         "5.0",
         "1.0",
         null,
         null,
         null,
         null
        ],
        [
         "23",
         "13001",
         "2025-04-01",
         "2025-04-01 23:00:00",
         "23",
         "6.0",
         "5.0",
         "1.0",
         null,
         null,
         null,
         null
        ],
        [
         "24",
         "13001",
         "2025-04-02",
         "2025-04-02 00:00:00",
         "0",
         "6.0",
         "5.0",
         "1.0",
         "10.0",
         null,
         null,
         null
        ],
        [
         "25",
         "13001",
         "2025-04-02",
         "2025-04-02 01:00:00",
         "1",
         "5.0",
         "5.0",
         "0.0",
         "7.0",
         null,
         null,
         null
        ],
        [
         "26",
         "13001",
         "2025-04-02",
         "2025-04-02 02:00:00",
         "2",
         "5.0",
         "5.0",
         "0.0",
         "7.0",
         null,
         null,
         null
        ],
        [
         "27",
         "13001",
         "2025-04-02",
         "2025-04-02 03:00:00",
         "3",
         "6.0",
         "5.0",
         "1.0",
         "9.0",
         null,
         null,
         null
        ],
        [
         "28",
         "13001",
         "2025-04-02",
         "2025-04-02 04:00:00",
         "4",
         "7.0",
         "8.0",
         "-1.0",
         "9.0",
         null,
         null,
         null
        ],
        [
         "29",
         "13001",
         "2025-04-02",
         "2025-04-02 05:00:00",
         "5",
         "10.0",
         "10.0",
         "0.0",
         "3.0",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 30
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>future_12h_inflow</th>\n",
       "      <th>future_12h_outflow</th>\n",
       "      <th>future_12h_netflow</th>\n",
       "      <th>net_flow_12h_lag_1d</th>\n",
       "      <th>net_flow_12h_lag_2d</th>\n",
       "      <th>net_flow_12h_lag_6d</th>\n",
       "      <th>net_flow_12h_lag_13d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 05:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 06:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 07:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>40.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>26.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 10:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>25.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 11:00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 12:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 13:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 14:00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 15:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 16:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 17:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 18:00:00</td>\n",
       "      <td>18</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 19:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 20:00:00</td>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 21:00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 22:00:00</td>\n",
       "      <td>22</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-04-01 23:00:00</td>\n",
       "      <td>23</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>2025-04-02 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>2025-04-02 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>2025-04-02 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>2025-04-02 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>2025-04-02 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13001</td>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>2025-04-02 05:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id        date                 hour  hour_of_day  \\\n",
       "0       13001  2025-04-01  2025-04-01 00:00:00            0   \n",
       "1       13001  2025-04-01  2025-04-01 01:00:00            1   \n",
       "2       13001  2025-04-01  2025-04-01 02:00:00            2   \n",
       "3       13001  2025-04-01  2025-04-01 03:00:00            3   \n",
       "4       13001  2025-04-01  2025-04-01 04:00:00            4   \n",
       "5       13001  2025-04-01  2025-04-01 05:00:00            5   \n",
       "6       13001  2025-04-01  2025-04-01 06:00:00            6   \n",
       "7       13001  2025-04-01  2025-04-01 07:00:00            7   \n",
       "8       13001  2025-04-01  2025-04-01 08:00:00            8   \n",
       "9       13001  2025-04-01  2025-04-01 09:00:00            9   \n",
       "10      13001  2025-04-01  2025-04-01 10:00:00           10   \n",
       "11      13001  2025-04-01  2025-04-01 11:00:00           11   \n",
       "12      13001  2025-04-01  2025-04-01 12:00:00           12   \n",
       "13      13001  2025-04-01  2025-04-01 13:00:00           13   \n",
       "14      13001  2025-04-01  2025-04-01 14:00:00           14   \n",
       "15      13001  2025-04-01  2025-04-01 15:00:00           15   \n",
       "16      13001  2025-04-01  2025-04-01 16:00:00           16   \n",
       "17      13001  2025-04-01  2025-04-01 17:00:00           17   \n",
       "18      13001  2025-04-01  2025-04-01 18:00:00           18   \n",
       "19      13001  2025-04-01  2025-04-01 19:00:00           19   \n",
       "20      13001  2025-04-01  2025-04-01 20:00:00           20   \n",
       "21      13001  2025-04-01  2025-04-01 21:00:00           21   \n",
       "22      13001  2025-04-01  2025-04-01 22:00:00           22   \n",
       "23      13001  2025-04-01  2025-04-01 23:00:00           23   \n",
       "24      13001  2025-04-02  2025-04-02 00:00:00            0   \n",
       "25      13001  2025-04-02  2025-04-02 01:00:00            1   \n",
       "26      13001  2025-04-02  2025-04-02 02:00:00            2   \n",
       "27      13001  2025-04-02  2025-04-02 03:00:00            3   \n",
       "28      13001  2025-04-02  2025-04-02 04:00:00            4   \n",
       "29      13001  2025-04-02  2025-04-02 05:00:00            5   \n",
       "\n",
       "    future_12h_inflow  future_12h_outflow  future_12h_netflow  \\\n",
       "0                23.0                13.0                10.0   \n",
       "1                23.0                16.0                 7.0   \n",
       "2                25.0                18.0                 7.0   \n",
       "3                28.0                19.0                 9.0   \n",
       "4                31.0                22.0                 9.0   \n",
       "5                33.0                30.0                 3.0   \n",
       "6                36.0                36.0                 0.0   \n",
       "7                40.0                36.0                 4.0   \n",
       "8                34.0                33.0                 1.0   \n",
       "9                26.0                33.0                -7.0   \n",
       "10               25.0                33.0                -8.0   \n",
       "11               24.0                30.0                -6.0   \n",
       "12               23.0                30.0                -7.0   \n",
       "13               24.0                27.0                -3.0   \n",
       "14               22.0                25.0                -3.0   \n",
       "15               19.0                24.0                -5.0   \n",
       "16               16.0                21.0                -5.0   \n",
       "17               14.0                13.0                 1.0   \n",
       "18               11.0                 6.0                 5.0   \n",
       "19                4.0                 5.0                -1.0   \n",
       "20                2.0                 5.0                -3.0   \n",
       "21                5.0                 6.0                -1.0   \n",
       "22                6.0                 5.0                 1.0   \n",
       "23                6.0                 5.0                 1.0   \n",
       "24                6.0                 5.0                 1.0   \n",
       "25                5.0                 5.0                 0.0   \n",
       "26                5.0                 5.0                 0.0   \n",
       "27                6.0                 5.0                 1.0   \n",
       "28                7.0                 8.0                -1.0   \n",
       "29               10.0                10.0                 0.0   \n",
       "\n",
       "    net_flow_12h_lag_1d  net_flow_12h_lag_2d  net_flow_12h_lag_6d  \\\n",
       "0                   NaN                  NaN                  NaN   \n",
       "1                   NaN                  NaN                  NaN   \n",
       "2                   NaN                  NaN                  NaN   \n",
       "3                   NaN                  NaN                  NaN   \n",
       "4                   NaN                  NaN                  NaN   \n",
       "5                   NaN                  NaN                  NaN   \n",
       "6                   NaN                  NaN                  NaN   \n",
       "7                   NaN                  NaN                  NaN   \n",
       "8                   NaN                  NaN                  NaN   \n",
       "9                   NaN                  NaN                  NaN   \n",
       "10                  NaN                  NaN                  NaN   \n",
       "11                  NaN                  NaN                  NaN   \n",
       "12                  NaN                  NaN                  NaN   \n",
       "13                  NaN                  NaN                  NaN   \n",
       "14                  NaN                  NaN                  NaN   \n",
       "15                  NaN                  NaN                  NaN   \n",
       "16                  NaN                  NaN                  NaN   \n",
       "17                  NaN                  NaN                  NaN   \n",
       "18                  NaN                  NaN                  NaN   \n",
       "19                  NaN                  NaN                  NaN   \n",
       "20                  NaN                  NaN                  NaN   \n",
       "21                  NaN                  NaN                  NaN   \n",
       "22                  NaN                  NaN                  NaN   \n",
       "23                  NaN                  NaN                  NaN   \n",
       "24                 10.0                  NaN                  NaN   \n",
       "25                  7.0                  NaN                  NaN   \n",
       "26                  7.0                  NaN                  NaN   \n",
       "27                  9.0                  NaN                  NaN   \n",
       "28                  9.0                  NaN                  NaN   \n",
       "29                  3.0                  NaN                  NaN   \n",
       "\n",
       "    net_flow_12h_lag_13d  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                    NaN  \n",
       "3                    NaN  \n",
       "4                    NaN  \n",
       "5                    NaN  \n",
       "6                    NaN  \n",
       "7                    NaN  \n",
       "8                    NaN  \n",
       "9                    NaN  \n",
       "10                   NaN  \n",
       "11                   NaN  \n",
       "12                   NaN  \n",
       "13                   NaN  \n",
       "14                   NaN  \n",
       "15                   NaN  \n",
       "16                   NaN  \n",
       "17                   NaN  \n",
       "18                   NaN  \n",
       "19                   NaN  \n",
       "20                   NaN  \n",
       "21                   NaN  \n",
       "22                   NaN  \n",
       "23                   NaN  \n",
       "24                   NaN  \n",
       "25                   NaN  \n",
       "26                   NaN  \n",
       "27                   NaN  \n",
       "28                   NaN  \n",
       "29                   NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lag_feature_df[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d853809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using in-memory 'lag_feature_df'.\n",
      "Feature columns (8): ['future_12h_netflow', 'future_12h_inflow', 'future_12h_outflow', 'net_flow_12h_lag_1d', 'net_flow_12h_lag_2d', 'net_flow_12h_lag_6d', 'net_flow_12h_lag_13d', 'hour_of_day']\n",
      "Feature columns (8): ['future_12h_netflow', 'future_12h_inflow', 'future_12h_outflow', 'net_flow_12h_lag_1d', 'net_flow_12h_lag_2d', 'net_flow_12h_lag_6d', 'net_flow_12h_lag_13d', 'hour_of_day']\n",
      "Total stations serialized: 753\n",
      "Total stations serialized: 753\n",
      "Saved CSV: lag_feature_array_divvy.csv\n",
      "Sample station_id: 13001\n",
      "First 2 feature rows:\n",
      "[5.0, 19.0, 14.0, 21.0, -8.0, 8.0, 1.0, 0.0]\n",
      "[5.0, 20.0, 15.0, 19.0, -7.0, 9.0, 0.0, 1.0]\n",
      "Saved CSV: lag_feature_array_divvy.csv\n",
      "Sample station_id: 13001\n",
      "First 2 feature rows:\n",
      "[5.0, 19.0, 14.0, 21.0, -8.0, 8.0, 1.0, 0.0]\n",
      "[5.0, 20.0, 15.0, 19.0, -7.0, 9.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# 将 lag_feature_df 转换为按站点聚合的 JSON 数组并保存\n",
    "# 1) 准备数据：优先使用内存中的 lag_feature_df，否则从磁盘读取\n",
    "if 'lag_feature_df' in globals():\n",
    "    df = lag_feature_df.copy()\n",
    "    print(\"Using in-memory 'lag_feature_df'.\")\n",
    "else:\n",
    "    try:\n",
    "        df = pd.read_csv('lag_feature_df_divvy.csv')\n",
    "        print(\"Loaded 'lag_feature_df_divvy.csv'.\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"未找到 lag_feature_df_divvy.csv，且变量 lag_feature_df 不存在。请先运行上游特征生成单元。\")\n",
    "\n",
    "# 2) 规范时间列（若存在），确保排序稳定后再移除\n",
    "if 'hour' in df.columns:\n",
    "    df['hour'] = pd.to_datetime(df['hour'], errors='coerce')\n",
    "    # 保留不存在none的时间部分\n",
    "    start_date = pd.Timestamp('2025-04-15 00:00:00')\n",
    "    df = df[(df['hour'] >= start_date)]\n",
    "\n",
    "\n",
    "sort_cols = ['station_id'] + (['hour'] if 'hour' in df.columns else [])\n",
    "df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "# 3) 去除不再保留的时间列\n",
    "cols_to_drop = ['date', 'hour']\n",
    "cols_to_keep = ['future_12h_netflow', 'future_12h_inflow', 'future_12h_outflow', \n",
    "                'net_flow_12h_lag_1d', 'net_flow_12h_lag_2d', 'net_flow_12h_lag_6d',\n",
    "                'net_flow_12h_lag_13d', 'hour_of_day']\n",
    "\n",
    "df_nottime = df.drop(columns=cols_to_drop)\n",
    "df_nottime = df_nottime[ ['station_id'] + cols_to_keep]\n",
    "\n",
    "# 4) 选择特征列（除 station_id 外全部作为特征）\n",
    "feature_cols = [c for c in df_nottime.columns if c != 'station_id']\n",
    "print(f\"Feature columns ({len(feature_cols)}):\", feature_cols)\n",
    "\n",
    "# 5) 将每个站点的特征按行组装为二维数组（list of lists）\n",
    "records = []\n",
    "for sid, g in df_nottime.groupby('station_id', sort=False):\n",
    "    features_array = g[feature_cols].values.tolist()\n",
    "    records.append({\n",
    "        'station_id': sid,\n",
    "        'features': features_array\n",
    "    })\n",
    "\n",
    "print(f\"Total stations serialized: {len(records)}\")\n",
    "\n",
    "# 6) 保存为 JSON Lines（每行一个站点的特征数组）和 CSV（第二列为 JSON 字符串）\n",
    "\n",
    "# def _to_serializable(obj):\n",
    "#     if isinstance(obj, (np.integer,)):\n",
    "#         return int(obj)\n",
    "#     if isinstance(obj, (np.floating,)):\n",
    "#         return float(obj)\n",
    "#     if isinstance(obj, (np.ndarray,)):\n",
    "#         return obj.tolist()\n",
    "#     if isinstance(obj, (pd.Timestamp, datetime)):\n",
    "#         return obj.isoformat()\n",
    "#     return obj\n",
    "\n",
    "# jsonl_path = 'lag_feature_array_divvy.jsonl'\n",
    "# with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "#     for rec in records:\n",
    "#         f.write(json.dumps(rec, ensure_ascii=False, default=_to_serializable) + \"\\n\")\n",
    "# print(f\"Saved JSONL: {jsonl_path}\")\n",
    "\n",
    "# 也保存为 CSV，便于快速浏览（features 为 JSON 字符串）\n",
    "csv_path = 'lag_feature_array_divvy.csv'\n",
    "array_df = pd.DataFrame({\n",
    "    'station_id': [r['station_id'] for r in records],\n",
    "    'features': [json.dumps(r['features'], ensure_ascii=False) for r in records]\n",
    "})\n",
    "array_df.to_csv(csv_path, index=False, sep='|')\n",
    "print(f\"Saved CSV: {csv_path}\")\n",
    "\n",
    "# 7) 预览前 1 个站点的前 2 条特征行\n",
    "if records:\n",
    "    sample = records[0]\n",
    "    print(\"Sample station_id:\", sample['station_id'])\n",
    "    print(\"First 2 feature rows:\")\n",
    "    for row in sample['features'][:2]:\n",
    "        print(row)\n",
    "\n",
    "# 暴露结果变量供后续使用\n",
    "lag_feature_array_records = records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cca933db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCN 3D Array shape: (753, 3312, 8)\n",
      "  - 点位数: 753\n",
      "  - 时间步数: 3312\n",
      "  - 特征数: 8\n",
      "\n",
      "Station IDs (first 5): ['13001', '13006', '13008', '13011', '13016']\n",
      "\n",
      "✓ Saved: tcn_data_3d.npy, station_ids.npy\n"
     ]
    }
   ],
   "source": [
    "# 将 lag_feature_array_records 转换为三维数组用于TCN模型训练\n",
    "# 维度: (点位数, 时间步数, 特征数)\n",
    "\n",
    "if 'lag_feature_array_records' not in globals():\n",
    "    print(\"ERROR: 'lag_feature_array_records' not found. Run the previous cell first.\")\n",
    "else:\n",
    "    # 提取所有站点的特征数组\n",
    "    tcn_array_3d = np.array([record['features'] for record in lag_feature_array_records])\n",
    "    \n",
    "    # 提取站点ID列表（用于后续映射）\n",
    "    station_ids = [record['station_id'] for record in lag_feature_array_records]\n",
    "    \n",
    "    print(f\"TCN 3D Array shape: {tcn_array_3d.shape}\")\n",
    "    print(f\"  - 点位数: {tcn_array_3d.shape[0]}\")\n",
    "    print(f\"  - 时间步数: {tcn_array_3d.shape[1]}\")\n",
    "    print(f\"  - 特征数: {tcn_array_3d.shape[2]}\")\n",
    "    print(f\"\\nStation IDs (first 5): {station_ids[:5]}\")\n",
    "    \n",
    "    # 保存为numpy文件\n",
    "    np.save('tcn_data_3d.npy', tcn_array_3d)\n",
    "    np.save('station_ids.npy', np.array(station_ids))\n",
    "    print(f\"\\n✓ Saved: tcn_data_3d.npy, station_ids.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f727a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_seq2seq_io_data(\n",
    "        df, x_offsets, y_offsets, scaler=None\n",
    "):\n",
    "    \"\"\"\n",
    "    data:total flow, average speed, and average occupancy\n",
    "    Generate samples from\n",
    "    :param df: (l,n)->(l,n,,f)\n",
    "    :param x_offsets:\n",
    "    :param y_offsets:\n",
    "    :param scaler:\n",
    "    :return:\n",
    "    # x: (epoch_size, input_length, num_nodes, input_dim)\n",
    "    # y: (epoch_size, output_length, num_nodes, output_dim)\n",
    "    \"\"\"\n",
    "    #num_samples, num_nodes = df.shape\n",
    "    num_samples, num_nodes, num_features = df.shape\n",
    "    print(df.shape)\n",
    "    #print(df[16991,0,0])\n",
    "    #data = np.expand_dims(df.values, axis=-1) #(l,n,1)\n",
    "    feature_list = [df]\n",
    "    #df = df.tolist()\n",
    "\n",
    "    data = np.concatenate(feature_list, axis=-1)  #(l,n,f)\n",
    "    print(\"data shape: \", data.shape)\n",
    "    x, y = [], []\n",
    "    min_t = abs(min(x_offsets))  #11\n",
    "    max_t = abs(num_samples - abs(max(y_offsets)))  # Exclusive\n",
    "    for t in range(min_t, max_t):  # t is the index of the last observation.\n",
    "        x.append(data[t + x_offsets, ...])\n",
    "        y.append(data[t + y_offsets, ...])\n",
    "        #print(\"x shape: \", x.shape, \", y shape: \", y.shape)\n",
    "    x = np.stack(x, axis=0)\n",
    "    y = np.stack(y, axis=0)\n",
    "    print(\"x shape: \", x.shape, \", y shape: \", y.shape)\n",
    "    return x, y  #(l,t,n,f)\n",
    "\n",
    "\n",
    "def generate_train_val_test(filename, output_dir, seq_length_x=168, seq_length_y=24, y_start=1):\n",
    "    \"\"\"\n",
    "    生成训练、验证和测试数据集\n",
    "    :param filename: 输入的npy文件路径\n",
    "    :param output_dir: 输出目录\n",
    "    :param seq_length_x: 输入序列长度，默认168\n",
    "    :param seq_length_y: 输出序列长度，默认24\n",
    "    :param y_start: 预测起始位置，默认1\n",
    "    \"\"\"\n",
    "    df = np.load(filename)\n",
    "    df = df.transpose((1,0,2))  #(length, num of nodes, features)\n",
    "    print(\"df shape: \", df.shape)\n",
    "    #df = pd.read_hdf(args.traffic_df_filename) #(length, num of nodes)\n",
    "    # 0 is the latest observed sample.\n",
    "    x_offsets = np.sort(np.concatenate((np.arange(-(seq_length_x - 1), 1, 1),)))  #[-11, -10,...,0]\n",
    "    # Predict the next one hour\n",
    "    y_offsets = np.sort(np.arange(y_start, (seq_length_y + 1), 1)) #[1,2,...,11,12]\n",
    "    # x: (num_samples, input_length, num_nodes, input_dim)\n",
    "    # y: (num_samples, output_length, num_nodes, output_dim)\n",
    "    x, y = generate_graph_seq2seq_io_data(\n",
    "        df,\n",
    "        x_offsets=x_offsets,\n",
    "        y_offsets=y_offsets,\n",
    "    )\n",
    "\n",
    "    print(\"x shape: \", x.shape, \", y shape: \", y.shape)  #(l,t,n,f)\n",
    "    # Write the data into npz file.\n",
    "    num_samples = x.shape[0]\n",
    "    num_test = round(num_samples * 0.2)\n",
    "    num_train = round(num_samples * 0.6)\n",
    "    num_val = num_samples - num_test - num_train\n",
    "    x_train, y_train = x[:num_train], y[:num_train]\n",
    "    x_val, y_val = (\n",
    "        x[num_train: num_train + num_val],\n",
    "        y[num_train: num_train + num_val],\n",
    "    )\n",
    "    x_test, y_test = x[-num_test:], y[-num_test:]\n",
    "\n",
    "    for cat in [\"train\", \"val\", \"test\"]:\n",
    "        _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
    "        print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
    "        np.savez_compressed(\n",
    "            os.path.join(output_dir, f\"{cat}.npz\"),\n",
    "            x=_x,\n",
    "            y=_y,\n",
    "            x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
    "            y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec5bb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (3312, 753, 8)\n",
      "(3312, 753, 8)\n",
      "data shape:  (3312, 753, 8)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.31 MiB for an array with shape (72, 753, 8) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOperation cancelled.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         \u001b[43mgenerate_train_val_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_start\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     19\u001b[39m     os.makedirs(output_dir)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mgenerate_train_val_test\u001b[39m\u001b[34m(filename, output_dir, seq_length_x, seq_length_y, y_start)\u001b[39m\n\u001b[32m     54\u001b[39m y_offsets = np.sort(np.arange(y_start, (seq_length_y + \u001b[32m1\u001b[39m), \u001b[32m1\u001b[39m)) \u001b[38;5;66;03m#[1,2,...,11,12]\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# x: (num_samples, input_length, num_nodes, input_dim)\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# y: (num_samples, output_length, num_nodes, output_dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m x, y = \u001b[43mgenerate_graph_seq2seq_io_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mx shape: \u001b[39m\u001b[33m\"\u001b[39m, x.shape, \u001b[33m\"\u001b[39m\u001b[33m, y shape: \u001b[39m\u001b[33m\"\u001b[39m, y.shape)  \u001b[38;5;66;03m#(l,t,n,f)\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Write the data into npz file.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mgenerate_graph_seq2seq_io_data\u001b[39m\u001b[34m(df, x_offsets, y_offsets, scaler)\u001b[39m\n\u001b[32m     27\u001b[39m max_t = \u001b[38;5;28mabs\u001b[39m(num_samples - \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mmax\u001b[39m(y_offsets)))  \u001b[38;5;66;03m# Exclusive\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_t, max_t):  \u001b[38;5;66;03m# t is the index of the last observation.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     x.append(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_offsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     30\u001b[39m     y.append(data[t + y_offsets, ...])\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m#print(\"x shape: \", x.shape, \", y shape: \", y.shape)\u001b[39;00m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 3.31 MiB for an array with shape (72, 753, 8) and data type float64"
     ]
    }
   ],
   "source": [
    "# pems = np.load('D:/myfiles/project/bike_prediction/feature_data/pems04.npz')\n",
    "# pems['data']\n",
    "\n",
    "# 设置参数\n",
    "output_dir = \"D:/myfiles/project/bike_prediction/feature_data\"\n",
    "filename = \"D:/myfiles/project/bike_prediction/feature_data/tcn_data_3d.npy\"\n",
    "seq_length_x = 72  # 输入序列长度\n",
    "seq_length_y = 24   # 输出序列长度\n",
    "y_start = 1         # 预测起始位置\n",
    "\n",
    "# 检查输出目录\n",
    "if os.path.exists(output_dir):\n",
    "    reply = str(input(f'{output_dir} exists. Do you want to overwrite it? (y/n)')).lower().strip()\n",
    "    if reply[0] != 'y': \n",
    "        print(\"Operation cancelled.\")\n",
    "    else:\n",
    "        generate_train_val_test(filename, output_dir, seq_length_x, seq_length_y, y_start)\n",
    "else:\n",
    "    os.makedirs(output_dir)\n",
    "    generate_train_val_test(filename, output_dir, seq_length_x, seq_length_y, y_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bikpred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
