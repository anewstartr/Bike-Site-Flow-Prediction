{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72e2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.data import Dataset, DataLoader,RandomSampler,SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "# import optuna\n",
    "from torch.nn import functional\n",
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15417a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753, 3312, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = np.load('D:/myfiles/project/bike_prediction/feature_data/tcn_data_3d.npy')\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d5e1b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  9.,  17.,   8.,  12.,   2.,  13.,   6.,   0.],\n",
       "        [  9.,  19.,  10.,   9.,   2.,  10.,   6.,   1.],\n",
       "        [ 10.,  21.,  11.,  13.,   2.,  13.,   7.,   2.],\n",
       "        [ 10.,  22.,  12.,  11.,   1.,  12.,   6.,   3.],\n",
       "        [ 10.,  26.,  16.,   8.,  -1.,   9.,   8.,   4.],\n",
       "        [  2.,  27.,  25.,   0.,   0.,   7.,   3.,   5.],\n",
       "        [ -2.,  27.,  29.,  -5.,   0.,   6.,   1.,   6.],\n",
       "        [ -2.,  28.,  30.,  -6.,   0.,   0.,   1.,   7.],\n",
       "        [ -4.,  26.,  30., -12.,  -1.,  -6.,  -1.,   8.],\n",
       "        [-10.,  18.,  28., -14.,  -1., -10.,  -4.,   9.],\n",
       "        [ -8.,  19.,  27., -14.,  -2., -10.,  -4.,  10.],\n",
       "        [ -9.,  17.,  26., -14.,  -2., -10.,  -4.,  11.],\n",
       "        [-10.,  15.,  25., -12.,  -2.,  -9.,  -3.,  12.],\n",
       "        [-10.,  13.,  23., -10.,  -2.,  -6.,  -3.,  13.],\n",
       "        [-11.,  11.,  22., -14.,  -2.,  -9.,  -4.,  14.],\n",
       "        [-11.,  10.,  21., -12.,  -1.,  -8.,  -3.,  15.],\n",
       "        [-11.,   6.,  17.,  -9.,   1.,  -5.,  -5.,  16.],\n",
       "        [ -3.,   5.,   8.,  -1.,   0.,  -2.,   0.,  17.],\n",
       "        [  2.,   6.,   4.,   2.,   3.,   0.,   1.,  18.],\n",
       "        [  3.,   6.,   3.,   4.,   4.,   4.,   6.,  19.],\n",
       "        [  8.,   9.,   1.,   4.,  11.,   9.,  10.,  20.],\n",
       "        [ 14.,  17.,   3.,  10.,  13.,   9.,  11.,  21.],\n",
       "        [ 13.,  17.,   4.,   9.,  13.,   8.,  11.,  22.],\n",
       "        [ 12.,  17.,   5.,   9.,  13.,   9.,  11.,  23.],\n",
       "        [ 12.,  18.,   6.,   9.,  12.,  10.,  11.,   0.],\n",
       "        [ 12.,  19.,   7.,   9.,   9.,  10.,  10.,   1.],\n",
       "        [ 13.,  21.,   8.,  10.,  13.,  11.,   9.,   2.],\n",
       "        [ 13.,  22.,   9.,  10.,  11.,   8.,   5.,   3.],\n",
       "        [ 12.,  24.,  12.,  10.,   8.,   6.,   5.,   4.],\n",
       "        [  8.,  27.,  19.,   2.,   0.,   4.,   2.,   5.],\n",
       "        [  2.,  25.,  23.,  -2.,  -5.,   2.,  -1.,   6.],\n",
       "        [ -1.,  22.,  23.,  -2.,  -6.,   0.,  -6.,   7.],\n",
       "        [ -4.,  19.,  23.,  -4., -12.,  -6.,  -9.,   8.],\n",
       "        [-10.,  11.,  21., -10., -14.,  -5., -11.,   9.],\n",
       "        [-10.,  10.,  20.,  -8., -14.,  -5., -11.,  10.],\n",
       "        [ -8.,  10.,  18.,  -9., -14.,  -6., -11.,  11.],\n",
       "        [ -8.,   9.,  17., -10., -12.,  -7., -11.,  12.],\n",
       "        [ -8.,   8.,  16., -10., -10.,  -7.,  -9.,  13.],\n",
       "        [ -9.,   6.,  15., -11., -14.,  -8.,  -8.,  14.],\n",
       "        [ -9.,   5.,  14., -11., -12.,  -5.,  -4.,  15.],\n",
       "        [ -8.,   3.,  11., -11.,  -9.,  -3.,  -4.,  16.],\n",
       "        [ -4.,   0.,   4.,  -3.,  -1.,  -2.,  -1.,  17.],\n",
       "        [  1.,   1.,   0.,   2.,   2.,   0.,   2.,  18.],\n",
       "        [  4.,   4.,   0.,   3.,   4.,   0.,   4.,  19.],\n",
       "        [  8.,   8.,   0.,   8.,   4.,   3.,   5.,  20.],\n",
       "        [ 11.,  13.,   2.,  14.,  10.,   6.,   9.,  21.],\n",
       "        [ 11.,  13.,   2.,  13.,   9.,   8.,  10.,  22.],\n",
       "        [  9.,  14.,   5.,  12.,   9.,   8.,  11.,  23.],\n",
       "        [  9.,  15.,   6.,  12.,   9.,  10.,  10.,   0.],\n",
       "        [  8.,  15.,   7.,  12.,   9.,   9.,   8.,   1.],\n",
       "        [  9.,  18.,   9.,  13.,  10.,   7.,   8.,   2.],\n",
       "        [ 11.,  22.,  11.,  13.,  10.,   6.,   4.,   3.],\n",
       "        [  7.,  25.,  18.,  12.,  10.,   6.,   4.,   4.],\n",
       "        [  2.,  25.,  23.,   8.,   2.,   5.,   2.,   5.],\n",
       "        [  0.,  24.,  24.,   2.,  -2.,   0.,  -1.,   6.],\n",
       "        [ -4.,  21.,  25.,  -1.,  -2.,   0.,  -2.,   7.],\n",
       "        [ -8.,  17.,  25.,  -4.,  -4.,  -2.,  -4.,   8.],\n",
       "        [-11.,  12.,  23., -10., -10.,  -5.,  -7.,   9.],\n",
       "        [-11.,  12.,  23., -10.,  -8.,  -6.,  -8.,  10.],\n",
       "        [ -8.,  12.,  20.,  -8.,  -9.,  -6.,  -9.,  11.],\n",
       "        [ -8.,  11.,  19.,  -8., -10.,  -8.,  -8.,  12.],\n",
       "        [ -7.,  11.,  18.,  -8., -10.,  -8.,  -7.,  13.],\n",
       "        [ -8.,   8.,  16.,  -9., -11.,  -6.,  -7.,  14.],\n",
       "        [-10.,   4.,  14.,  -9., -11.,  -5.,  -3.,  15.],\n",
       "        [ -6.,   1.,   7.,  -8., -11.,  -5.,  -3.,  16.],\n",
       "        [ -1.,   1.,   2.,  -4.,  -3.,  -4.,  -1.,  17.],\n",
       "        [  1.,   2.,   1.,   1.,   2.,  -1.,   1.,  18.],\n",
       "        [  1.,   2.,   1.,   4.,   3.,   0.,   0.,  19.],\n",
       "        [  5.,   6.,   1.,   8.,   8.,   0.,   0.,  20.],\n",
       "        [  6.,   9.,   3.,  11.,  14.,   0.,   0.,  21.],\n",
       "        [  5.,  11.,   6.,  11.,  13.,   0.,   1.,  22.],\n",
       "        [  3.,  14.,  11.,   9.,  12.,  -2.,   1.,  23.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[1:2,0:72,0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c63c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【站点数量，序列长度，特征数量】\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, his_datas, his_label, output_size, feature_size, seq_num, time_of_day):\n",
    "        self.his_datas = his_datas  #【N，1080，X】\n",
    "        # self.sta_datas = sta_datas  #【N，26，Y】\n",
    "        self.his_label = his_label  #【N，1080，1】\n",
    "        self.output_size = output_size  # 输出长度24\n",
    "        self.feature_size = feature_size  # 卷积塔时序特征数量\n",
    "        # self.static_feature_size = static_feature_size  # 特征塔天粒度/静态特征数量\n",
    "        self.seq_num = seq_num  # 窗口大小\n",
    "        self.time_of_day = time_of_day  # 每天24小时\n",
    "         \n",
    "        self.site_num = his_datas.shape[0]  # 站点数量\n",
    "        self.time_num = his_datas.shape[1] // time_of_day  - (seq_num + 3) # 单个站点的样本数量：26-15=11个样本\n",
    "        self.sample_num = self.time_num * self.site_num  # 总样本数量：32*1080=3w\n",
    "        # print(his_datas.shape)\n",
    "        print('单个样本数量：', self.time_num)\n",
    "        print('站点数量：', self.site_num)\n",
    "        print('总样本数量：', self.sample_num)\n",
    "        print(\"a\", his_datas.shape, his_label.shape)\n",
    "        \n",
    "    def __getitem__(self, index): # 0-3w\n",
    "        # 是第几个样本？\n",
    "        cls_indx, time_indx = divmod(index, self.time_num)\n",
    "        start_index = time_indx * self.time_of_day\n",
    "        end_index = (time_indx + self.seq_num) * self.time_of_day\n",
    "        # [站点,小时粒度序列,小时粒度特征]\n",
    "        tmp_data = self.his_datas[cls_indx, start_index:end_index, 0:self.feature_size].astype(float)  # [0, 14*24, time_feature_size]\n",
    "        sample_time_data = torch.tensor(tmp_data, dtype=torch.float32)\n",
    "        # [站点,天粒度序列,天粒度特征]\n",
    "        # static_data = self.sta_datas[cls_indx, static_index:static_index+1, 0:self.static_feature_size].astype(float)  # [0, 1, time_feature_size]\n",
    "        # sample_static_data = torch.tensor(static_data, dtype=torch.float32)\n",
    "        # [站点,序列,1]\n",
    "        label_start = end_index\n",
    "        label_end = label_start + self.output_size\n",
    "        target_label = self.his_label[cls_indx, label_start:label_end, 0:1].astype(float)\n",
    "        sample_labels = torch.tensor(target_label, dtype=torch.float32)\n",
    "        \n",
    "        return sample_time_data, sample_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sample_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505b001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(all_data):  # 56天\n",
    "    tmp_data_info = np.array(all_data)\n",
    "    # sta_data_info = np.array(sta_data)\n",
    "    # 当前总时长为138天，4.15-8.30\n",
    "    train_start_idx = 0\n",
    "    train_end_idx = 76 * 24 \n",
    "    val_start_idx = 76 * 24\n",
    "    val_end_idx = 107 * 24 \n",
    "    test_start_idx = 107 * 24\n",
    "    test_end_idx = 138 * 24 \n",
    "    # train_start_sta_idx = 0\n",
    "    # train_end_sta_idx = 18\n",
    "    # val_end_sta_idx = 22\n",
    "    # test_end_sta_idx = 26\n",
    "    \n",
    "#     train_start_idx = 0\n",
    "#     train_end_idx = 38 * 24  # 9\n",
    "#     val_start_idx = (38 - 30) * 24  # 13使用14，14使用15\n",
    "#     val_end_idx = 42 * 24  # 4\n",
    "#     test_start_idx = (42 - 30) * 24\n",
    "#     test_end_idx = 49 * 24  # 7\n",
    "    \n",
    "    train_data = tmp_data_info[:, train_start_idx:train_end_idx, :]  # 所有特征\n",
    "    # train_data_sta = sta_data_info[:, train_start_sta_idx:train_end_sta_idx, :]\n",
    "    train_label = tmp_data_info[:, train_start_idx:train_end_idx, 0:1]\n",
    "    val_data = tmp_data_info[:, val_start_idx:val_end_idx, :]\n",
    "    # val_data_sta = sta_data_info[:, train_end_sta_idx:val_end_sta_idx, :]    \n",
    "    val_label = tmp_data_info[:, val_start_idx:val_end_idx, 0:1]\n",
    "    test_data = tmp_data_info[:, test_start_idx:test_end_idx, :]\n",
    "    # test_data_sta = sta_data_info[:, val_end_sta_idx:test_end_sta_idx, :]  \n",
    "    test_label = tmp_data_info[:, test_start_idx:test_end_idx, 0:1]\n",
    "    return train_data, train_label, val_data, val_label, test_data, test_label\n",
    "    # return train_data, train_data_sta, train_label, val_data, val_data_sta, val_label, test_data, test_data_sta, test_label\n",
    "\n",
    "\n",
    "\n",
    "def load_data(all_data, batch_size):\n",
    "    train_data, train_label, val_data, val_label, test_data, test_label = train_test_split(all_data)\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = MyDataset(his_datas=train_data, his_label=train_label, \n",
    "                             output_size=24, feature_size=8, seq_num=1, time_of_day=24)\n",
    "    \n",
    "    # 创建训练样本索引\n",
    "    n_train = len(train_dataset)\n",
    "    indices = list(range(n_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split_point = int(n_train * 0.4)\n",
    "    train_indices = indices[:split_point]\n",
    "    \n",
    "    # 创建采样器\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        sampler=train_sampler,\n",
    "        pin_memory=True  # 加速GPU数据传输\n",
    "    )\n",
    "    \n",
    "    # 验证和测试集保持完整\n",
    "    val_dataset = MyDataset(his_datas=val_data, his_label=val_label, \n",
    "                           output_size=24, feature_size=8, seq_num=1, time_of_day=24)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_dataset = MyDataset(his_datas=test_data, his_label=test_label, \n",
    "                             output_size=24, feature_size=8, seq_num=1, time_of_day=24)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "# def load_multitask_data(all_data, sta_data, batch_size, output_sizes={0: 24, 1: 24, 2: 24}):\n",
    "#     # 自定义collate函数处理多目标数据\n",
    "#     def multitask_collate(batch):\n",
    "#         time_data = torch.stack([item[0] for item in batch])\n",
    "#         static_data = torch.stack([item[1] for item in batch])\n",
    "#         labels = {}\n",
    "#         for target_idx in output_sizes.keys():\n",
    "#             labels[target_idx] = torch.stack([item[2][target_idx] for item in batch])\n",
    "#         return time_data, static_data, labels\n",
    "#     train_data, train_data_sta, train_label, val_data, val_data_sta, val_label, test_data, test_data_sta, test_label = train_test_split(all_data, sta_data)\n",
    "#     train_dataset = MyDataset(his_datas=train_data, sta_datas = train_data_sta, his_label=train_label, \n",
    "#                               output_sizes=output_sizes, time_feature_size=22, static_feature_size=7, seq_num=14, time_of_day=24)\n",
    "#     n_samples = len(train_dataset)\n",
    "#     indices = list(range(n_samples))\n",
    "#     # 随机选择50%的样本\n",
    "#     split = int(0.4 * n_samples)\n",
    "#     np.random.shuffle(indices)\n",
    "#     train_indices = indices[:split]  # 前50%作为本次训练样本\n",
    "#     # 创建采样器\n",
    "#     train_sampler = SubsetRandomSampler(train_indices)\n",
    "#     train_dataloader = DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         sampler=train_sampler,\n",
    "#         collate_fn=multitask_collate\n",
    "#     )\n",
    "#     # train_rand_sampler = RandomSampler(train_dataset, replacement=False, num_samples=int(len(train_dataset)*0.3))\n",
    "#     # train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, sampler=train_rand_sampler) \n",
    "#     # train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True) \n",
    "      \n",
    "#     val_dataset = MyDataset(his_datas=val_data, sta_datas = val_data_sta, his_label=val_label,\n",
    "#                             output_sizes=output_sizes, time_feature_size=22, static_feature_size=7, seq_num=14, time_of_day=24)\n",
    "#     val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False, collate_fn=multitask_collate)\n",
    "\n",
    "#     test_dataset = MyDataset(his_datas=test_data, sta_datas = test_data_sta, his_label=test_label,\n",
    "#                              output_sizes=output_sizes, time_feature_size=22, static_feature_size=7, seq_num=14, time_of_day=24)\n",
    "#     test_dataloader = DataLoader(test_dataset, batch_size = 4, shuffle=False, collate_fn=multitask_collate)\n",
    "\n",
    "#     return train_dataloader , val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c44f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "def huber_loss(y_pred, y_true):\n",
    "    loss = torch.nn.SmoothL1Loss(reduction='mean',beta=5.0)(y_pred, y_true)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    loss = torch.nn.MSELoss(reduction='mean')(y_pred, y_true)\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def printbar():\n",
    "    t = datetime.datetime.now()\n",
    "    print('==========='*8 + str(t))\n",
    "\n",
    "\n",
    "import os\n",
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] =str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.daterministic = True\n",
    "    \n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        # [N, 38, 14*24]->[N, 300, 14*24]\n",
    "        # n_inputs=38, n_outputs=1\n",
    "        # weight_norm(\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.norm1 = nn.GroupNorm(1, n_outputs)  #加一层试试\n",
    "\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # [N, 300, 14*24]->[N, 300, 14*24]\n",
    "        # weight_norm(\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.norm2 = nn.GroupNorm(1, n_outputs)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.norm1, self.relu1, self.dropout1,\n",
    "                                  self.conv2, self.chomp2, self.norm2, self.relu2, self.dropout2)\n",
    "\n",
    "        # [N, 38, 14*24]->[N, 300, 14*24]\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None # 1x1 conv\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: n*emb_size*seq_len\n",
    "        out: n*layer_outchannel* seq_len\"\"\"\n",
    "        # [N, 38, 14*24]->[N, 1, 14*24]\n",
    "        out = self.net(x)\n",
    "        # [N, 38, 14*24]->[N, 1, 14*24]\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)  # [N, 1, 14*24]\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    # num_inputs=38, out_channels=[300, 200, 100, 50, 1]\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        # dilation_sizes = [1,4,16,24]\n",
    "        for i in range(num_levels):\n",
    "            \"\"\"dilated conv\"\"\"\n",
    "            dilation_size = 2 ** i   #认为此处不合理，待改                                                    \n",
    "            # dilation_size = dilation_sizes[i]\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            # [N, 300, 14*24] + [N, 200, 14*24] + [N, 100, 14*24] + [N, 50, 14*24] + [N, 1, 14*24]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    " \n",
    "\n",
    "class MultiTaskTCN(nn.Module):\n",
    "    def __init__(self, input_size, input_len, output_sizes, num_channels,\n",
    "                 kernel_size=2, dropout=0.3, emb_dropout=0.1, tied_weights=False):\n",
    "        super(MultiTaskTCN, self).__init__()\n",
    "        # [N, 38, 14*24]->[N, 3, 14*24]\n",
    "        # input_size=39, num_channels=[300, 200, 100, 50, 1],output_size=1*24\n",
    "        self.output_sizes = output_sizes\n",
    "        self.time_tasks = list(output_sizes.keys())\n",
    "        # 为每个时间粒度创建独立的预测头\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        # self.decoder = nn.Linear(input_len, output_size)\n",
    "        self.drop = nn.Dropout(emb_dropout)\n",
    "        self.emb_dropout = emb_dropout\n",
    "        # 为每个时间粒度创建独立的预测头\n",
    "        self.prediction_heads = nn.ModuleDict()\n",
    "        for scale in self.time_tasks:\n",
    "            self.prediction_heads[str(scale)] = nn.Linear(\n",
    "                input_len,\n",
    "                output_sizes[scale]\n",
    "            )\n",
    "            self.prediction_heads[str(scale)].bias.data.fill_(0)\n",
    "            self.prediction_heads[str(scale)].weight.data.normal_(0, 0.01)\n",
    "        # self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "#         self.decoder.bias.data.fill_(0)\n",
    "#         self.decoder.weight.data.normal_(0, 0.01)\n",
    "        for head in self.prediction_heads:\n",
    "            head.bias.data.fill_(0)\n",
    "            head.weight.data.normal_(0, 0.01)\n",
    "            \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Input ought to have dimension (N, C_in, L_in), \n",
    "        where L_in is the seq_len; \n",
    "        here the input is (N, L, C)\"\"\"\n",
    "        # [N, 14*24, 38]->[N, 38, 14*24]\n",
    "        y = input.transpose(1, 2)\n",
    "        # [N, 38, 14*24]->[N, 3, 14*24]\n",
    "        y = self.tcn(y)\n",
    "        # [N, 3, 14*24]->拆分多目标->[N, 1, 1*24]->[N, 1*24, 1]\n",
    "        # 为每个时间粒度生成预测\n",
    "        predictions = {}\n",
    "        for scale in self.time_tasks:\n",
    "            scale_features = y[:,scale:scale+1,:]\n",
    "            predictions[scale] = self.prediction_heads[str(scale)](scale_features).transpose(1, 2).contiguous()\n",
    "        \n",
    "        return predictions\n",
    "#         y = self.decoder(y).transpose(1, 2)\n",
    "#         return y.contiguous()\n",
    "\n",
    "    \n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, input_len, output_size, num_channels,\n",
    "                 kernel_size=2, dropout=0.3, emb_dropout=0.1, tied_weights=False):\n",
    "        super(TCN, self).__init__()\n",
    "        # [N, 38, 14*24]->[N, 1, 14*24]\n",
    "        # input_size=39, num_channels=[300, 200, 100, 50, 1],output_size=1*24\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        self.decoder = nn.Linear(input_len, output_size)\n",
    "        self.drop = nn.Dropout(emb_dropout)\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.init_weights()\n",
    "        self.linear = nn.Linear(input_len, output_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Input ought to have dimension (N, C_in, L_in), \n",
    "        where L_in is the seq_len; \n",
    "        here the input is (N, L, C)\"\"\"\n",
    "        # [N, 14*24, 38]->[N, 38, 14*24]\n",
    "        y = input.transpose(1, 2)\n",
    "        # [N, 38, 14*24]->[N, 1, 14*24]\n",
    "        y = self.tcn(y)\n",
    "        # [N, 1, 14*24]->[N, 1, 1*24]->[N, 1*24, 1]\n",
    "        y = self.decoder(y).transpose(1, 2)\n",
    "        # 使用linear的效果  [N, 14*24, 1]->[N, 1, 14*24]->[N, 1, 1*24]->[N, 1*24, 1]\n",
    "        return y.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913ebf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeakHuberLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PeakHuberLoss, self).__init__()\n",
    "    def forward(self, y_pred, y_true, delta = 5):\n",
    "        # y_pred: [B, 24, 1]; y_true: [B, 24, 1]\n",
    "        # 标准化形状，确保可广播\n",
    "        if y_pred.ndim == 2:\n",
    "            y_pred = y_pred.unsqueeze(-1)\n",
    "        if y_true.ndim == 2:\n",
    "            y_true = y_true.unsqueeze(-1)\n",
    "        error = y_true - y_pred\n",
    "        peak_mask = (y_true >= 5)\n",
    "        # 让空集合时保持为张量而不是 Python float\n",
    "        if torch.any(peak_mask):\n",
    "            peak_err = error[peak_mask]\n",
    "            peak_loss = torch.where(torch.abs(peak_err) <= delta,\n",
    "                                    0.5 * peak_err**2,\n",
    "                                    delta * (torch.abs(peak_err) - 0.5 * delta)).mean()\n",
    "        else:\n",
    "            peak_loss = torch.zeros((), device=error.device)\n",
    "        non_peak_mask = ~peak_mask\n",
    "        if torch.any(non_peak_mask):\n",
    "            non_peak_err = error[non_peak_mask]\n",
    "            non_peak_loss = torch.abs(non_peak_err).mean()\n",
    "        else:\n",
    "            non_peak_loss = torch.zeros((), device=error.device)\n",
    "        total_loss = peak_loss * 2 + non_peak_loss\n",
    "        return total_loss  # 返回单个标量张量\n",
    "    \n",
    "class MultiTaskPHLoss(nn.Module):\n",
    "    def __init__(self, loss_weights=None):\n",
    "        super(MultiTaskPHLoss, self).__init__()\n",
    "        self.peakhuberloss = PeakHuberLoss()\n",
    "        self.loss_weights = loss_weights\n",
    "    \n",
    "    def forward(self, predictions, targets, delta = 5):\n",
    "        total_loss = 0\n",
    "        losses = {}\n",
    "        for scale, pred in predictions.items():\n",
    "            target = targets[scale]\n",
    "            scale_loss = self.peakhuberloss(pred, target, delta = delta)\n",
    "            weight = self.loss_weights[scale] if self.loss_weights else 1.0\n",
    "            weighted_loss = weight * scale_loss\n",
    "            losses[scale] = scale_loss.item()\n",
    "            total_loss += weighted_loss\n",
    "        return total_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ed22de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "(753, 3312, 8)\n",
      "单个样本数量： 72\n",
      "站点数量： 753\n",
      "总样本数量： 54216\n",
      "a (753, 1824, 8) (753, 1824, 1)\n",
      "单个样本数量： 27\n",
      "站点数量： 753\n",
      "总样本数量： 20331\n",
      "a (753, 744, 8) (753, 744, 1)\n",
      "单个样本数量： 27\n",
      "站点数量： 753\n",
      "总样本数量： 20331\n",
      "a (753, 744, 8) (753, 744, 1)\n"
     ]
    }
   ],
   "source": [
    "setup_seed(12345)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "output_sizes = 24\n",
    "\n",
    "# device = 'cpu'\n",
    "print('device:', device)\n",
    "print(all_data.shape)\n",
    "# print(static_all_data.shape)\n",
    "\n",
    "# 加载数据\n",
    "train_dataloader, val_dataloader, test_dataloader = load_data(all_data[:, :, :], 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4df45f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'pred_model/net_divvy_TCN_4.pth'\n",
    "\n",
    "num_channels = [64, 128, 32, 1]  # TCN隐藏层维度\n",
    "\n",
    "# 训练模型\n",
    "lr = 0.001\n",
    "# loss_weights = {10: 1.0, 16: 1.0, 21: 1.0}  # 更长期的预测给予更高权重    \n",
    "es_cnt = 0\n",
    "max_es_epoch = 10\n",
    "min_val_loss = float('inf')\n",
    "epoches = 50\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# tcn_model = HybridTCN(input_size=22, input_len=14*24, output_sizes=output_sizes, num_channels=num_channels, \n",
    "#                          static_dict=static_dict, static_hidden_dims=[32, 16],\n",
    "#                          kernel_size=3, dropout=0.25, static_dropout=0.2, tied_weights=False).to(device)\n",
    "tcn_model = TCN(input_size=8, input_len=1*24, output_size=24, num_channels=num_channels, \n",
    "                         kernel_size=3, dropout=0.25, emb_dropout=0.2, tied_weights=False).to(device)\n",
    "criterion = PeakHuberLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    tcn_model.parameters(),\n",
    "    lr=lr,                    # 学习率\n",
    "    betas=(0.9, 0.999),         # 动量参数\n",
    "    eps=1e-8,                   # 数值稳定性\n",
    "    weight_decay=1e-2,          # 权重衰减\n",
    "    amsgrad=False               # 是否使用 AMSGrad 变体\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "491b1875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0, Train Loss:81.721069, Val Loss:117.444611\n",
      "Epoch=2, Train Loss:33.274699, Val Loss:71.997129\n",
      "Epoch=4, Train Loss:29.381841, Val Loss:61.213430\n",
      "Epoch=6, Train Loss:24.640171, Val Loss:51.102310\n",
      "Epoch=8, Train Loss:23.153331, Val Loss:46.127585\n",
      "Epoch=10, Train Loss:22.742082, Val Loss:43.024810\n",
      "Epoch=12, Train Loss:21.247876, Val Loss:46.824858\n",
      "Epoch=14, Train Loss:20.740135, Val Loss:43.581237\n",
      "Epoch=16, Train Loss:23.489024, Val Loss:54.298407\n",
      "Epoch=18, Train Loss:20.800681, Val Loss:41.735164\n",
      "Epoch=20, Train Loss:19.965673, Val Loss:42.334273\n",
      "Epoch=22, Train Loss:20.088468, Val Loss:48.071153\n",
      "Epoch=24, Train Loss:20.469164, Val Loss:40.219119\n",
      "Epoch=26, Train Loss:19.683221, Val Loss:41.774037\n",
      "Epoch=28, Train Loss:19.448854, Val Loss:40.830687\n",
      "Epoch=30, Train Loss:19.065644, Val Loss:45.415265\n",
      "Epoch=32, Train Loss:19.423810, Val Loss:41.030278\n",
      "Epoch=34, Train Loss:19.235192, Val Loss:42.497341\n",
      "Epoch=36, Train Loss:19.440825, Val Loss:42.200446\n",
      "Epoch=38, Train Loss:18.673383, Val Loss:43.810007\n",
      "Epoch=40, Train Loss:17.998761, Val Loss:43.855501\n",
      "Epoch=42, Train Loss:17.933802, Val Loss:42.296519\n",
      "Epoch=44, Train Loss:20.070186, Val Loss:41.675360\n",
      "触发早停机制！\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoches + 1):\n",
    "    tcn_model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for time_data, label in train_dataloader:\n",
    "        time_data = time_data.to(device)\n",
    "        label_on_device = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        forecast = tcn_model(time_data)  # [batch, 24, 1]\n",
    "        loss = criterion(forecast, label_on_device)  # 已修改：不再解包\n",
    "        if torch.isnan(loss):\n",
    "            print(\"训练损失为NaN! 中止该 batch\")\n",
    "            break\n",
    "        train_losses.append(loss.detach().item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss_avg = sum(train_losses) / len(train_losses) if train_losses else 0\n",
    "\n",
    "    if e % 2 == 0:  # 每2个epoch验证一次\n",
    "        tcn_model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for val_time_data, val_label in val_dataloader:\n",
    "                val_time_data = val_time_data.to(device)\n",
    "                val_label_on_device = val_label.to(device)\n",
    "                val_forecast = tcn_model(val_time_data)\n",
    "                val_loss = criterion(val_forecast, val_label_on_device)  # 已修改：不再解包\n",
    "                val_losses.append(val_loss.item())\n",
    "        val_loss_avg = sum(val_losses) / len(val_losses) if val_losses else 0\n",
    "        print(f'Epoch={e}, Train Loss:{train_loss_avg:.6f}, Val Loss:{val_loss_avg:.6f}')\n",
    "        if val_loss_avg < min_val_loss:\n",
    "            min_val_loss = val_loss_avg\n",
    "            es_cnt = 0\n",
    "            torch.save(tcn_model.state_dict(), model_save_path)\n",
    "        else:\n",
    "            es_cnt += 1\n",
    "            if es_cnt >= max_es_epoch:\n",
    "                print('触发早停机制！')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec783b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 40.089904\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_channels = [64, 128, 32, 1]  # TCN隐藏层维度\n",
    "criterion = PeakHuberLoss()\n",
    "# 加载模型，预测\n",
    "tcn_save_model = TCN(input_size=8, input_len=1*24, output_size=24, num_channels=num_channels, \n",
    "                         kernel_size=3, dropout=0.25, emb_dropout=0.2, tied_weights=False).to(device)\n",
    "# batch*length*size 输入， batch = 32个点位，len = 7天*24小时，size = 8个特征 \n",
    "tcn_save_model.load_state_dict(torch.load('pred_model/net_divvy_TCN_4.pth'))\n",
    "\n",
    "# 初始化存储\n",
    "test_losses = []\n",
    "true_values = []\n",
    "pred_values = []\n",
    "\n",
    "# 测试循环\n",
    "tcn_save_model.eval()\n",
    "with torch.no_grad():\n",
    "    for test_time_data, test_labels in test_dataloader:\n",
    "        test_time_data = test_time_data.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "        \n",
    "        # 标准化输入数据（如果需要）\n",
    "        # test_data, mean, std = transform_series2(test_data)\n",
    "        \n",
    "        # 前向传播\n",
    "        test_forecasts = tcn_save_model(test_time_data)\n",
    "        \n",
    "        # 反标准化预测结果（如果需要）\n",
    "        # for target_idx, forecast in test_forecasts.items():\n",
    "        #     test_forecasts[target_idx] = transform_series2_decode(forecast, mean, std)\n",
    "        \n",
    "        # 计算损失\n",
    "        test_loss = criterion(test_forecasts, test_labels)\n",
    "        test_losses.append(test_loss.item())\n",
    "        # 存储真实值和预测值\n",
    "        true_values.append(test_labels.cpu().numpy())\n",
    "        pred_values.append(test_forecasts.cpu().numpy())\n",
    "\n",
    "# 计算平均损失\n",
    "test_loss_avg = sum(test_losses) / len(test_losses) if test_losses else 0\n",
    "print(f'Test Loss: {test_loss_avg:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77dc3f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Metrics (y_true > 5 only) ===\n",
      "Morning 7-9  -> MSE: 200.3086, MAPE: 0.2619, WMAPE: 0.3377\n",
      "Evening 18-20-> MSE: 105.4965, MAPE: 0.2685, WMAPE: 0.3152\n",
      "All-day 0-23 -> MSE: 99.2215, MAPE: 0.2545, WMAPE: 0.2883\n"
     ]
    }
   ],
   "source": [
    "# 评估测试集的 MSE / MAPE / WMAPE（仅统计真值>5的样本；早峰7-9、晚峰18-20、全天0-23）\n",
    "import numpy as np\n",
    "\n",
    "if not pred_values or not true_values:\n",
    "    print(\"pred_values / true_values 为空，请先运行测试循环。\")\n",
    "else:\n",
    "    y_pred = np.concatenate(pred_values, axis=0)  # [N, 24, 1]\n",
    "    y_true = np.concatenate(true_values, axis=0)  # [N, 24, 1]\n",
    "    \n",
    "    # 去掉最后一个特征维度\n",
    "    y_pred = y_pred.squeeze(-1)  # [N, 24]\n",
    "    y_true = y_true.squeeze(-1)  # [N, 24]\n",
    "\n",
    "    def compute_metrics_gt5(y_true_slice, y_pred_slice, gt_min=5):\n",
    "        # 仅在真值>gt_min的样本上计算\n",
    "        mask = y_true_slice > gt_min\n",
    "        if not np.any(mask):\n",
    "            return float('nan'), float('nan'), float('nan')\n",
    "        yt = y_true_slice[mask]\n",
    "        yp = y_pred_slice[mask]\n",
    "        mse = float(np.mean((yp - yt) ** 2))\n",
    "        # MAPE（分母为真值>5的子集）\n",
    "        mape = float(np.mean(np.abs((yp - yt) / yt)))\n",
    "        # WMAPE（分母为真值>5的子集和）\n",
    "        denom = float(np.sum(np.abs(yt)))\n",
    "        wmape = float(np.sum(np.abs(yp - yt)) / denom) if denom > 0 else float('nan')\n",
    "        return mse, mape, wmape\n",
    "\n",
    "    # 定义时段索引（含端点）\n",
    "    morning_idx = np.array([7, 8, 9])\n",
    "    evening_idx = np.array([18, 19, 20])\n",
    "    all_idx = np.arange(24)\n",
    "\n",
    "    # 早峰（仅真值>5）\n",
    "    mse_morning, mape_morning, wmape_morning = compute_metrics_gt5(\n",
    "        y_true[:, morning_idx].reshape(-1), y_pred[:, morning_idx].reshape(-1)\n",
    "    )\n",
    "    # 晚峰（仅真值>5）\n",
    "    mse_evening, mape_evening, wmape_evening = compute_metrics_gt5(\n",
    "        y_true[:, evening_idx].reshape(-1), y_pred[:, evening_idx].reshape(-1)\n",
    "    )\n",
    "    # 全天（仅真值>5）\n",
    "    mse_all, mape_all, wmape_all = compute_metrics_gt5(\n",
    "        y_true[:, all_idx].reshape(-1), y_pred[:, all_idx].reshape(-1)\n",
    "    )\n",
    "\n",
    "    print(\"=== Test Metrics (y_true > 5 only) ===\")\n",
    "    print(f\"Morning 7-9  -> MSE: {mse_morning:.4f}, MAPE: {mape_morning:.4f}, WMAPE: {wmape_morning:.4f}\")\n",
    "    print(f\"Evening 18-20-> MSE: {mse_evening:.4f}, MAPE: {mape_evening:.4f}, WMAPE: {wmape_evening:.4f}\")\n",
    "    print(f\"All-day 0-23 -> MSE: {mse_all:.4f}, MAPE: {mape_all:.4f}, WMAPE: {wmape_all:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdeca8aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 计算每个目标的MAPE（按天计算）\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 合并所有批次的预测和真实值\u001b[39;00m\n\u001b[32m      3\u001b[39m lth = \u001b[38;5;28mlen\u001b[39m(true_values)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m all_true = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m all_true = np.reshape(all_true, (lth, \u001b[32m4\u001b[39m, \u001b[32m24\u001b[39m))\n\u001b[32m      6\u001b[39m all_pred = np.array(pred_values)\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# 计算每个目标的MAPE（按天计算）\n",
    "# 合并所有批次的预测和真实值\n",
    "lth = len(true_values)\n",
    "all_true = np.array(true_values)\n",
    "all_true = np.reshape(all_true, (lth, 4, 24))\n",
    "all_pred = np.array(pred_values)\n",
    "all_pred = np.reshape(all_pred, (lth, 4, 24))\n",
    "print(all_true.shape)\n",
    "\n",
    "print(f\"\\n=== 目标 {target_idx} MAPE分析 ===\")\n",
    "\n",
    "# 计算每天的MAPE（只考虑真值>=5的点）\n",
    "daily_mapes = []\n",
    "morning_mapes = []\n",
    "afternoon_mapes = []\n",
    "evening_mapes = []\n",
    "daily_wapes = []  # 新增：存储每天的WMAPE\n",
    "morning_wapes = []  # 新增：存储每天早峰的WMAPE\n",
    "afternoon_wapes = []  # 新增：存储每天午峰的WMAPE\n",
    "evening_wapes = []  # 新增：存储每天晚峰的WMAPE\n",
    "\n",
    "for i in range(all_pred.shape[1]):\n",
    "    sub_day_pred = all_pred[:, i,:].round()\n",
    "    sub_day_true = all_true[:, i,:]\n",
    "    \n",
    "    # 只考虑真值>=5的点\n",
    "    where_res = np.where(sub_day_true>=5)\n",
    "    sub_day_true1 = sub_day_true[where_res]\n",
    "    sub_day_pred1 = sub_day_pred[where_res]\n",
    "    \n",
    "    # 计算MAPE\n",
    "    mape = np.mean(np.abs((sub_day_pred1 - sub_day_true1) / sub_day_true1))\n",
    "    daily_mapes.append(mape)\n",
    "    \n",
    "    # 计算WMAPE (新增)\n",
    "    wape = np.sum(np.abs(sub_day_pred1 - sub_day_true1)) / np.sum(sub_day_true1)\n",
    "    daily_wapes.append(wape)\n",
    "    \n",
    "    print(f\"Day {i+1}: MAPE = {mape:.4f}, WAPE = {wape:.4f}\")\n",
    "    \n",
    "    # 早峰时段 (9时)\n",
    "    mor_day_pred = all_pred[:, i, 8:9].round()\n",
    "    mor_day_true = all_true[:, i, 8:9]\n",
    "    where_resmon = np.where(mor_day_true>=5)\n",
    "    mor_day_true1 = mor_day_true[where_resmon]\n",
    "    mor_day_pred1 = mor_day_pred[where_resmon]\n",
    "    \n",
    "    mor_mape = np.mean(np.abs((mor_day_pred1 - mor_day_true1) / mor_day_true1))\n",
    "    morning_mapes.append(mor_mape)\n",
    "    \n",
    "    # 早峰WMAPE (新增)\n",
    "    mor_wape = np.sum(np.abs(mor_day_pred1 - mor_day_true1)) / np.sum(mor_day_true1)\n",
    "    morning_wmapes.append(mor_wape)\n",
    "    \n",
    "    print(f\"Day {i+1} Morning MAPE = {mor_mape:.4f}, WAPE = {mor_wape:.4f}\")\n",
    "    \n",
    "    # 午峰时段 (14时)\n",
    "    aft_day_pred = all_pred[:, i, 13:14].round()\n",
    "    aft_day_true = all_true[:, i, 13:14]\n",
    "    where_resaft = np.where(aft_day_true>=5)\n",
    "    aft_day_true1 = aft_day_true[where_resaft]\n",
    "    aft_day_pred1 = aft_day_pred[where_resaft]\n",
    "    \n",
    "    aft_mape = np.mean(np.abs((aft_day_pred1 - aft_day_true1) / aft_day_true1))\n",
    "    afternoon_mapes.append(aft_mape)\n",
    "    \n",
    "    # 午峰WMAPE (新增)\n",
    "    aft_wape = np.sum(np.abs(aft_day_pred1 - aft_day_true1)) / np.sum(aft_day_true1)\n",
    "    afternoon_wapes.append(aft_wape)\n",
    "    \n",
    "    print(f\"Day {i+1} Afternoon MAPE = {aft_mape:.4f}, WAPE = {aft_wmape:.4f}\")\n",
    "    \n",
    "    # 晚峰时段 (20时)\n",
    "    eve_day_pred = all_pred[:, i, 19:20].round()\n",
    "    eve_day_true = all_true[:, i, 19:20]\n",
    "    where_reseve = np.where(eve_day_true>=5)\n",
    "    eve_day_true1 = eve_day_true[where_reseve]\n",
    "    eve_day_pred1 = eve_day_pred[where_reseve]\n",
    "    \n",
    "    eve_mape = np.mean(np.abs((eve_day_pred1 - eve_day_true1) / eve_day_true1))\n",
    "    evening_mapes.append(eve_mape)\n",
    "    \n",
    "    # 晚峰WMAPE (新增)\n",
    "    eve_wape = np.sum(np.abs(eve_day_pred1 - eve_day_true1)) / np.sum(eve_day_true1)\n",
    "    evening_wapes.append(eve_wape)\n",
    "    \n",
    "    print(f\"Day {i+1} Evening MAPE = {eve_mape:.4f}, WAPE = {eve_wape:.4f}\")\n",
    "\n",
    "# 计算平均MAPE和WMAPE\n",
    "if daily_mapes:\n",
    "    avg_mape = np.mean(daily_mapes)\n",
    "    avg_wape = np.mean(daily_wapes)  # 新增\n",
    "    print(f\"目标 {target_idx} 平均MAPE: {avg_mape:.4f}, 平均WAPE: {avg_wmape:.4f}\")\n",
    "if morning_mapes:\n",
    "    monavg_mape = np.mean(morning_mapes)\n",
    "    monavg_wape = np.mean(morning_wapes)  # 新增\n",
    "    print(f\"目标 {target_idx} 早峰平均MAPE: {monavg_mape:.4f}, 平均WAPE: {monavg_wape:.4f}\")\n",
    "if afternoon_mapes:\n",
    "    aftavg_mape = np.mean(afternoon_mapes)\n",
    "    aftavg_wape = np.mean(afternoon_wapes)  # 新增\n",
    "    print(f\"目标 {target_idx} 午峰平均MAPE: {aftavg_mape:.4f}, 平均WAPE: {aftavg_wape:.4f}\")\n",
    "if evening_mapes:\n",
    "    eveavg_mape = np.mean(evening_mapes)\n",
    "    eveavg_wape = np.mean(evening_wapes)  # 新增\n",
    "    print(f\"目标 {target_idx} 晚峰平均MAPE: {eveavg_mape:.4f}, 平均WAPE: {eveavg_wape:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bikpred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
